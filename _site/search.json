[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To FinEdTech",
    "section": "",
    "text": "Welcome to my data science blog!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/volatility/index.html",
    "href": "posts/volatility/index.html",
    "title": "Volatility is a model",
    "section": "",
    "text": "Contents\nLoad packages\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(patchwork)\nlibrary(scales)\nlibrary(writexl)\nlibrary(zoo)\nlibrary(moments)\n\nlibrary(rugarch); library(broom)\n\nlibrary(gt)"
  },
  {
    "objectID": "posts/volatility/index.html#volatility-as-annualized-standard-deviation",
    "href": "posts/volatility/index.html#volatility-as-annualized-standard-deviation",
    "title": "Volatility is a model",
    "section": "Volatility as annualized standard deviation",
    "text": "Volatility as annualized standard deviation\nRetrieve prices, calculate log returns and create all_returns_* dataframe\n\n# symbols &lt;- c(\"PG\", \"JPM\", \"NVDA\")\n# mult_stocks &lt;- tq_get(symbols, get = \"stock.prices\", from = \"2010-12-31\", to = \"2023-11-09\")\n# mult_stocks$symbol &lt;- mult_stocks$symbol &lt;- factor(mult_stocks$symbol, levels = c(\"PG\", \"JPM\", \"NVDA\"))\n# saveRDS(mult_stocks, \"mult_stocks.rds\")\n\nmult_stocks &lt;- readRDS(\"mult_stocks.rds\")\n\n# tq_mutate_fun_options() returns list of compatible mutate functions by pkg \n\ncalculate_returns &lt;- function(data, period) {\n  data |&gt;\n    group_by(symbol) |&gt;\n    tq_transmute(select = adjusted,\n                 mutate_fun = periodReturn, \n                 period = period, \n                 type = \"log\")\n}\n\nperiods &lt;- c(\"daily\", \"weekly\", \"monthly\", \"quarterly\", \"yearly\")\n\n# all_returns_daily &lt;- calculate_returns(mult_stocks, \"daily\")\n# all_returns_weekly &lt;- calculate_returns(mult_stocks, \"weekly\")\n# all_returns_monthly &lt;- calculate_returns(mult_stocks, \"monthly\")\n# all_returns_quarterly &lt;- calculate_returns(mult_stocks, \"quarterly\")\n\nall_returns &lt;- set_names(periods) |&gt; map(~ calculate_returns(mult_stocks, .x))\nall_returns &lt;- map(all_returns, ~ .x %&gt;%\n                                     group_by(symbol) %&gt;%\n                                     arrange(date) %&gt;%  # Sort by date within each group\n                                     slice(-1) %&gt;%      # Remove the first row for each group\n                                     ungroup())\n\nall_returns_daily     &lt;- all_returns$daily\nall_returns_weekly    &lt;- all_returns$weekly\nall_returns_quarterly &lt;- all_returns$quarterly\nall_returns_monthly   &lt;- all_returns$monthly\n\nSave to Excel (to manually check calcualtions). But after first time, eval = FALSE\n\nwrite_xlsx(all_returns, path = \"all_returns.xlsx\")\n\n# This assumes 'all_returns' is a named list of data frames\n# and each data frame has a 'symbol' column for stock tickers\n\n# Step 1: Split each data frame by 'symbol' and create a named list of data frames\nall_returns_by_stock &lt;- map(all_returns, function(df) {\n  split(df, df$symbol)\n})\n\n# Step 2: Name each data frame according to the period and stock\nall_returns_by_stock &lt;- imap(all_returns_by_stock, function(df_list, period_name) {\n  set_names(df_list, paste(period_name, names(df_list), sep = \"_\"))\n})\n\n# Flatten the list of lists into a single list of data frames\nall_returns_by_stock_flat &lt;- flatten(all_returns_by_stock)\n\n# Step 3 and 4: Write the list of data frames to an Excel file\nwrite_xlsx(all_returns_by_stock_flat, path = \"all_returns_by_stock.xlsx\")\n\n\nDifferent frequencies but the same (four year) window\n\ncalc_rolling_sd &lt;- function(data, return_col_name, window_width) {\n  data |&gt; \n        group_by(symbol) |&gt; \n        mutate(rolling_sd = rollapply(get(return_col_name), \n                                      width = window_width, \n                                      FUN = sd, \n                                      align = \"right\", fill = NA)) |&gt; \n        ungroup()\n}\n\n#  Window lengths for each frequency (aka, periodicity)\nW.daily     &lt;- 250 * 4 \nW.weekly    &lt;- 52 * 4  \nW.monthly   &lt;- 12 * 4 \nW.quarterly &lt;- 4 * 4 \n\nall_returns_daily &lt;- calc_rolling_sd(all_returns_daily, \"daily.returns\", W.daily)\nall_returns_weekly &lt;- calc_rolling_sd(all_returns_weekly, \"weekly.returns\", W.weekly)\nall_returns_monthly &lt;- calc_rolling_sd(all_returns_monthly, \"monthly.returns\", W.monthly)\nall_returns_quarterly &lt;- calc_rolling_sd(all_returns_quarterly, \"quarterly.returns\", W.quarterly)\n\nStill different frequencies (aka, periodicity) but annualized\n\n# Function to calculate annualized volatility and add frequency\ncalc_annualized_vol &lt;- function(df, sd_col, periods_per_year, freq_label) {\n  df |&gt; \n        mutate(\n            annualized_vol = !!rlang::sym(sd_col) * sqrt(periods_per_year),\n            frequency = freq_label\n    )\n}\n\n# Calculate annualized volatility for each dataframe EXCEPT Quarterly\nall_returns_daily &lt;- calc_annualized_vol(all_returns_daily, \"rolling_sd\", 252, \"Daily\")\nall_returns_weekly &lt;- calc_annualized_vol(all_returns_weekly, \"rolling_sd\", 52, \"Weekly\")\nall_returns_monthly &lt;- calc_annualized_vol(all_returns_monthly, \"rolling_sd\", 12, \"Monthly\")\n\n# Combine the data frames into one\nall_returns_combined &lt;- bind_rows(all_returns_daily, all_returns_weekly, all_returns_monthly)\nall_returns_combined &lt;- all_returns_combined |&gt; \n    mutate(frequency = factor(frequency, levels = c(\"Daily\", \"Weekly\", \"Monthly\")))\n\n# Plot the combined data with ggplot2\n\ncolor_pg &lt;- \"chartreuse2\" # don't end up using these\ncolor_jpm &lt;- \"dodgerblue2\"\ncolor_nvda &lt;- \"coral1\"\n\ncolor_tenor1 &lt;- \"khaki3\"\ncolor_tenor2 &lt;- \"coral\"\ncolor_tenor3 &lt;- \"lightseagreen\"\ncolor_tenor4 &lt;- \"slateblue\" \n\ncustom_colors &lt;- c(\"Daily\" = color_tenor1, \"Weekly\" = color_tenor2, \"Monthly\" = color_tenor3)\n\nall_returns_combined |&gt; ggplot(aes(x = date, y = annualized_vol, \n                                   color = frequency, \n                                   group = interaction(symbol, frequency))) +\n    geom_line() +\n    labs(title = \"Annualized Volatility by Frequency\",\n         x = \"Date\",\n         y = \"Annualized Volatility\") +\n    theme_minimal() +\n    scale_color_manual(values = custom_colors) +\n    facet_wrap(~symbol)\n\n\n\n# mean annualized vol by symbol and frequency\nall_returns_combined |&gt; \n    group_by(symbol, frequency) |&gt; \n    summarize(annualized_vol = mean(annualized_vol, na.rm = TRUE)) |&gt; \n    pivot_wider(names_from = frequency, values_from = annualized_vol) |&gt; \n    arrange(Daily)\n\n# A tibble: 3 × 4\n# Groups:   symbol [3]\n  symbol Daily Weekly Monthly\n  &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 PG     0.176  0.164   0.147\n2 JPM    0.265  0.255   0.239\n3 NVDA   0.421  0.396   0.388\n\n\n\n\nDifferent windows but same (DAILY) frequency\n\n# colors from my var article\n# col_ticker_fills &lt;- c(\"PG\" = \"chartreuse2\", \"JPM\" = \"dodgerblue2\", \"NVDA\" = \"coral1\")\n\nwindows &lt;- c(20, 90, 250, 500)\n\n# parse into per-ticker df and delete rolling_sd column from prior\npg_data   &lt;- all_returns_daily |&gt; filter(symbol == \"PG\")   |&gt; select(!rolling_sd)\njpm_data  &lt;- all_returns_daily |&gt; filter(symbol == \"JPM\")  |&gt; select(!rolling_sd)\nnvda_data &lt;- all_returns_daily |&gt; filter(symbol == \"NVDA\") |&gt; select(!rolling_sd)\n\n# Specify your window sizes directly in the loop\nfor (w in windows) {\n  pg_data &lt;- pg_data |&gt; \n      mutate(!!paste0(\"rolling_sd_\", w) := rollapply(daily.returns, \n                                                     width = w, \n                                                     FUN = sd, \n                                                     align = \"right\", fill = NA))\n  jpm_data &lt;- jpm_data |&gt; \n      mutate(!!paste0(\"rolling_sd_\", w) := rollapply(daily.returns, \n                                                     width = w, \n                                                     FUN = sd, \n                                                     align = \"right\", fill = NA))\n  nvda_data &lt;- nvda_data |&gt; \n      mutate(!!paste0(\"rolling_sd_\", w) := rollapply(daily.returns, \n                                                     width = w, \n                                                     FUN = sd,\n                                                     align = \"right\", fill = NA))\n}\n\nplot_rolling_sd &lt;- function(data, stock_name, ylim, custom_colors, custom_alphas) {\n    # Assume data has columns 'date', and rolling_sd_* where * is the window size\n    rolling_sd_long &lt;- data |&gt; \n        pivot_longer(cols = starts_with(\"rolling_sd_\"), names_to = \"window\", values_to = \"rolling_sd\")\n    desired_order &lt;- c(\"rolling_sd_20\", \"rolling_sd_90\", \"rolling_sd_250\", \"rolling_sd_500\")\n    rolling_sd_long$window &lt;- factor(rolling_sd_long$window, levels = desired_order)\n    ggplot(rolling_sd_long, aes(x = date, y = rolling_sd, color = window)) +\n        geom_line(aes(alpha = window)) +\n        scale_color_manual(values = custom_colors) +\n        scale_alpha_manual(values = custom_alphas) + \n        labs(title = paste(\"Rolling Daily Vol (StdDev) for\", stock_name),\n             subtitle = paste(\"Different windows:\", paste(windows, collapse = \", \"), \"days\"),\n             x = \"Date\",\n             y = \"Rolling Standard Deviation\") + \n        theme_minimal() +\n        coord_cartesian(ylim = ylim) +\n        theme(legend.position = \"right\")\n}\n\n# custom_colors &lt;- c(\"rolling_sd_20\" = \"firebrick1\", \n#                   \"rolling_sd_90\" = \"royalblue\", \n#                   \"rolling_sd_250\" = \"springgreen4\",\n#                   \"rolling_sd_500\" = \"darkorange\")\n\ncustom_colors &lt;- c(\"rolling_sd_20\" = color_tenor1, \n                   \"rolling_sd_90\" = color_tenor2, \n                   \"rolling_sd_250\" = color_tenor3, \n                   \"rolling_sd_500\" = color_tenor4)\n\ncustom_alphas &lt;- c(\"rolling_sd_20\" = 0.5, \n                   \"rolling_sd_90\" = 0.7, \n                   \"rolling_sd_250\" = 1.0,\n                   \"rolling_sd_500\" = 1.0)\n\npg_roll_plot &lt;- plot_rolling_sd(pg_data, \"PG\", c(0, 0.0350), custom_colors, custom_alphas)\njpm_roll_plot &lt;- plot_rolling_sd(jpm_data, \"JPM\", c(0, 0.0550), custom_colors, custom_alphas)\nnvda_roll_plot &lt;- plot_rolling_sd(nvda_data, \"NVDA\", c(0, 0.0750), custom_colors, custom_alphas)\n\npg_roll_plot\n\n\n\njpm_roll_plot\n\n\n\nnvda_roll_plot\n\n\n\n\n\n# Function to summarize and extract the most recent rolling standard deviations\n\nsummarize_and_extract_recent_sd &lt;- function(data) {\n  summarized &lt;- data %&gt;%\n    summarize(\n      mean_sd_20 = mean(rolling_sd_20, na.rm = TRUE),\n      mean_sd_90 = mean(rolling_sd_90, na.rm = TRUE),\n      mean_sd_250 = mean(rolling_sd_250, na.rm = TRUE),\n      mean_sd_500 = mean(rolling_sd_500, na.rm = TRUE)\n    )\n  \n  most_recent &lt;- data %&gt;%\n    slice(n()) %&gt;%\n    select(rolling_sd_20, rolling_sd_90, rolling_sd_250, rolling_sd_500) %&gt;%\n    rename(recent_sd_20 = rolling_sd_20,\n           recent_sd_90 = rolling_sd_90,\n           recent_sd_250 = rolling_sd_250,\n           recent_sd_500 = rolling_sd_500)\n  \n  bind_cols(summarized, most_recent)\n}\n\n# Apply the function to each data frame and bind rows into a single tibble\nsummary_tibble &lt;- bind_rows(\n  pg = summarize_and_extract_recent_sd(pg_data),\n  jpm = summarize_and_extract_recent_sd(jpm_data),\n  nvda = summarize_and_extract_recent_sd(nvda_data),\n  .id = \"stock\"\n)\n\nsummary_tibble |&gt; \n    gt() |&gt; \n    tab_header(\n        title = \"Rolling daily volatility\"\n    ) |&gt; \n    cols_label(\n        mean_sd_20 = \"20\",\n        mean_sd_90 = \"90\",\n        mean_sd_250 = \"250\",\n        mean_sd_500 = \"500\",\n        recent_sd_20 = \"20\",\n        recent_sd_90 = \"90\",\n        recent_sd_250 = \"250\",\n        recent_sd_500 = \"500\"\n    ) |&gt; \n    fmt_percent(columns = 2:9, \n                rows = everything()) |&gt; \n    tab_spanner(\n        label = \"Mean\",\n        columns = 2:5\n    ) |&gt; \n    tab_spanner(\n        label = \"Most recent (2023-11-08)\",\n        columns = 6:9\n    ) |&gt; \n    cols_width(\n        mean_sd_20 ~ px(80),\n        recent_sd_20 ~ px(80)\n    ) |&gt; \n    data_color(\n        columns = 6:9,\n        rows = 3,\n        method = \"numeric\",\n        domain = c(0.02, 0.04),\n        palette = c(\"lightcyan\", \"lightcyan3\")\n    )\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Rolling daily volatility\n    \n    \n    \n      stock\n      \n        Mean\n      \n      \n        Most recent (2023-11-08)\n      \n    \n    \n      20\n      90\n      250\n      500\n      20\n      90\n      250\n      500\n    \n  \n  \n    pg\n0.98%\n1.03%\n1.07%\n1.09%\n0.96%\n0.86%\n0.93%\n1.18%\n    jpm\n1.53%\n1.61%\n1.66%\n1.66%\n1.26%\n1.01%\n1.36%\n1.64%\n    nvda\n2.54%\n2.64%\n2.66%\n2.64%\n2.76%\n2.47%\n3.25%\n3.63%\n  \n  \n  \n\n\n\n\n\n# GPT wrote this routine to summarize!\n\nsummarize_rolling_sd &lt;- function(data) {\n  # Define the desired order for the window sizes\n  window_order &lt;- c(\"rolling_sd_20\", \"rolling_sd_90\", \"rolling_sd_250\", \"rolling_sd_500\")\n  \n  summarized_data &lt;- data %&gt;% \n    summarize(across(starts_with(\"rolling_sd_\"), mean, na.rm = TRUE)) %&gt;%\n    pivot_longer(cols = everything(), names_to = \"window\", values_to = \"rolling_sd\") %&gt;%\n    mutate(window = factor(window, levels = window_order)) %&gt;%\n    arrange(window)\n  \n  return(summarized_data)\n}\n\n# Apply the summarize function to each stock and combine into a single tibble\nall_summaries &lt;- bind_rows(\n  pg = summarize_rolling_sd(pg_data), \n  jpm = summarize_rolling_sd(jpm_data), \n  nvda = summarize_rolling_sd(nvda_data), \n  .id = \"stock\"\n)\n\n# Print the combined summarized data\nprint(all_summaries)\n\n# A tibble: 12 × 3\n   stock window         rolling_sd\n   &lt;chr&gt; &lt;fct&gt;               &lt;dbl&gt;\n 1 pg    rolling_sd_20     0.00977\n 2 pg    rolling_sd_90     0.0103 \n 3 pg    rolling_sd_250    0.0107 \n 4 pg    rolling_sd_500    0.0109 \n 5 jpm   rolling_sd_20     0.0153 \n 6 jpm   rolling_sd_90     0.0161 \n 7 jpm   rolling_sd_250    0.0166 \n 8 jpm   rolling_sd_500    0.0166 \n 9 nvda  rolling_sd_20     0.0254 \n10 nvda  rolling_sd_90     0.0264 \n11 nvda  rolling_sd_250    0.0266 \n12 nvda  rolling_sd_500    0.0264"
  },
  {
    "objectID": "posts/volatility/index.html#exponentially-weighted-moving-average-ewma",
    "href": "posts/volatility/index.html#exponentially-weighted-moving-average-ewma",
    "title": "Volatility is a model",
    "section": "Exponentially weighted moving average (EWMA)",
    "text": "Exponentially weighted moving average (EWMA)\n\n# Function to calculate EWMA variance\ncalculate_ewma_variance &lt;- function(returns, lambda, W) {\n    ewma_var &lt;- rep(NA, length(returns))\n    ewma_var[W] &lt;- var(returns[1:W])  # Initialize the first EWMA variance\n    for (i in (W+1):length(returns)) {\n        # Using the EWMA formula\n        ewma_var[i] &lt;- lambda * ewma_var[i-1] + (1 - lambda) * returns[i]^2\n    }\n    return(ewma_var)\n}\n\n# Example lambda value\nW       &lt;- 250\nlambda  &lt;- 0.94\n\n# Use rollapply with your custom EWMA function\n# Assuming the 'returns' column in all_returns_daily is the daily return for each stock\nall_returns_daily &lt;- all_returns_daily |&gt; \n    group_by(symbol) |&gt; \n    mutate(ewma_variance = calculate_ewma_variance(daily.returns, lambda, W)) |&gt; \n    ungroup()\n\n# Calculate the EWMA volatility from the EWMA variance\nall_returns_daily &lt;- all_returns_daily |&gt;\n    mutate(ewma_sd = sqrt(ewma_variance))\n\n# Reshape the data to long format for ggplot2\nall_returns_long &lt;- all_returns_daily |&gt; \n    pivot_longer(cols = c(\"rolling_sd\", \"ewma_sd\"), \n                 names_to = \"sd_type\", \n                 values_to = \"sd_value\")\n\n# Plot the data with ggplot2\ncustom_sd_colors &lt;- c(\"rolling_sd\" = color_tenor4, \"ewma_sd\" = color_tenor2)\n\nall_returns_long |&gt; ggplot (aes(x = date, y = sd_value, color = sd_type)) +\n    geom_line(aes(alpha = sd_type)) + \n    scale_color_manual(values = custom_sd_colors) + \n    scale_alpha_manual(values = c(0.5, 1.0)) +\n    labs(title = \"EWMA is much more reactive than unweighted StdDev\",\n         subtitle = \"(rolling_sd window = 250 days * 4 years = 1000 days)\",\n         x = \"Date\",\n         y = \"Standard Deviation\") +\n    facet_wrap(~symbol) +\n    coord_cartesian(ylim = c(0, 0.050)) +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n\n\n\n\nLet’s compare different lambda (aka, smoothing) parameters\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\nplot_ewma_for_stock &lt;- function(data, symbol, returns_col, lambda_list, \n                                custom_colors, custom_alphas, ylim) {\n\n    # Function to calculate EWMA variance - make sure this is defined in your environment\n    calculate_and_store_ewma &lt;- function(data, symbol, returns_col, lambda_list) {\n\n        for (lambda in lambda_list) {\n            W = 250\n            lambda &lt;- lambda\n            ewma_col_name &lt;- paste0(\"lambda_\", as.character(lambda))\n            data &lt;- data |&gt; \n                mutate(\n                    !!ewma_col_name := sqrt(calculate_ewma_variance(get(returns_col), lambda, W))\n                )\n            }\n        return(data)\n        }\n    \n    # Calculate and store EWMA with multiple parameters for the specified stock\n    stock_ewma &lt;- calculate_and_store_ewma(data, symbol, returns_col, lambda_list)\n    \n    # Pivot the data to long format for plotting\n    stock_ewma_long &lt;- stock_ewma |&gt; \n        pivot_longer(cols = starts_with(\"lambda\"), names_to = \"lambda\", values_to = \"ewma_sd\")\n    \n    # Plot the EWMA standard deviations for different parameter sets\n    gg &lt;- ggplot(stock_ewma_long, aes(x = date, y = ewma_sd, color = lambda)) +\n        geom_line(aes(alpha = lambda)) +\n        scale_color_manual(values = custom_colors) +\n        scale_alpha_manual(values = custom_alphas) +\n        labs(title = paste(\"EWMA volatility for\", symbol, \"with Different Parameters\"),\n             x = \"Date\",\n             y = \"EWMA Standard Deviation\") +\n        coord_cartesian(ylim = c(0, ylim)) +\n        theme_minimal() +\n        theme(legend.position = \"right\")\n\n  return(gg)\n}\n\n# Define the sets of parameters you want to compare\nlambda_list &lt;- c(0.91, 0.94, 0.97)\n\n# custom_colors &lt;- c(\"lambda_0.91\" = \"firebrick1\", \n#                   \"lambda_0.94\" = \"royalblue\", \n#                   \"lambda_0.97\" = \"springgreen4\")\n\ncustom_colors &lt;- c(\"lambda_0.91\" = color_tenor1, \n                   \"lambda_0.94\" = color_tenor2, \n                   \"lambda_0.97\" = color_tenor4)\n\ncustom_alphas &lt;- c(\"lambda_0.91\" = 0.6, \n                   \"lambda_0.94\" = 0.8, \n                   \"lambda_0.97\" = 1.0)\n\njpm_ewma_plot &lt;- plot_ewma_for_stock(jpm_data, \"JPM\", \"daily.returns\", lambda_list, \n                                custom_colors, custom_alphas, 0.080)\npg_ewma_plot &lt;- plot_ewma_for_stock(pg_data, \"PG\", \"daily.returns\", lambda_list, \n                               custom_colors, custom_alphas, 0.0550)\nnvda_ewma_plot &lt;- plot_ewma_for_stock(nvda_data, \"NVDA\", \"daily.returns\", lambda_list, \n                                 custom_colors, custom_alphas, 0.070)\n\njpm_ewma_plot\n\n\n\npg_ewma_plot\n\n\n\nnvda_ewma_plot"
  },
  {
    "objectID": "posts/volatility/index.html#garch11",
    "href": "posts/volatility/index.html#garch11",
    "title": "Volatility is a model",
    "section": "GARCH(1,1)",
    "text": "GARCH(1,1)\n\nFitting the parameters\n\n# library(rugarch)\n# Define the GARCH(1,1) specification outside the loop to avoid redundancy\nspec &lt;- ugarchspec(variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n                   mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),\n                   distribution.model = \"norm\")\n\n# Fit the GARCH(1,1) model for each stock symbol\ngarch_models &lt;- all_returns_daily |&gt; \n    group_by(symbol) |&gt; \n    nest() |&gt; \n    mutate(model = map(data, ~ ugarchfit(spec, data = .$daily.returns)))\n\n# Manual extraction of coefficients\ngarch_coef_manual &lt;- garch_models |&gt; \n    mutate(coef = map(model, function(m) as.data.frame(t(coef(m))))) |&gt; \n    select(symbol, coef) |&gt; \n    unnest(coef)\n\ngarch_coef_manual &lt;- garch_coef_manual |&gt; ungroup()\ngarch_coef_manual &lt;- garch_coef_manual |&gt; mutate(\n    gamma = 1 - alpha1 - beta1,\n    LR_vol = sqrt(omega / (1 - alpha1 - beta1))\n    )\n\ngarch_coef_manual |&gt; select(-mu) |&gt; \n    gt(rowname_col = \"symbol\") |&gt; \n    fmt_scientific(columns = 2) |&gt;\n    fmt_number(columns = 3:6, decimals = 4) \n\n\n\n\n\n  \n    \n    \n      \n      omega\n      alpha1\n      beta1\n      gamma\n      LR_vol\n    \n  \n  \n    PG\n1.06 × 10−5\n0.1310\n0.7744\n0.0946\n0.0106\n    JPM\n9.31 × 10−6\n0.0994\n0.8659\n0.0347\n0.0164\n    NVDA\n3.28 × 10−5\n0.1006\n0.8628\n0.0366\n0.0300\n  \n  \n  \n\n\n\n\n\n\nPlotting GARCH(1,1) volatility\n\n# Function to calculate GARCH(1,1) variance\ncalculate_garch_variance &lt;- function(returns, alpha, beta, gamma, W) {\n    long_run_variance &lt;- var(returns[1:W], na.rm = TRUE)\n    omega &lt;- gamma * long_run_variance\n    garch_var &lt;- rep(NA, length(returns))\n    garch_var[W] &lt;- var(returns[1:W])  # Initialize the first GARCH variance\n    \n    for (i in (W+1):length(returns)) {\n        garch_var[i] &lt;- omega + alpha * returns[i]^2 + beta * garch_var[i-1]\n    }\n    \n    return(garch_var)\n}\n\nalpha &lt;- 0.12; beta &lt;- 0.78; gamma &lt;- 1 - alpha - beta\nalpha_mr &lt;- 0.03; beta_mr &lt;- 0.83; gamma_mr &lt;- 1 - alpha_mr - beta_mr\n\n# Calculate rolling GARCH(1,1) variance\nall_returns_daily &lt;- all_returns_daily |&gt; \n  group_by(symbol) |&gt; \n  mutate(garch_variance   = calculate_garch_variance(daily.returns, alpha, beta, \n                                                   gamma, W.daily),\n         garch_variance_mr = calculate_garch_variance(daily.returns, alpha_mr, beta_mr,\n                                                     gamma_mr, W.daily))  |&gt; \n  ungroup()\n\n# Calculate the GARCH(1,1) standard deviation from the GARCH variance\nall_returns_daily &lt;- all_returns_daily |&gt; \n    mutate(garch_sd = sqrt(garch_variance),\n           garch_sd_mr = sqrt(garch_variance_mr))\n\nall_returns_long &lt;- all_returns_daily |&gt; \n    pivot_longer(cols = c(\"garch_sd\", \"garch_sd_mr\", \"ewma_sd\"), \n                 names_to = \"sd_type\", values_to = \"sd_value\")\n\n\ncustom_line_colors &lt;- c(\"garch_sd\" = color_tenor3, \n                        \"garch_sd_mr\" = color_tenor2, \n                        \"ewma_sd\" = color_tenor1)\ncustom_line_alphas &lt;- c(\"garch_sd\" = 1.0, \n                        \"garch_sd_mr\" = 1.0, \n                        \"ewma_sd\" = 0.8)\n\n# Plot the data with ggplot2\ngarch_p1 &lt;- ggplot(all_returns_long, aes(x = date, y = sd_value, color = sd_type)) +\n  geom_line(aes(alpha = sd_type)) +\n    scale_color_manual(values = custom_line_colors) +  # Set custom colors\n    scale_alpha_manual(values = custom_line_alphas) +  # Set custom alphas\n  labs(title = \"GARCH(1,1) versus EMWA\",\n       subtitle = \"The second GARCH,garch_sd_mr, has higher γ -&gt; greater mean reversion (m.r.)\",\n       x = \"Date\",\n       y = \"Standard Deviation\") +\n    facet_wrap(~symbol) +\n    coord_cartesian(ylim = c(0, 0.05)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\ngarch_p1"
  },
  {
    "objectID": "posts/volatility/index.html#none-of-the-returns-are-nearly-normal",
    "href": "posts/volatility/index.html#none-of-the-returns-are-nearly-normal",
    "title": "Volatility is a model",
    "section": "None of the returns are nearly normal",
    "text": "None of the returns are nearly normal\n\nplot_histogram_with_normal &lt;- function(stock_data, symbol, return_col) {\n    # Calculate mean and standard deviation of returns\n    mean_return &lt;- mean(stock_data[[return_col]], na.rm = TRUE)\n    sd_return &lt;- sd(stock_data[[return_col]], na.rm = TRUE)\n    \n    binwidth &lt;- (max(stock_data[[return_col]], na.rm = TRUE) - \n                     min(stock_data[[return_col]], na.rm = TRUE)) / 120\n    # print(binwidth) experimenting with binwidth\n    \n    # Create the histogram and overlay the normal distribution\n    ggplot(stock_data, aes_string(x = return_col)) +\n        geom_histogram(aes(y = ..density..), binwidth = binwidth, \n                       color = \"coral\", fill = \"khaki3\", alpha = 0.6) +\n        stat_function(fun = dnorm, args = list(mean = mean_return, sd = sd_return), \n                      color = \"slateblue\", size = 1) +\n        labs(title = paste(\"Histogram of Returns with Normal Distribution for\", symbol),\n             x = \"Returns\",\n             y = \"Density\") +\n        theme_minimal() +\n        coord_cartesian(xlim = c(-0.1, 0.1))\n}\n\njpm_hist_plot &lt;- plot_histogram_with_normal(jpm_data, \"JPM\", \"daily.returns\")\npg_hist_plot &lt;- plot_histogram_with_normal(pg_data, \"PG\", \"daily.returns\")\nnvda_hist_plot &lt;- plot_histogram_with_normal(nvda_data, \"NVDA\", \"daily.returns\")\n\njpm_hist_plot\n\n\n\npg_hist_plot\n\n\n\nnvda_hist_plot\n\n\n\n\nSummarize the moments\n\n# Function to calculate the summary statistics\ncalculate_stats &lt;- function(data, return_col) {\n    data |&gt; \n        summarize(\n            mean = mean({{ return_col }}, na.rm = TRUE),\n            sd = sd({{ return_col }}, na.rm = TRUE),\n            skew = skewness({{ return_col }}, na.rm = TRUE),\n            kurt = kurtosis({{ return_col }}, na.rm = TRUE)\n            )\n}\n\n\n# Calculate statistics for each stock and store them in a tibble\nstats_tibble &lt;- tibble(\n    stock = c(\"PG\", \"JPM\", \"NVDA\"),\n    stats = list(\n        calculate_stats(pg_data, daily.returns),\n        calculate_stats(jpm_data, daily.returns),\n        calculate_stats(nvda_data, daily.returns)\n        )\n    )\n\n# Unnest the stats column to expand the tibble\nstats_tibble &lt;- stats_tibble |&gt; \n    unnest(stats)\n\nstats_tibble |&gt; gt() |&gt; \n    fmt_percent(columns = vars(mean, sd), decimals = 4) |&gt;\n    fmt_number(columns = vars(skew, kurt), decimals = 4) |&gt; \n    data_color(\n        columns = 4,\n        domain = c(-0.2, 0.4),\n        palette = c(\"lightpink\", \"seagreen1\"),\n        alpha = 0.5\n    ) |&gt; \n    data_color(\n        columns = 5,\n        domain = c(0, 15),\n        palette = c(\"lightpink\", \"seagreen1\"),\n        alpha = 0.5\n    )\n\n\n\n\n\n  \n    \n    \n      stock\n      mean\n      sd\n      skew\n      kurt\n    \n  \n  \n    PG\n0.0380%\n1.1017%\n−0.0216\n14.7342\n    JPM\n0.0490%\n1.7601%\n−0.0967\n13.3437\n    NVDA\n0.1509%\n2.8227%\n0.2871\n10.6039\n  \n  \n  \n\n\n\n\nFor fun (the social thumb)\n\nnvda_roll_plot / garch_p1"
  },
  {
    "objectID": "posts/this-quarto-site/index.html",
    "href": "posts/this-quarto-site/index.html",
    "title": "This is a Quarto website",
    "section": "",
    "text": "I want to test-demo Quarto’s capabilities (how-to details are here on my substack). My previous data science blog was this distill site. Specifically, in this post I’d like to show:\n\nHow the openai package can prompt GPT and DALL-E via the API\nHow Quarto is truly multi-language: this page runs both R and python code chunks (even sharing the diamonds_df dataframe)\nHow the gptstudio package enables GPT as a copilot within the RStudio IDE (this is not a Quarto feature per se)\n\n\nPrompting GPT and DALL-E via API\nThe openai package includes create_image() which returns a convenient list that contains a URL of the image. For this post, I only evaluated create_image() once and saved the DALL-E image to a .png file because I don’t like every image; then commented the code. However, the subsequent GPT text prompt (i.e., completion object) is evaluated. In other words, the response to my prompt (“what are likely to be the top three implications of artificial intelligence on edtech?”) is different each time the page is rendered.\n\nSys.setenv(OPENAI_API_KEY = openai_key)\n\nlibrary(openai)\nlibrary(gptstudio)\nlibrary(tidyverse)\n# library(reticulate)\n\nprompt_dalle &lt;- \"Create a high quality background for my laptop with a minimalist landscape of a mountain with forest with multiple sky colors during sunset\"\n\nlandscape &lt;- create_image(prompt_dalle)\nlandscape_url &lt;- landscape$data$url\n\n# destination &lt;- \"mylandscape.png\"\n# download.file(landscape_url, destfile = destination, mode = \"wb\")\n\n\n\n\nlandscape by DALL·E\n\n\nSimilarly, create_chat_completion returns a list. We can easily retrieve the reply:\n\nlibrary(stringr)\nprompt_gpt &lt;- \"what are likely to be the top three implications of artificial intelligence on edtech?\"\nprompt_gpt_chars &lt;- nchar(prompt_gpt)\nprompt_gpt_words &lt;- length(strsplit(prompt_gpt, \"\\\\s+\")[[1]])\n\n# Here is the call to GPT 3.5 with my prompt_gpt text\nreply_gpt &lt;- create_chat_completion(\n    model = \"gpt-3.5-turbo\",\n    messages = list(\n        list(\n            \"role\" = \"user\",\n            \"content\" = prompt_gpt\n        )\n    )\n)\n\n# The response by GPT is a chat completion object that contains an\n# array (list) of choices (can be more than one) including the message.content\nreply_gpt_message &lt;- reply_gpt$choices$message.content\nreply_gpt_chars &lt;- nchar(reply_gpt_message)\nreply_gpt_words &lt;- length(strsplit(reply_gpt_message, \"\\\\s+\")[[1]])\n\ntotal_chars &lt;- prompt_gpt_chars + reply_gpt_chars\ntotal_words &lt;- prompt_gpt_words + reply_gpt_words\ntotal_tokens &lt;- reply_gpt$usage$total_tokens\ntoken_stats_text &lt;- paste(\"Total tokens =\", total_tokens, \n                          \". Given\", total_words, \"words and\", total_chars, \"characters, that's\",\n                          sprintf(\"%.3f\", total_tokens/total_words), \"tokens/word and\",\n                          sprintf(\"%.3f\", total_tokens/total_chars), \"tokens/character.\")\n\nprint(token_stats_text)\n\n[1] \"Total tokens = 302 . Given 241 words and 1822 characters, that's 1.253 tokens/word and 0.166 tokens/character.\"\n\ncat(reply_gpt_message, sep = \"\\n\")\n\nThe top three implications of artificial intelligence (AI) on edtech (education technology) are:\n\n1. Personalized learning: AI has the potential to revolutionize education by enabling personalized learning experiences tailored to the individual needs and abilities of each student. AI-powered systems can analyze vast amounts of data collected from students to identify their strengths, weaknesses, and learning styles, allowing educators to provide customized content, resources, and feedback. This helps students learn at their own pace and in ways that align with their specific requirements.\n\n2. Intelligent tutoring: AI-powered intelligent tutoring systems offer individualized support and guidance to students, mimicking the role of a human tutor. These systems use natural language processing and machine learning algorithms to assess students' knowledge, predict misconceptions, and provide targeted recommendations and explanations. With AI, students can access on-demand assistance and personalized tutoring, which enhances their understanding and academic success.\n\n3. Enhanced administrative processes: AI can streamline administrative tasks and processes within education institutions, freeing up time for educators to focus on teaching. AI-powered tools can automate routine tasks such as grading assessments, generating reports, and managing student records. This automation improves efficiency, reduces administrative burdens, and enables educators to devote more time to instruction and student engagement.\n\nOverall, AI in edtech holds the potential to provide tailored learning experiences, personalized tutoring, and improved administrative efficiency, ultimately enhancing the quality and effectiveness of education.\n\n\n\n\nExecuting a python code block and the sharing the dataframe\nNow I will just load the built-in diamonds dataset and lazily convert the three factor levels (cut, clarity, and color) to integers. But I will skip R’s regression model, lm(), because I am going to let python fit the linear model …\n\ndiamonds_df &lt;- diamonds\ndiamonds_df$cut_int &lt;- as.integer(diamonds_df$cut)\ndiamonds_df$clarity_int &lt;- as.integer(diamonds_df$clarity)\ndiamonds_df$color_int &lt;- as.integer(diamonds_df$color)\n\n# Going to skip lm() in R and let python fit the model!\n# lm_diamonds &lt;- lm(price ~ carat + cut_int + color_int + clarity_int, data = diamonds_df)\n# diamonds_df$residuals &lt;- resid(lm_diamonds)\n# diamonds_df$predictions &lt;- predict(lm_diamonds)\n# diamonds_df |&gt; ggplot(aes(x = predictions, y = residuals)) +\n#   geom_point() +\n#   geom_hline(yintercept = 0, linetype = \"dashed\") +\n#   labs(title = \"Residual Plot\", x = \"Predicted Values\", y = \"Residuals\")\n\n… and here is the python code chunk! This is possible because the first line of the fenced code braces the executable code with “python” per these instructions.. Of course, a python installation is required to render locally.\n\n```{python}\n#| message: false\n\ndiamonds_data_py = r.diamonds_df\n\nimport statsmodels.api as sm\ny = diamonds_data_py[[\"price\"]]\n\nx = diamonds_data_py[[\"carat\", \"cut_int\", \"color_int\", \"clarity_int\"]]\nx = sm.add_constant(x)\nmod = sm.OLS(y, x).fit()\ndiamonds_data_py[\"Predicted\"] = mod.predict(x)\ndiamonds_data_py[\"Residuals\"] = mod.resid\n```\n\nAnd, finally, I will revert back to R to utilize ggplot. As explained by Nicola Rennie the key here is to load the reticulate package so that we can use the py prefix to retrieve the diamonds_data_py object. But you can see: the original R dataframe, diamonds_df, was retreived in python, via diamonds_data_py = r.diamonds_df, and then R retrieved that model via diamonds_residuals &lt;- py$diamonds_data_py. Sweet! But, okay, not the best line we’ve ever fit. That’s why we look at the residuals, after all.\n\nlibrary(reticulate)\nlibrary(ggplot2)\nlibrary(ggthemes)\ndiamonds_residuals &lt;- py$diamonds_data_py\nggplot(data = diamonds_residuals,\n       mapping = aes(x = Predicted,\n                     y = Residuals)) +\n    geom_point(colour = \"#2F4F4F\") +\n    geom_hline(yintercept = 0, colour = \"red\") +\n    theme_economist()\n\n\n\n\n\n\n\n\n\n\nRStudio Co-pilot with gptstudio package\nWhat you can’t easily see on the page is the co-pilot enabled by the gptstudio package. Once the package is installed, I only neeed to setup the API key via the command:\n\nSys.setenv(OPENAI_API_KEY = \"&lt;APIKEY&gt;\")\n\nFor example, below I call for help with this prompt (“generate a ggplot …”)\n\n\n\ngptstudio as co-pilot\n\n\n… and the comment changes to the following:\n\n\n\nchanges the comment to code!\n\n\nIt’s not a great example but instructive. This code requires two quick tweaks in order to work. You do need to know how to write code, but at the same time, co-pilot clearly saves time; e.g., for some analyis, it’s cut the time required in half or better."
  },
  {
    "objectID": "posts/regression-simple/index.html",
    "href": "posts/regression-simple/index.html",
    "title": "Univariate regression",
    "section": "",
    "text": "Illustration of residual sum of squares (RSS) with n = 12 subset\nUnivariate (aka, simple) linear regression: AAPL vs S&P 1500, n = 72 months\nModel diagnostics\nAutocorrelation test\n\nLoading packages\n\nlibrary(tidyverse); library(gt)\nlibrary(patchwork)\n\nlibrary(broom); library(performance); library(lmtest)\nlibrary(desk)"
  },
  {
    "objectID": "posts/regression-simple/index.html#regressing-apples-aapl-returns-against-sp-1500",
    "href": "posts/regression-simple/index.html#regressing-apples-aapl-returns-against-sp-1500",
    "title": "Univariate regression",
    "section": "Regressing Apple’s (AAPL) returns against S&P 1500",
    "text": "Regressing Apple’s (AAPL) returns against S&P 1500\n\nSubset of 12 months just to illustrate RSS boxes\nThe full set is 72 months of returns. The sample of 12 months is just to illustrate the residual sum of squares (RSS) concept; the squares are less cluttered.\n\ndata_72 &lt;- readRDS(\"t2-20-17-aapl-sp1500.rds\") # 72 monthly returns\nrow.names(data_72) &lt;- 1:nrow(data_72)\nmodel_72 &lt;- lm(r_m_AAPL ~ r_SP_1500, data = data_72) # linear model\n\nset.seed(97531) # Adding Y ~ X just because they're familiar axes\ndata_12 &lt;- sample_n(data_72, 12) # sample of 12 monthly returns \ndata_12$y &lt;- data_12$r_m_AAPL # just to illustrate RSS\ndata_12$x &lt;- data_12$r_SP_1500\n\nmodel_12 &lt;- lm(y ~ x, data=data_12) # linear model\ndata_12$residuals &lt;- residuals(model_12)\n\n\nsum(model_12$residuals^2)\n\n[1] 0.0448225\n\nRSS &lt;- sum(model_12$residuals^2) \npred_Y &lt;- predict(model_12)\ncooks_d &lt;- cooks.distance(model_12)\n\n# colors for plots\ngreen_line = \"#3aaf85\"; blue_points = \"#1b6ca8\"; red_color = \"#cd201f\"\n\np0 &lt;- data_12 |&gt; ggplot(aes(x=x, y=y)) +\n    geom_point(size=3, color=blue_points) +\n    geom_smooth(method=\"lm\", se=FALSE, color=green_line)\n\np1 &lt;- p0 +\n    theme_minimal() +\n    xlab(\"S&P 1500 return\") +\n    ylab(\"AAPL return\") + \n    coord_cartesian(xlim = c(-0.15, 0.05), ylim = c(-0.15, 0.20))\n\np2 &lt;- p0 +\n    geom_segment(aes(xend=x, yend=y - residuals), color=\"purple4\", linewidth = 1, linetype = \"dashed\") +\n    geom_rect(aes(xmin = x - abs(residuals),\n                  xmax = x, \n                  ymin = ifelse(residuals &gt; 0, y - abs(residuals), y), \n                  ymax = ifelse(residuals &gt; 0, y, y + abs(residuals))), \n              fill=\"purple4\", color=\"purple\", linewidth=0.5, alpha = 0.10) +\n    theme_minimal() +\n    theme(axis.title = element_blank()) + \n    coord_cartesian(xlim = c(-0.15, 0.05), ylim = c(-0.15, 0.20))\n\nscatter_pw &lt;- p1 + p2 \nscatter_pw + plot_annotation(\n    title = \"The OLS line minimizes the residual sum of squares (RSS)\",\n    subtitle = sprintf(\"In ths case, RSS = %.4f\", RSS)\n)\n\n\n\n# To show the residuals in a gt table\nresult_df &lt;- data.frame(\n  X = data_12$x,\n  Y = data_12$y,\n  Pred_Y = pred_Y,\n  residual = model_12$residuals,\n  residual_sq = model_12$residuals^2,\n  cooksD = cooks_d\n)\n\n# But sorting by X = SP1500 \nresult_df_sorted &lt;- result_df[order(result_df$X), ]\nresult_df_sorted_tbl &lt;- gt(result_df_sorted)\np1_tbl &lt;- result_df_sorted_tbl |&gt; \n    fmt_percent(\n        columns = 1:4,\n        decimals = 2\n    ) |&gt; \n    fmt_number(\n        columns = 5:6,\n        decimals = 5\n    ) |&gt; \n    cols_label(\n        X = md(\"**S&P 1500**\"),\n        Y = md(\"**AAPL**\"),\n        Pred_Y = md(\"**Pred(AAPL)**\"),\n        residual = md(\"**Residual**\"),\n        residual_sq = md(\"**Residual^2**\"),\n        cooksD = md(\"**Cook's D**\")\n    ) |&gt; \n    data_color(\n        columns = 5,\n        palette = c(\"white\",\"purple4\"),\n        domain = c(0,0.02),\n        na_color = \"lightgrey\"\n    ) |&gt; \n     data_color(\n        columns = 6,\n        palette = c(\"white\",\"purple4\"),\n        domain = c(0,0.50),\n        na_color = \"lightgrey\"\n    ) |&gt; \n    tab_options(\n        table.font.size = 12\n    )\n\np1_tbl\n\n\n\n\n\n  \n    \n    \n      S&P 1500\n      AAPL\n      Pred(AAPL)\n      Residual\n      Residual^2\n      Cook’s D\n    \n  \n  \n    −9.15%\n−12.41%\n−11.61%\n−0.80%\n0.00006\n0.00797\n    −7.60%\n−3.10%\n−9.62%\n6.53%\n0.00426\n0.28504\n    −6.64%\n−13.26%\n−8.38%\n−4.88%\n0.00238\n0.11176\n    0.19%\n−4.36%\n0.40%\n−4.76%\n0.00226\n0.02610\n    0.42%\n0.58%\n0.70%\n−0.12%\n0.00000\n0.00002\n    0.55%\n−1.51%\n0.86%\n−2.37%\n0.00056\n0.00675\n    0.66%\n6.34%\n1.00%\n5.34%\n0.00285\n0.03485\n    1.00%\n−5.89%\n1.44%\n−7.34%\n0.00538\n0.06959\n    2.09%\n6.95%\n2.84%\n4.11%\n0.00169\n0.02787\n    3.18%\n2.76%\n4.25%\n−1.49%\n0.00022\n0.00499\n    3.42%\n18.27%\n4.55%\n13.72%\n0.01883\n0.45582\n    4.37%\n−2.18%\n5.77%\n−7.95%\n0.00632\n0.20839\n  \n  \n  \n\n\n\n\nLet’s see the effect of removing the most influential observation:\n\ninfluential_obs &lt;- which.max(cooks_d)\ndata_11_no_influential &lt;- data_12[-influential_obs, ]\nmodel_11_no_influential &lt;- lm(y ~ x, data = data_11_no_influential)\n\ncoef_original &lt;- coef(model_12)\ncoef_no_influential &lt;- coef(model_11_no_influential)\n\ncomparison &lt;- data.frame(Original = coef_original, Minus_Influential = coef_no_influential)\ncomparison\n\n               Original Minus_Influential\n(Intercept) 0.001526393       -0.01380419\nx           1.285736582        0.99796665\n\nequation_label_p1 &lt;- sprintf(\"Y = %.3f + %.3fX\", coef_original[1], coef_original[2])\nequation_label_p1i &lt;- sprintf(\"Y = %.3f + %.3fX\", coef_no_influential[1], coef_no_influential[2])\n\np1 &lt;- p1 + \n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"black\") +  # X = 0 axis\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") + # Y = 0 axis\n    annotate(\"text\", x = -0.08, y = 0.15, label = equation_label_p1, \n             size = 5.0, color = \"black\")\n\np1i &lt;- data_11_no_influential |&gt; ggplot(aes(x=x, y=y)) +\n    geom_point(size=3, color=blue_points) +\n    geom_smooth(method=\"lm\", se=FALSE, color=green_line) + # Adding regression line\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"black\") +  # X = 0 axis\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +  # Y = 0 axis\n    annotate(\"text\", x = -0.08, y = 0.15, label = equation_label_p1i, \n             size = 5.0, color = \"black\") + \n    theme_minimal() +\n    theme(axis.title = element_blank()) +\n    coord_cartesian(xlim = c(-0.15, 0.05), ylim = c(-0.15, 0.20))\n\np1 + p1i\n\n\n\n\n\n\nThe full dataset of 72 monthly returns\n\nrow.names(data_72) &lt;- 1:nrow(data_72)\nmodel_72 &lt;- lm(r_m_AAPL ~ r_SP_1500, data = data_72)\nmodel_72_coeff &lt;- coef(model_72)\nequation_label_72 &lt;- sprintf(\"Y = %.3f + %.3fX\", model_72_coeff[1], model_72_coeff[2])\n\np1_model_72 &lt;- data_72 %&gt;% ggplot(aes(r_SP_1500, r_m_AAPL)) +\n    geom_point(size = 2, color = blue_points) +\n    geom_smooth(method = \"lm\", color = green_line, fill = \"mediumpurple1\", alpha = 0.20) +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"black\") +  # X = 0 axis\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +  # Y = 0 axis\n    theme_minimal() +\n    xlab(\"S&P 1500 return\") +\n    ylab(\"AAPL return\") + \n    annotate(\"text\", x = -0.06, y = 0.15, label = equation_label_72, \n             size = 5.0, color = \"black\")\n\np1_model_72\n\n\n\nsummary(model_72) # Just to show the standard/typical output\n\n\nCall:\nlm(formula = r_m_AAPL ~ r_SP_1500, data = data_72)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.226290 -0.027060  0.002344  0.040667  0.131313 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.007969   0.007477   1.066     0.29    \nr_SP_1500   1.269625   0.215600   5.889 1.23e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06123 on 70 degrees of freedom\nMultiple R-squared:  0.3313,    Adjusted R-squared:  0.3217 \nF-statistic: 34.68 on 1 and 70 DF,  p-value: 1.23e-07\n\n\n\n\nModel output in gt table\n\nmodel_72_tidy &lt;- tidy(model_72)\ngt_table_model_72 &lt;- gt(model_72_tidy)\n\ngt_table_model_72 &lt;- \n  gt_table_model_72 %&gt;% \n  tab_options(\n    table.font.size = 14\n  ) %&gt;% \n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body()\n  ) %&gt;% \n  tab_header(\n    title = \"AAPL versus S&P_1500: Gross (incl. Rf) monthly log return\",\n    subtitle = md(\"Six years (2014 - 2019), n = 72 months\")\n  ) %&gt;% \n  tab_source_note(\n    source_note = \"Source: tidyquant https://cran.r-project.org/web/packages/tidyquant/\"\n  ) %&gt;% cols_label(\n    term = \"Coefficient\",\n    estimate = \"Estimate\",\n    std.error = \"Std Error\",\n    statistic = \"t-stat\",\n    p.value = \"p value\"\n  ) %&gt;% fmt_number(\n    columns = vars(estimate, std.error, statistic),\n    decimals = 3\n  ) %&gt;% fmt_scientific(\n    columns = vars(p.value),\n  ) %&gt;% \n  tab_options(\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12\n  )\n\ngt_table_model_72\n\n\n\n\n\n  \n    \n      AAPL versus S&P_1500: Gross (incl. Rf) monthly log return\n    \n    \n      Six years (2014 - 2019), n = 72 months\n    \n    \n      Coefficient\n      Estimate\n      Std Error\n      t-stat\n      p value\n    \n  \n  \n    (Intercept)\n0.008\n0.007\n1.066\n2.90 × 10−1\n    r_SP_1500\n1.270\n0.216\n5.889\n1.23 × 10−7\n  \n  \n    \n      Source: tidyquant https://cran.r-project.org/web/packages/tidyquant/\n    \n  \n  \n\n\n\n\nThe table above is featured in one of my practice questions:\n\n\n\n\n\n\nBT Question 20.17.1\n\n\n\nBelow [above] the results of a linear regression analysis are displayed. The dataset is monthly returns over a six-year period; i.e., n = 72 months. The gross returns of Apple’s stock (ticker: AAPL) were regressed against the S&P 1500 Index (the S&P 1500 is our proxy for the market). The explanatory variable is SP_1500 and the response (aka, dependent) variable is AAPL.\nWhich is nearest to the 90.0% confidence interval for the beta of Apple’s (AAPL) stock?\n\n90.0% CI = (0.56; 1.98)\n90.0% CI = (0.70; 1.84)\n90.0% CI = (0.91; 1.63)\n90.0% CI = (-0.004; 0.020)\n\n\n\n\n\n\n\n\n\nAnswer: C. True: 90.0% CI = (0.91; 1.63)\n\n\n\nThe two-tailed critical-Z at 90.0% confidence is 1.645 such that the CI = 1.270 +/- 1.645 × 0.216 = (0.91; 1.63). The confidence interval is given by: coefficient ± (standard error) × (critical value). The sample size is large so we can use the normal deviate of 1.645 associated with 90.0% two-tailed confidence; note this should not require any lookup because we already know the 95.0% confident one-tailed normal deviate is 1.645. With 70 degrees of freedom, the critical t value is T.INV.2T(0.10, 70) = 1.666914, so we can see that normal Z is a close approximation.\n\n\n\n# Confidence interval around the slope\nbeta &lt;- model_72_tidy$estimate[2]\nse_beta &lt;- model_72_tidy$std.error[2]\nci_confidence = 0.90\nz_2s &lt;- qnorm((1 + ci_confidence)/2)\nci_lower &lt;- beta - se_beta*z_2s\nci_upper &lt;- beta + se_beta*z_2s\n\nci_lower\n\n[1] 0.9149948\n\nci_upper\n\n[1] 1.624256"
  },
  {
    "objectID": "posts/regression-simple/index.html#model-diagnostics",
    "href": "posts/regression-simple/index.html#model-diagnostics",
    "title": "Univariate regression",
    "section": "Model diagnostics",
    "text": "Model diagnostics\nThere are many choices but I like the performance package.\n\ncheck_model(model_72, check = c(\"linearity\", \"homogeneity\", \"outliers\", \"qq\"))\n\n\n\n\nIn regard to the above:\n\nLinearity plot; aka, Tukey-Anscombe\nHomogeneity (of variance); aka, scale-location plot\nOutliers (Influential Observations) uses Cook’s distance\nQ-Q plot is test of residual normality\n\nBoth of the first two plots (upper row) can be used to check for heteroscedasticity. The second is supposedly better: by rooting the absolute value, differences are amplified. Notice it’s Y-axis (Homogeneity of Variance) is non-negative such that the “perfect” reference line is nearer to one than zero."
  },
  {
    "objectID": "posts/regression-simple/index.html#autocorrelation-tests",
    "href": "posts/regression-simple/index.html#autocorrelation-tests",
    "title": "Univariate regression",
    "section": "Autocorrelation tests",
    "text": "Autocorrelation tests\nFirst, Durbin-Watson with check_autocorrelation() in performance package:\n\ncheck_autocorrelation(model_72)\n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.068).\n\n\nLet’s plot residual against lag 1 residual.\n\nresiduals_72 &lt;- residuals(model_72)\nlagged_residuals_72 &lt;- c(NA, residuals_72[-length(residuals_72)])\n\nresidual_data &lt;- data.frame(\n  Residuals = residuals_72[-1],  # Exclude the first value as it doesn't have a lagged residual\n  Lagged_Residuals = lagged_residuals_72[-1]  # Exclude the last value as it is NA\n)\n\nggplot(residual_data, aes(x = Lagged_Residuals, y = Residuals)) +\n  geom_point(color = blue_points) +\n  labs(title = \"Scatter Plot of Residuals vs. Lagged Residuals\",\n       x = \"Lagged Residuals (i-1)\",\n       y = \"Residuals (i)\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = green_line) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = green_line) +\n  theme_minimal()\n\n\n\nlinear_model &lt;- lm(Residuals ~ Lagged_Residuals, data = residual_data)\nlinear_model\n\n\nCall:\nlm(formula = Residuals ~ Lagged_Residuals, data = residual_data)\n\nCoefficients:\n     (Intercept)  Lagged_Residuals  \n        0.001015         -0.218861  \n\ncor(residual_data$Residuals, residual_data$Lagged_Residuals)\n\n[1] -0.2207468\n\nsummary(linear_model)$r.squared\n\n[1] 0.04872914\n\ncor(residual_data$Residuals, residual_data$Lagged_Residuals)^2\n\n[1] 0.04872914\n\n\nFinally, let’s try dw.test from the desk package which is new but looks good:\n\ndw.test(model_72, dir = \"right\")\n\n\nDurbin-Watson Test on AR(1) autocorrelation \n--------------------------------------------\n\nHypotheses: \n                              H0:                         H1:\n  d &lt;= 2 (rho &gt;= 0, no neg. a.c.)  d &gt; 2 (rho &lt; 0, neg. a.c.)\n\nTest results: \n      dw  crit.value  p.value  sig.level        H0\n  2.3979      2.3765   0.0411       0.05  rejected\n\ndw.test(model_72, dir = \"both\")\n\n\nDurbin-Watson Test on AR(1) autocorrelation \n--------------------------------------------\n\nHypotheses: \n                       H0:                      H1:\n  d = 2 (rho = 0, no a.c.)  d &lt;&gt; 2 (rho &lt;&gt; 0, a.c.)\n\nTest results: \n      dw  crit.value  p.value  sig.level            H0\n  2.3979      2.4482   0.0821       0.05  not rejected"
  },
  {
    "objectID": "posts/ppc/index.html",
    "href": "posts/ppc/index.html",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "",
    "text": "Contents\nload libraries\nlibrary(tidyverse)\nlibrary(tidyquant)\n# library(dplyr); library(tidyr); library(purrr)\n# library(ggplot)"
  },
  {
    "objectID": "posts/ppc/index.html#define-the-sets-of-stocksindices",
    "href": "posts/ppc/index.html#define-the-sets-of-stocksindices",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Define the SETS of stocks/indices",
    "text": "Define the SETS of stocks/indices\nThe container in this approach is stock_sets, a dataframe the we initialize (our TOC) with three columns:\n\nset_id\ndescription\nsymbols: a list of tickers\n\n\nsector_3eft_list &lt;- c( \"XLK\", \"XLV\", \"XLP\") # Tech, Health, Staples\n# sector_4etf_list &lt;- c( \"XLK\", \"XLV\", \"XLP\",  \"XLE\",) # Tech, Health, Staples, Energy\nsector_5etf_list &lt;- c( \"XLK\", \"XLV\", \"XLP\", \"XLE\", \"XLF\") # Tech, Health, Staples, Energy, Financials\n# sector_7etf_list &lt;- c(sector_5etf_list, \"XLI\", \"XLU\")\nsector_11etf_list &lt;- c(\"XLK\",  # Technology\n                      \"XLV\",  # Health Care\n                      \"XLF\",  # Financials\n                      \"XLY\",  # Consumer Discretionary\n                      \"XLP\",  # Consumer Staples\n                      \"XLE\",  # Energy\n                      \"XLU\",  # Utilities\n                      \"XLI\",  # Industrials\n                      \"XLB\",  # Materials\n                      \"XLRE\", # Real Estate\n                      \"XLC\") # Communication Services\n\nsize_etfs &lt;- c(\"SPY\", \"MDY\", \"IWM\") # Large, Mid, Small\n# size_style_etfs &lt;- c(\"IWF\",  # Large-Cap Growth\n#                      \"IWD\",  # Large-Cap Value\n#                      \"SPY\",  # Large-Cap Blend\n#                      \"IWP\",  # Mid-Cap Growth\n#                      \"IWS\",  # Mid-Cap Value\n#                      \"MDY\",  # Mid-Cap Blend\n#                      \"IWO\",  # Small-Cap Growth\n#                      \"IWN\",  # Small-Cap Value\n#                      \"IWM\")  # Small-Cap Blend\n\nstock_sets &lt;- tibble(\n    set_id = c(\"3_sectors\",\n               \"5_sectors\", \n               \"11_sectors\",\n               \"3_sizes\"),\n    \n    description = c(\"3 Sectors picked by GPT-4: Tech, Health, Staples\",\n                    \"5 Sectors picked by GPT-4: above + Energy + Financials\",\n                    \"All  11 Sectors\",\n                    \"Size: Large, Mid, Small--Blend\"),\n    \n    # this is a list column, see https://adv-r.hadley.nz/vectors-chap.html#list-columns \n    symbols = list(sector_3eft_list, sector_5etf_list, sector_11etf_list, size_etfs)\n    )\n\ndate_start &lt;- \"2013-01-01\"\ndate_end   &lt;- \"2023-11-17\""
  },
  {
    "objectID": "posts/ppc/index.html#retrieve-returns-for-each-periodicity-aka-frequency",
    "href": "posts/ppc/index.html#retrieve-returns-for-each-periodicity-aka-frequency",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Retrieve returns for each periodicity; aka, frequency",
    "text": "Retrieve returns for each periodicity; aka, frequency\nFor each SET of tickers, get_returns will retrieve log returns for each of three periods:\n\ndaily\nweekly\nmonthly\n\nThen we will call the get_returns function via map (my favorite function) to create a new list column called nested_data. Each row of nested_data will contain a list of three dataframes, one for each period. These dataframes will contain the log returns for each ticker in the set.\n\nget_returns &lt;- function(symbols, start_date, end_date) {\n    mult_stocks &lt;- tq_get(symbols, get = \"stock.prices\", \n                          from = start_date, to = end_date)\n    \n    periods &lt;- c(\"daily\", \"weekly\", \"monthly\")\n    returns_list &lt;- lapply(periods, function(period) {\n        mult_stocks |&gt; \n            group_by(symbol) |&gt; \n            tq_transmute(select = adjusted,\n                         mutate_fun = periodReturn, \n                         period = period, \n                         type = \"log\")\n    })\n    \n    names(returns_list) &lt;- periods\n    return(returns_list)\n}\n\n# Nest return data for each stock set\nstock_sets &lt;- stock_sets |&gt; \n    mutate(nested_data = map(symbols, \n                             ~ get_returns(.x, date_start, date_end)))\n\nprint(stock_sets)\n\n# A tibble: 4 × 4\n  set_id     description                                    symbols nested_data \n  &lt;chr&gt;      &lt;chr&gt;                                          &lt;list&gt;  &lt;list&gt;      \n1 3_sectors  3 Sectors picked by GPT-4: Tech, Health, Stap… &lt;chr&gt;   &lt;named list&gt;\n2 5_sectors  5 Sectors picked by GPT-4: above + Energy + F… &lt;chr&gt;   &lt;named list&gt;\n3 11_sectors All  11 Sectors                                &lt;chr&gt;   &lt;named list&gt;\n4 3_sizes    Size: Large, Mid, Small--Blend                 &lt;chr&gt;   &lt;named list&gt;"
  },
  {
    "objectID": "posts/ppc/index.html#analyze-add-the-analysis-list-column",
    "href": "posts/ppc/index.html#analyze-add-the-analysis-list-column",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Analyze: add the analysis list column",
    "text": "Analyze: add the analysis list column\nFor each set and periodicity, the analysis list column generates:\n\nvector of volatilities\nvector of average returns\ncorrelation matrix (diagonal is 1)\naverage correlation (as a rough measure of diversification)\n\n\nperform_analysis &lt;- function(data, returns_column) {\n    volatilities &lt;- data |&gt;  \n        group_by(symbol) |&gt;  \n        summarise(volatility = sd(.data[[returns_column]], na.rm = TRUE)) |&gt;  \n        ungroup()\n    \n    avg_returns &lt;- data |&gt;  \n        group_by(symbol) |&gt;  \n        summarise(avg_return = mean(.data[[returns_column]], na.rm = TRUE)) |&gt;  \n        ungroup()\n    \n    data_wide &lt;- data |&gt;  \n        pivot_wider(names_from = symbol, values_from = .data[[returns_column]])\n    corr_matrix &lt;- cor(select(data_wide, -date), use = \"complete.obs\")\n    avg_corr &lt;- mean(corr_matrix[lower.tri(corr_matrix)])\n    \n    return(list(volatilities = volatilities, avg_returns = avg_returns, corr_matrix = corr_matrix, avg_corr = avg_corr))\n}\n\n# Applying the perform_analysis function to the stock_sets\nstock_sets &lt;- stock_sets |&gt; \n    mutate(analysis = map(nested_data, ~ {\n        data_daily &lt;- .x$daily\n        data_weekly &lt;- .x$weekly\n        data_monthly &lt;- .x$monthly\n        \n        analysis_daily &lt;- perform_analysis(data_daily, \"daily.returns\")\n        analysis_weekly &lt;- perform_analysis(data_weekly, \"weekly.returns\")\n        analysis_monthly &lt;- perform_analysis(data_monthly, \"monthly.returns\")\n        \n        list(daily = analysis_daily, weekly = analysis_weekly, monthly = analysis_monthly)\n    }))\n\n# Examine data structure \nprint(stock_sets) # Notice the analysis list column has been added\n\n# A tibble: 4 × 5\n  set_id     description                       symbols nested_data  analysis    \n  &lt;chr&gt;      &lt;chr&gt;                             &lt;list&gt;  &lt;list&gt;       &lt;list&gt;      \n1 3_sectors  3 Sectors picked by GPT-4: Tech,… &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n2 5_sectors  5 Sectors picked by GPT-4: above… &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n3 11_sectors All  11 Sectors                   &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n4 3_sizes    Size: Large, Mid, Small--Blend    &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n\nglimpse(stock_sets)\n\nRows: 4\nColumns: 5\n$ set_id      &lt;chr&gt; \"3_sectors\", \"5_sectors\", \"11_sectors\", \"3_sizes\"\n$ description &lt;chr&gt; \"3 Sectors picked by GPT-4: Tech, Health, Staples\", \"5 Sec…\n$ symbols     &lt;list&gt; &lt;\"XLK\", \"XLV\", \"XLP\"&gt;, &lt;\"XLK\", \"XLV\", \"XLP\", \"XLE\", \"XLF\"&gt;…\n$ nested_data &lt;list&gt; [[&lt;grouped_df[8217 x 3]&gt;], [&lt;grouped_df[1704 x 3]&gt;], [&lt;gr…\n$ analysis    &lt;list&gt; [[[&lt;tbl_df[3 x 2]&gt;], [&lt;tbl_df[3 x 2]&gt;], &lt;&lt;matrix[3 x 3]&gt;&gt;…\n\nstock_sets$analysis[[1]] # first row is the first stock set\n\n$daily\n$daily$volatilities\n# A tibble: 3 × 2\n  symbol volatility\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK       0.0138 \n2 XLP       0.00906\n3 XLV       0.0105 \n\n$daily$avg_returns\n# A tibble: 3 × 2\n  symbol avg_return\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK      0.000719\n2 XLP      0.000345\n3 XLV      0.000484\n\n$daily$corr_matrix\n          XLK       XLV       XLP\nXLK 1.0000000 0.7254977 0.6357681\nXLV 0.7254977 1.0000000 0.7182649\nXLP 0.6357681 0.7182649 1.0000000\n\n$daily$avg_corr\n[1] 0.6931769\n\n\n$weekly\n$weekly$volatilities\n# A tibble: 3 × 2\n  symbol volatility\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK        0.0270\n2 XLP        0.0194\n3 XLV        0.0227\n\n$weekly$avg_returns\n# A tibble: 3 × 2\n  symbol avg_return\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK       0.00347\n2 XLP       0.00167\n3 XLV       0.00233\n\n$weekly$corr_matrix\n          XLK       XLV       XLP\nXLK 1.0000000 0.6976196 0.6515028\nXLV 0.6976196 1.0000000 0.7046562\nXLP 0.6515028 0.7046562 1.0000000\n\n$weekly$avg_corr\n[1] 0.6845929\n\n\n$monthly\n$monthly$volatilities\n# A tibble: 3 × 2\n  symbol volatility\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK        0.0517\n2 XLP        0.0364\n3 XLV        0.0399\n\n$monthly$avg_returns\n# A tibble: 3 × 2\n  symbol avg_return\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK       0.0150 \n2 XLP       0.00722\n3 XLV       0.0101 \n\n$monthly$corr_matrix\n          XLK       XLV       XLP\nXLK 1.0000000 0.6136564 0.5679385\nXLV 0.6136564 1.0000000 0.6780583\nXLP 0.5679385 0.6780583 1.0000000\n\n$monthly$avg_corr\n[1] 0.6198844\n\n\nHere is a class diagram of the stock_sets data structure rendered by DiagrammeR via Graphviz."
  },
  {
    "objectID": "posts/ppc/index.html#setup-the-simulation",
    "href": "posts/ppc/index.html#setup-the-simulation",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Setup the simulation",
    "text": "Setup the simulation\nThe get_random_weights function returns a dataframe of random weights. Each column is a set of weights for a single simulation. Each row is the weight for a single stock. The weights are normalized so that they sum to 1.\nSo I’m starting with an incredibly naive approach to the simulation. I’m going to assume that the expected return for each stock is the average return for that stock over the entire period. I’m also going to assume that the volatility for each stock is the average volatility for that stock over the entire period. Most importantly, the only randomness in the simulation is the weights and they are totally naive because they are independent of the analysis. We can’t expect anything like an efficient frontier from the raw scatterplot. However, this will illustrate the future risk/reward trade-off faced by a “totally naive” investor!\n\n# returns a data frame of random weights\n# rows = weight per stock; columns = number of simulations\nget_random_weights &lt;- function(num_stocks, num_simulations) {\n    set.seed(123)\n    weights_df &lt;- matrix(nrow = num_stocks, ncol = num_simulations)\n\n    for (i in 1:num_simulations) {\n        weights &lt;- runif(num_stocks)\n        weights_df[, i] &lt;- weights / sum(weights)\n    }\n\n    return(as.data.frame(weights_df))\n}\n\n# single simulation: given a set of weights, computes the expected return and volatility\nport_sim &lt;- function(exp_returns, volatilities, corr_matrix, weights) {\n    \n    cov_matrix &lt;- outer(volatilities, volatilities) * corr_matrix\n    port_variance &lt;- t(weights) %*% cov_matrix %*% weights\n    port_exp_return &lt;- sum(weights * exp_returns)\n\n    return(list(exp_returns = exp_returns, \n                volatilities = volatilities,\n                cov_matrix = cov_matrix, \n                corr_matrix = corr_matrix,\n                port_variance = port_variance,\n                port_exp_return = port_exp_return))\n}\n\n# runs a port_simulation for each column in the weights_df\nrun_sims &lt;- function(exp_returns, volatilities, corr_matrix, weights_df) {\n    simulations &lt;- map(1:ncol(weights_df), ~ {\n        weights_vector &lt;- weights_df[, .x]\n        port_sim(exp_returns, volatilities, corr_matrix, weights_vector)\n        })\n    \n    return(simulations)\n}"
  },
  {
    "objectID": "posts/ppc/index.html#run-the-simulation-on-a-single-set",
    "href": "posts/ppc/index.html#run-the-simulation-on-a-single-set",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Run the simulation (on a single set)",
    "text": "Run the simulation (on a single set)\n\n# Selecting the desired set (e.g., \"Set 1\")\nselect_set &lt;- stock_sets |&gt; \n    filter(set_id == \"5_sectors\") |&gt; \n    pull(\"analysis\")\n\nanalyze_set &lt;- select_set[[1]]\nanalyze_period &lt;- analyze_set$monthly\n\n# Extracting components from the selected set\nexp_returns_period &lt;- analyze_period$avg_returns$avg_return\nvolatilities_period &lt;- analyze_period$volatilities$volatility\ncorr_matrix_period &lt;- analyze_period$corr_matrix\nnum_stocks_period &lt;- length(volatilities_period)\n\nnum_sims &lt;- 20000  # Set the number of simulations\nrandom_weights_df_period &lt;- get_random_weights(num_stocks_period, num_sims)\nsim_results_period &lt;- run_sims(exp_returns_period, \n                              volatilities_period, \n                              corr_matrix_period, \n                              random_weights_df_period)\n\n# Print results of the first simulation\nprint(sim_results_period[[1]])\n\n$exp_returns\n[1] 0.004030846 0.008677509 0.015025342 0.007222011 0.010114014\n\n$volatilities\n[1] 0.08350623 0.05444006 0.05165270 0.03642462 0.03991410\n\n$cov_matrix\n            XLK          XLV          XLP          XLE         XLF\nXLK 0.006973290 0.0027897366 0.0024497012 0.0013125262 0.002137823\nXLV 0.002789737 0.0029637200 0.0019066845 0.0008771254 0.001387132\nXLP 0.002449701 0.0019066845 0.0026680010 0.0007372934 0.001139679\nXLE 0.001312526 0.0008771254 0.0007372934 0.0013267528 0.001034452\nXLF 0.002137823 0.0013871321 0.0011396791 0.0010344519 0.001593135\n\n$corr_matrix\n          XLK       XLV       XLP       XLE       XLF\nXLK 1.0000000 0.6136571 0.5679384 0.4315132 0.6413965\nXLV 0.6136571 1.0000000 0.6780586 0.4423317 0.6383707\nXLP 0.5679384 0.6780586 1.0000000 0.3918793 0.5527939\nXLE 0.4315132 0.4423317 0.3918793 1.0000000 0.7115231\nXLF 0.6413965 0.6383707 0.5527939 0.7115231 1.0000000\n\n$port_variance\n            [,1]\n[1,] 0.001517294\n\n$port_exp_return\n[1] 0.009078192\n\nresults_df_period &lt;- map_dfr(sim_results_period, ~ data.frame(Exp_Return = .x$port_exp_return, \n                                                            Std_Dev = sqrt(.x$port_variance)))\n# View summarized results for daily returns\nprint(head(results_df_period))\n\n   Exp_Return    Std_Dev\n1 0.009078192 0.03895245\n2 0.010822326 0.03906639\n3 0.008376929 0.04953396\n4 0.007454897 0.04938544\n5 0.008529608 0.04357777\n6 0.008795629 0.04981343\n\nresults_df &lt;- results_df_period"
  },
  {
    "objectID": "posts/ppc/index.html#visualize-the-results",
    "href": "posts/ppc/index.html#visualize-the-results",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Visualize the results",
    "text": "Visualize the results\n\nlibrary(patchwork)\n\nresults_df &lt;- results_df |&gt; \n    arrange(Std_Dev) |&gt; \n    mutate(is_efficient = Exp_Return &gt;= cummax(Exp_Return))\n\nefficient_portfolios &lt;- results_df |&gt; \n    arrange(Std_Dev)  |&gt; \n    mutate(cummax_return = cummax(Exp_Return)) |&gt; \n    filter(Exp_Return &gt;= cummax_return)\n\nefficient_model &lt;- lm(Exp_Return ~ poly(Std_Dev, 2), data = efficient_portfolios)\n\np1 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return, color = is_efficient)) +\n    geom_point() +\n    scale_color_manual(values = c(\"azure2\", \"springgreen4\")) + \n    theme_minimal() +\n    theme(\n        axis.title = element_blank(),\n        legend.position = \"none\"\n    )\n\np2 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return)) +\n    geom_point(aes(color = is_efficient), size = 1) +  # Default size for all points\n    geom_point(data = filter(results_df, is_efficient), \n               aes(color = is_efficient), size = 2) +  # Larger size for efficient points\n    scale_color_manual(values = c(\"azure2\", \"springgreen4\")) +\n    theme_minimal() +\n    geom_line(data = efficient_portfolios, aes(x = Std_Dev, y = Exp_Return), colour = \"springgreen2\") +\n    theme(\n        axis.title = element_blank(),\n        legend.position = \"none\"\n    )\n\np3 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return)) +\n    geom_point(color = \"azure2\") +\n    geom_smooth(data = efficient_portfolios, method = \"lm\", formula = y ~ poly(x, 2), \n                se = FALSE, colour = \"springgreen4\", linewidth = 1.5) +\n    labs(x = \"Std Dev (Risk)\",\n         y = \"Return\") +\n    theme_minimal()\n\n# Calculate a color metric based on Exp_Return and Std_Dev\n\nRiskFree_temp &lt;- 0.0\nresults_df &lt;- results_df %&gt;%\n    mutate(efficiency = (Exp_Return - RiskFree_temp)/ Std_Dev)\n\n# Create a scatterplot with color gradient based on the color_metric\n# p4 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return, color = color_metric)) +\n#     geom_point() +\n#     scale_color_gradient2(low = \"azure3\", high = \"springgreen1\", mid = \"yellow\", \n#                           midpoint = median(results_df$color_metric)) +\n#     theme_minimal() +\n#     labs(color = \"Color Metric\")\n\n# Assuming results_df and color_metric are already defined appropriately\n\np4 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return, color = efficiency)) +\n    geom_point() +\n    scale_color_gradientn(colors = c(\"azure4\", \"lightgoldenrod1\", \"springgreen2\"),\n                          values = scales::rescale(c(min(results_df$efficiency), \n                                                     max(results_df$efficiency)))) +\n    theme_minimal() +\n    labs(x = \"Std Dev (Risk)\",\n         y = \"Return\", \n         color = \"efficiency\") \n\n(p1 + p2) / (p3 + p4 )"
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Logistic regression",
    "section": "",
    "text": "The delightful simdata package allows us to specify a correlation matrix and tranform (via transformation function) the random multivarate normal distribution into the set of desired univariate (but correlated!) distributions.\n\nlibrary(tidyverse)\nlibrary(gt)\n\nlibrary(patchwork)\nlibrary(GGally); library(ggcorrplot)\nlibrary(ggeffects) # amazing package plots marginal effects\n\nlibrary(simdata)\nlibrary(matrixcalc); library(mbend)\nlibrary(gmodels) # CrossTable()\n\nlibrary(skimr)\n\n# this function in the simdata package builds a correlation\n# matrix by specifying c(col, row, rho)\ncorrelation_matrix = cor_from_upper(\n    8,\n    rbind(c(1,8,-0.20), # loyalty\n          c(2,8,-0.16), # bundle\n          c(3,8,0.12),  # jump (in price)\n          c(4,8,0.15),  # premium\n          c(5,8,-0.07), # age\n          c(6,8,-0.05), # income\n          c(7,8,0))     # mobile\n)\n\n# we require positive definite matrix\n# is.positive.definite(correlation_matrix) = TRUE\nif (!is.positive.definite(correlation_matrix)) {\n    correlation_matrix &lt;- bend(correlation_matrix)$bent |&gt; round(5)\n}\n\nggcorrplot(correlation_matrix,\n           colors = c(\"red\",\"white\", \"darkgreen\"))\n\n\n\ntransformation &lt;- simdata::function_list(\n    loyalty = function(z) qbeta(pnorm(z[,1]), shape1 = 2, shape2 = 5) * 30,\n    bundle_b = function(z) z[,2] &gt; qnorm(0.7), #bundle\n    pricejump_b = function(z) z[,3] &gt; qnorm(0.8),  # 80th for 20% probability\n    premium = function(z) pnorm(z[,4]) * (2000 - 300) + 300, # premium\n    age = function(z) pmax(18, pmin(80, z[,5] * 10 + 40)), #age\n    income = function(z) exp(z[,6] + 4), #income\n    mobile_b = function(z) z[,7] &gt; 0, #mobile\n    churn = function(z) z[,8] &gt; qnorm(.8)\n)\n\n# the multivarate normal design specification\nsim_design = simdata::simdesign_mvtnorm(\n  relations = correlation_matrix,\n  transform_initial = transformation,\n  prefix_final = NULL\n)\n\nsim_data = simdata::simulate_data(sim_design, n_obs = 1000, seed = 51493)\n\nsim_data$churn &lt;- as.factor(sim_data$churn)\nsim_data$loyalty &lt;- round(sim_data$loyalty, 1)\nsim_data$bundle_b &lt;- as.factor(sim_data$bundle_b) #ok\nsim_data$pricejump_b &lt;- as.factor(sim_data$pricejump_b) #ok\nsim_data$premium &lt;- round(sim_data$premium/10)*10\nsim_data$age &lt;- round(sim_data$age)\nsim_data$income &lt;- round(sim_data$income/10)*10\nsim_data$mobile_b &lt;- as.factor(sim_data$mobile_b) #ok\n\n# don't use v1, instead will split into train/test sets\n# model_sim_v1 &lt;- glm(formula = churn ~ .,\n#        family = binomial(link = \"logit\"), data = sim_data)\n# summary(model_sim_v1)\n\nset.seed(7553695)\ntrain_sample &lt;- sample(1000, 900)\nsim_train &lt;- sim_data[train_sample, ]\nsim_test &lt;- sim_data[-train_sample, ]\n\ndata_scenario_range &lt;- data.frame(\n    loyalty = c(25,20,15,10,5,1),\n    bundle_b = as.factor(c(TRUE,TRUE,TRUE,FALSE,FALSE,FALSE)),\n    pricejump_b = as.factor(c(FALSE,FALSE,FALSE,FALSE,TRUE,TRUE)),\n    premium = c(300,500,900,1100,1600,2000),\n    age = c(70,55,40,29,24,21),\n    income = c(200,150,120,100,80,60),\n    mobile_b = as.factor(c(TRUE,FALSE,TRUE,FALSE,TRUE,FALSE))\n)\n\ndata_feature_means &lt;- data.frame(\n    loyalty = mean(sim_train$loyalty),\n    bundle_b = as.factor(FALSE),\n    pricejump_b = as.factor(FALSE),\n    premium = mean(sim_train$premium),\n    age = mean(sim_train$age),\n    income = mean(sim_train$income),\n    mobile_b = as.factor(TRUE)\n)\n\nggpairs(sim_train, columns = 1:6, lower = \"blank\")\n\n\n\nskim(sim_train)\n\n\nData summary\n\n\nName\nsim_train\n\n\nNumber of rows\n900\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nbundle_b\n0\n1\nFALSE\n2\nFAL: 632, TRU: 268\n\n\npricejump_b\n0\n1\nFALSE\n2\nFAL: 720, TRU: 180\n\n\nmobile_b\n0\n1\nFALSE\n2\nTRU: 463, FAL: 437\n\n\nchurn\n0\n1\nFALSE\n2\nFAL: 726, TRU: 174\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nloyalty\n0\n1\n8.49\n4.83\n0.2\n4.8\n7.9\n11.5\n24.9\n▆▇▅▂▁\n\n\npremium\n0\n1\n1137.60\n486.65\n300.0\n720.0\n1120.0\n1552.5\n2000.0\n▇▇▆▆▇\n\n\nage\n0\n1\n40.08\n9.52\n18.0\n33.0\n40.0\n47.0\n71.0\n▂▇▇▃▁\n\n\nincome\n0\n1\n87.72\n106.29\n0.0\n30.0\n50.0\n100.0\n1040.0\n▇▁▁▁▁"
  },
  {
    "objectID": "posts/logistic-regression/index.html#simulate-data-with-simdata-package",
    "href": "posts/logistic-regression/index.html#simulate-data-with-simdata-package",
    "title": "Logistic regression",
    "section": "",
    "text": "The delightful simdata package allows us to specify a correlation matrix and tranform (via transformation function) the random multivarate normal distribution into the set of desired univariate (but correlated!) distributions.\n\nlibrary(tidyverse)\nlibrary(gt)\n\nlibrary(patchwork)\nlibrary(GGally); library(ggcorrplot)\nlibrary(ggeffects) # amazing package plots marginal effects\n\nlibrary(simdata)\nlibrary(matrixcalc); library(mbend)\nlibrary(gmodels) # CrossTable()\n\nlibrary(skimr)\n\n# this function in the simdata package builds a correlation\n# matrix by specifying c(col, row, rho)\ncorrelation_matrix = cor_from_upper(\n    8,\n    rbind(c(1,8,-0.20), # loyalty\n          c(2,8,-0.16), # bundle\n          c(3,8,0.12),  # jump (in price)\n          c(4,8,0.15),  # premium\n          c(5,8,-0.07), # age\n          c(6,8,-0.05), # income\n          c(7,8,0))     # mobile\n)\n\n# we require positive definite matrix\n# is.positive.definite(correlation_matrix) = TRUE\nif (!is.positive.definite(correlation_matrix)) {\n    correlation_matrix &lt;- bend(correlation_matrix)$bent |&gt; round(5)\n}\n\nggcorrplot(correlation_matrix,\n           colors = c(\"red\",\"white\", \"darkgreen\"))\n\n\n\ntransformation &lt;- simdata::function_list(\n    loyalty = function(z) qbeta(pnorm(z[,1]), shape1 = 2, shape2 = 5) * 30,\n    bundle_b = function(z) z[,2] &gt; qnorm(0.7), #bundle\n    pricejump_b = function(z) z[,3] &gt; qnorm(0.8),  # 80th for 20% probability\n    premium = function(z) pnorm(z[,4]) * (2000 - 300) + 300, # premium\n    age = function(z) pmax(18, pmin(80, z[,5] * 10 + 40)), #age\n    income = function(z) exp(z[,6] + 4), #income\n    mobile_b = function(z) z[,7] &gt; 0, #mobile\n    churn = function(z) z[,8] &gt; qnorm(.8)\n)\n\n# the multivarate normal design specification\nsim_design = simdata::simdesign_mvtnorm(\n  relations = correlation_matrix,\n  transform_initial = transformation,\n  prefix_final = NULL\n)\n\nsim_data = simdata::simulate_data(sim_design, n_obs = 1000, seed = 51493)\n\nsim_data$churn &lt;- as.factor(sim_data$churn)\nsim_data$loyalty &lt;- round(sim_data$loyalty, 1)\nsim_data$bundle_b &lt;- as.factor(sim_data$bundle_b) #ok\nsim_data$pricejump_b &lt;- as.factor(sim_data$pricejump_b) #ok\nsim_data$premium &lt;- round(sim_data$premium/10)*10\nsim_data$age &lt;- round(sim_data$age)\nsim_data$income &lt;- round(sim_data$income/10)*10\nsim_data$mobile_b &lt;- as.factor(sim_data$mobile_b) #ok\n\n# don't use v1, instead will split into train/test sets\n# model_sim_v1 &lt;- glm(formula = churn ~ .,\n#        family = binomial(link = \"logit\"), data = sim_data)\n# summary(model_sim_v1)\n\nset.seed(7553695)\ntrain_sample &lt;- sample(1000, 900)\nsim_train &lt;- sim_data[train_sample, ]\nsim_test &lt;- sim_data[-train_sample, ]\n\ndata_scenario_range &lt;- data.frame(\n    loyalty = c(25,20,15,10,5,1),\n    bundle_b = as.factor(c(TRUE,TRUE,TRUE,FALSE,FALSE,FALSE)),\n    pricejump_b = as.factor(c(FALSE,FALSE,FALSE,FALSE,TRUE,TRUE)),\n    premium = c(300,500,900,1100,1600,2000),\n    age = c(70,55,40,29,24,21),\n    income = c(200,150,120,100,80,60),\n    mobile_b = as.factor(c(TRUE,FALSE,TRUE,FALSE,TRUE,FALSE))\n)\n\ndata_feature_means &lt;- data.frame(\n    loyalty = mean(sim_train$loyalty),\n    bundle_b = as.factor(FALSE),\n    pricejump_b = as.factor(FALSE),\n    premium = mean(sim_train$premium),\n    age = mean(sim_train$age),\n    income = mean(sim_train$income),\n    mobile_b = as.factor(TRUE)\n)\n\nggpairs(sim_train, columns = 1:6, lower = \"blank\")\n\n\n\nskim(sim_train)\n\n\nData summary\n\n\nName\nsim_train\n\n\nNumber of rows\n900\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nbundle_b\n0\n1\nFALSE\n2\nFAL: 632, TRU: 268\n\n\npricejump_b\n0\n1\nFALSE\n2\nFAL: 720, TRU: 180\n\n\nmobile_b\n0\n1\nFALSE\n2\nTRU: 463, FAL: 437\n\n\nchurn\n0\n1\nFALSE\n2\nFAL: 726, TRU: 174\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nloyalty\n0\n1\n8.49\n4.83\n0.2\n4.8\n7.9\n11.5\n24.9\n▆▇▅▂▁\n\n\npremium\n0\n1\n1137.60\n486.65\n300.0\n720.0\n1120.0\n1552.5\n2000.0\n▇▇▆▆▇\n\n\nage\n0\n1\n40.08\n9.52\n18.0\n33.0\n40.0\n47.0\n71.0\n▂▇▇▃▁\n\n\nincome\n0\n1\n87.72\n106.29\n0.0\n30.0\n50.0\n100.0\n1040.0\n▇▁▁▁▁"
  },
  {
    "objectID": "posts/logistic-regression/index.html#the-regression-results",
    "href": "posts/logistic-regression/index.html#the-regression-results",
    "title": "Logistic regression",
    "section": "The regression results",
    "text": "The regression results\n\nmodel_sim_v2 &lt;- glm(formula = churn ~ .,\n        family = binomial(link = \"logit\"), data = sim_train)\nsummary(model_sim_v2)\n\n\nCall:\nglm(formula = churn ~ ., family = binomial(link = \"logit\"), data = sim_train)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.2202193  0.4613275  -0.477  0.63311    \nloyalty         -0.1179751  0.0214055  -5.511 3.56e-08 ***\nbundle_bTRUE    -0.6553063  0.2151626  -3.046  0.00232 ** \npricejump_bTRUE  0.4985342  0.2054056   2.427  0.01522 *  \npremium          0.0005120  0.0001806   2.834  0.00459 ** \nage             -0.0196931  0.0092299  -2.134  0.03287 *  \nincome          -0.0008402  0.0009273  -0.906  0.36491    \nmobile_bTRUE    -0.0086937  0.1764525  -0.049  0.96070    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 883.84  on 899  degrees of freedom\nResidual deviance: 815.98  on 892  degrees of freedom\nAIC: 831.98\n\nNumber of Fisher Scoring iterations: 5\n\npredicted_probs_range &lt;- predict(model_sim_v2, newdata = data_scenario_range, type = \"response\")\npredicted_probs_means &lt;- predict(model_sim_v2, newdata = data_feature_means, type = \"response\")\nround(predicted_probs_range,5)\n\n      1       2       3       4       5       6 \n0.00534 0.01495 0.04387 0.18364 0.48978 0.67269 \n\nround(predicted_probs_means,5)\n\n      1 \n0.18077 \n\ncoef_table &lt;- coef(summary(model_sim_v2)) \ncoef_tbl  &lt;-  as_tibble(coef_table)\nCoeff_labels &lt;- c(\"(Intercept)\", \"Loyalty, yrs\", \"Bundle?(T)\", \"Price Jumped?(T)\", \"Premium, $000s\", \n                 \"Age, yrs\", \"Income, $000s\",\"Mobile?(T)\")\ncoef_tbl &lt;- cbind(Coeff_labels, coef_tbl)\n\n# Using gt() to render a table\ncoef_tbl_gt &lt;- coef_tbl %&gt;% gt() |&gt; \n    opt_table_font(stack = \"humanist\") |&gt;\n    fmt_number(columns = everything(),\n               decimals = 3)\ncoef_tbl_gt |&gt; \n    data_color(\n        columns = 'Pr(&gt;|z|)', \n        palette = c(\"darkseagreen1\", \"darkseagreen3\", \"darkseagreen4\"),\n        domain = c(0,0.05),\n        na_color = \"lightgrey\"\n    )\n\n\n\n\n\n  \n    \n    \n      Coeff_labels\n      Estimate\n      Std. Error\n      z value\n      Pr(&gt;|z|)\n    \n  \n  \n    (Intercept)\n−0.220\n0.461\n−0.477\n0.633\n    Loyalty, yrs\n−0.118\n0.021\n−5.511\n0.000\n    Bundle?(T)\n−0.655\n0.215\n−3.046\n0.002\n    Price Jumped?(T)\n0.499\n0.205\n2.427\n0.015\n    Premium, $000s\n0.001\n0.000\n2.834\n0.005\n    Age, yrs\n−0.020\n0.009\n−2.134\n0.033\n    Income, $000s\n−0.001\n0.001\n−0.906\n0.365\n    Mobile?(T)\n−0.009\n0.176\n−0.049\n0.961"
  },
  {
    "objectID": "posts/logistic-regression/index.html#evaluate-with-confusion-matrix",
    "href": "posts/logistic-regression/index.html#evaluate-with-confusion-matrix",
    "title": "Logistic regression",
    "section": "Evaluate with confusion matrix",
    "text": "Evaluate with confusion matrix\n\npredict_test_probs &lt;- predict(model_sim_v2, sim_test, type = \"response\")\n\npredict_test_class &lt;- as.factor(ifelse(predict_test_probs &gt; 0.40, \"TRUE\", \"FALSE\"))\nCrossTable(sim_test$churn, predict_test_class,\n           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,\n           dnn = c(\"Actual\", \"Predicted\"))\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  100 \n\n \n             | Predicted \n      Actual |     FALSE |      TRUE | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |        75 |         2 |        77 | \n             |     0.750 |     0.020 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |        21 |         2 |        23 | \n             |     0.210 |     0.020 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |        96 |         4 |       100 | \n-------------|-----------|-----------|-----------|\n\n \n\npredict_test_class &lt;- as.factor(ifelse(predict_test_probs &gt; 0.32, \"TRUE\", \"FALSE\"))\nCrossTable(sim_test$churn, predict_test_class,\n           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,\n           dnn = c(\"Actual\", \"Predicted\"))\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  100 \n\n \n             | Predicted \n      Actual |     FALSE |      TRUE | Row Total | \n-------------|-----------|-----------|-----------|\n       FALSE |        67 |        10 |        77 | \n             |     0.670 |     0.100 |           | \n-------------|-----------|-----------|-----------|\n        TRUE |        18 |         5 |        23 | \n             |     0.180 |     0.050 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |        85 |        15 |       100 | \n-------------|-----------|-----------|-----------|"
  },
  {
    "objectID": "posts/logistic-regression/index.html#visualization",
    "href": "posts/logistic-regression/index.html#visualization",
    "title": "Logistic regression",
    "section": "Visualization",
    "text": "Visualization\nI think the most typical plot shows one predictor (ie., one feature) on the X axis while holding the other features constant. In this case, I will use Loyalty.\n\n# test set but varying loyalty while others constant\ntest_vary_loyalty &lt;- data.frame(\n    churn = sim_test$churn,\n    churnn = as.numeric(sim_test$churn)-1,\n    loyalty = sim_test$loyalty,\n    bundle_b = as.factor(FALSE), # the rest the same\n    pricejump_b = as.factor(FALSE),\n    premium = mean(sim_test$premium),\n    age = mean(sim_test$age),\n    income = mean(sim_test$income),\n    mobile_b = as.factor(TRUE)\n)\n    \ntest_vary_loyalty$predicted_probs &lt;- predict(model_sim_v2, newdata = test_vary_loyalty, type = \"response\")\n\np1 &lt;- ggplot(test_vary_loyalty, aes(x = loyalty, y = predicted_probs)) +\n    geom_line(color = \"red\", linewidth = 1) +\n    geom_jitter(aes(x = loyalty, y = churnn), color = \"black\", size = 2, width = 0.02, height = 0.02) +\n    labs(y = \"Probability of Churn\", x = \"Loyalty, yrs\") +\n    theme_minimal()\n\n\nVisualize sigmoid\nBut I also wanted to try plotting the sigmoid (aka, logistic) function which is given by p = 1/[1+exp(-z)] where z is the linear combination of all features. You’ll notice that I appended (binded) five artificial observations merely to extend the sigmoid.\n\nsim_train2 &lt;- sim_train\nlast_rows_n &lt;- 5\nlast_row &lt;- tail(sim_train2, n = last_rows_n)\nlast_row$loyalty = rep(0,last_rows_n)\nlast_row$bundle_b = rep(FALSE, last_rows_n)\nlast_row$pricejump_b = rep(TRUE, last_rows_n)\nlast_row$premium = c(3000,5000,9000,12000,14000)\nlast_row$age = rep(18, 5)\nlast_row$income = rep(40,5)\nlast_row$mobile_b = rep(FALSE, 5)\nlast_row$churn = rep(TRUE, 5)\nsim_train2 &lt;- rbind(sim_train2, last_row)\nsim_train2f &lt;- sim_train2\n\nsim_train2$bundle_b &lt;- as.numeric(sim_train2$bundle_b) - 1\nsim_train2$pricejump_b &lt;- as.numeric(sim_train2$pricejump_b) - 1\nsim_train2$mobile_b &lt;- as.numeric(sim_train2$mobile_b) - 1\n\n# Linear combination (logit) for each observation\nsim_train2$logit &lt;- as.matrix(sim_train2[, setdiff(names(sim_train2), \"churn\")]) %*% coef(model_sim_v2)[-1] + coef(model_sim_v2)[1]\n\n# Prediction\nsim_train2$predicted_probs &lt;- predict(model_sim_v2, newdata = sim_train2f, type = \"response\")\nsim_train2$churn &lt;- as.numeric(sim_train2$churn) - 1\n\np2 &lt;- ggplot(sim_train2, aes(x = logit, y = predicted_probs)) +\n    geom_jitter(alpha = 0.5, color = \"purple\", size = 2, width = 0.02, height = 0.02) +\n    geom_jitter(aes(x = logit, y = churn), color = \"black\", size = 2, width = 0.02, height = 0.02) +\n    # Next plots the sigmoid function\n    geom_line(aes(y = 1 / (1 + exp(-logit))), color = \"red\", linewidth = 1) +\n    labs(y = \"Probability of Churn\", x = \"Logit (Linear Combination)\") +\n    coord_cartesian(xlim = c(-5,5)) +\n    theme_minimal() \n\np1 + p2\n\n\n\n\n\n\nVisualize the marginal effects\n\np3 &lt;- plot(ggpredict(model_sim_v2,c(\"loyalty\", \"pricejump_b\")))\np4 &lt;- plot(ggpredict(model_sim_v2,c(\"loyalty\", \"bundle_b\")))\np3 &lt;- p3 + coord_cartesian(ylim = c(0,.6))\np4 &lt;- p4 + coord_cartesian(ylim = c(0,.6))\np3 + p4\n\n\n\nwhat_is &lt;- ggpredict(model_sim_v2,c(\"loyalty\", \"pricejump_b\"))\nwhat_is\n\n# Predicted probabilities of churn\n\n# pricejump_b = FALSE\n\nloyalty | Predicted |       95% CI\n----------------------------------\n      0 |      0.38 | [0.29, 0.48]\n      5 |      0.25 | [0.20, 0.31]\n     10 |      0.16 | [0.12, 0.20]\n     15 |      0.09 | [0.06, 0.14]\n     20 |      0.05 | [0.03, 0.10]\n     25 |      0.03 | [0.01, 0.07]\n\n# pricejump_b = TRUE\n\nloyalty | Predicted |       95% CI\n----------------------------------\n      0 |      0.50 | [0.38, 0.62]\n      5 |      0.36 | [0.27, 0.45]\n     10 |      0.23 | [0.17, 0.32]\n     15 |      0.15 | [0.09, 0.22]\n     20 |      0.09 | [0.05, 0.16]\n     25 |      0.05 | [0.02, 0.11]\n\nAdjusted for:\n* bundle_b =   FALSE\n*  premium = 1137.60\n*      age =   40.08\n*   income =   87.72\n* mobile_b =   FALSE"
  },
  {
    "objectID": "posts/logistic-regression/index.html#numerical-examples-to-help-explain-coefficient-interpretation",
    "href": "posts/logistic-regression/index.html#numerical-examples-to-help-explain-coefficient-interpretation",
    "title": "Logistic regression",
    "section": "Numerical examples to help explain coefficient interpretation",
    "text": "Numerical examples to help explain coefficient interpretation\n\nint_test &lt;- data_feature_means\nint_test$loyalty = 0\nint_test\n\n  loyalty bundle_b pricejump_b premium      age   income mobile_b\n1       0    FALSE       FALSE  1137.6 40.07667 87.72222     TRUE\n\npredict(model_sim_v2, newdata = int_test, type = \"response\")\n\n        1 \n0.3753352 \n\nint_test$pricejump_b = as.factor(TRUE)\npredict(model_sim_v2, newdata = int_test, type = \"response\")\n\n        1 \n0.4972846 \n\nint_test$loyalty = 1\npredict(model_sim_v2, newdata = int_test, type = \"response\")\n\n        1 \n0.4678353 \n\nint_test$loyalty = 10\npredict(model_sim_v2, newdata = int_test, type = \"response\")\n\n        1 \n0.2331494 \n\nint_test$pricejump_b = as.factor(FALSE)"
  },
  {
    "objectID": "posts/kmeans_p1/index.html",
    "href": "posts/kmeans_p1/index.html",
    "title": "Clustering with k-means algorithm",
    "section": "",
    "text": "Contents"
  },
  {
    "objectID": "posts/kmeans_p1/index.html#retrive-stocks-and-standardize-features",
    "href": "posts/kmeans_p1/index.html#retrive-stocks-and-standardize-features",
    "title": "Clustering with k-means algorithm",
    "section": "Retrive stocks and standardize features",
    "text": "Retrive stocks and standardize features\n\nlibrary(tidyverse)\nlibrary(corrplot); library(ggcorrplot) # may not use\nlibrary(factoextra) \n\n# source is free trial of S&P https://www.tiingo.com/ \n# This is approximately the S&P1500; i.e, large-, mid- and small-cap stocks\nstocks1500 &lt;- read_csv(\"k-means-set_v1_2.csv\") \n\nstocks1500 &lt;- stocks1500 |&gt; rename(\n    market_cap = 'Market Cap',\n    div_yield = 'Dividend Yield',\n    gross_margin = 'Gross Margin',\n    revenue_growth = 'Revenue Growth (QoQ)',\n    rho_sp500 = 'Correlation to S&P 500',\n    volatility = '1 Year Volatility',\n    pe_ratio = 'P/E Ratio',\n    debt_equity = 'Debt/Equity (D/E) Ratio',\n    ROE = 'Return on Equity (ROE)',\n    ROA = 'Return on Assets (ROA/ROI)',\n    TSR_1year = '1 Year Return',\n    rho_treasury = 'Correlation to U.S. Treasuries',\n    enterprise_value = 'Enterprise Val',\n    pb_ratio = 'P/B Ratio'\n)\n\n# remove outliers, observed ex post\nstocks1500 &lt;- stocks1500 |&gt; filter(Ticker != \"AMZN\")\nstocks1500 &lt;- stocks1500 |&gt; filter(!Ticker %in% c(\"PDD\", \"MELI\", \"NDAQ\", \"RCL\"))\n\n# filtering by market cap: important reduction here!\ndf &lt;- stocks1500 |&gt; filter(market_cap &gt; mean(stocks1500$market_cap))\nnumeric_cols &lt;- df |&gt; select_if(is.numeric)\noptions(scipen = 999)\n\n# because we're going to standardize the features\noriginal_means &lt;- colMeans(numeric_cols)\noriginal_sds &lt;- numeric_cols  |&gt;  map_dbl(sd)\n\nstd_cols &lt;- numeric_cols |&gt; \n  mutate(across(everything(), ~(. - mean(.)) / sd(.)))\n\ndf_std &lt;- df |&gt; \n    select(Ticker, Name, Sector, Industry) |&gt; \n    bind_cols(std_cols)\n\n\nSelect features\n\nselected_features &lt;- c(\"volatility\", \"TSR_1year\")"
  },
  {
    "objectID": "posts/kmeans_p1/index.html#elbow-method-for-optimal-clusters",
    "href": "posts/kmeans_p1/index.html#elbow-method-for-optimal-clusters",
    "title": "Clustering with k-means algorithm",
    "section": "Elbow method for optimal clusters",
    "text": "Elbow method for optimal clusters\n\ncompute_elbow &lt;- function(df, selected_columns) {\n    numeric_data &lt;- select(df, all_of(selected_columns))\n    \n    compute_wss &lt;- function(k) {\n        kmeans_result &lt;- kmeans(numeric_data, centers = k, nstart = 25)\n        kmeans_result$tot.withinss\n    }\n    \n    k_values &lt;- 1:25\n    wss_values &lt;- map_dbl(k_values, compute_wss)\n    \n    elbow_data &lt;- tibble(k = k_values, wss = wss_values)\n\n    # Calculate slopes\n    elbow_data &lt;- elbow_data %&gt;%\n        mutate(slope = c(NA, diff(wss) / diff(k)))\n\n    return(elbow_data)\n}\n\nelbow_data &lt;- compute_elbow(df_std, selected_features)\n\nplot_elbow &lt;- function(elbow_data) {\n    ggplot(elbow_data, aes(x = k, y = wss)) +\n        geom_line() +\n        geom_point() +\n        geom_text(aes(label = round(slope, 1)), vjust = -1.5) +\n        theme_minimal() +\n        labs(title = \"Elbow Method for Optimal Number of Clusters\",\n             x = \"Number of Clusters (k)\", \n             y = \"Total Within-Cluster Sum of Squares\")\n}\n\n# Use the function with your data\nelbow_plot &lt;- plot_elbow(elbow_data)\n\n# Display the plot\nprint(elbow_plot)"
  },
  {
    "objectID": "posts/kmeans_p1/index.html#k-means-clusters",
    "href": "posts/kmeans_p1/index.html#k-means-clusters",
    "title": "Clustering with k-means algorithm",
    "section": "K-means clusters",
    "text": "K-means clusters\n\nset.seed(9367) # Set a random seed for reproducibility\n\n# Color palette\n# location (ex post): top-middle, bottom-middle, bottom-right, left, top-right\ncustom_colors &lt;- c(\"blue1\", \"darkorange1\", \"firebrick3\", \"cyan3\", \"springgreen3\")\n\nnumeric_data &lt;- df_std |&gt; select(all_of(selected_features))\n\n# based on the elbow method's so-called \n# elbow point but ultimately is discretionary\nnum_clusters &lt;- 5 \n\nkmeans_result_n &lt;- kmeans(numeric_data, centers = num_clusters, nstart = 25)\n\n# Print out the results\nprint(kmeans_result_n)\n\nK-means clustering with 5 clusters of sizes 68, 69, 30, 85, 30\n\nCluster means:\n  volatility  TSR_1year\n1 -0.1663365  0.6846281\n2  0.0751603 -0.6467280\n3  1.7067078 -0.1904547\n4 -0.9749478 -0.6417393\n5  1.2598050  1.9443669\n\nClustering vector:\n  [1] 1 5 1 1 1 4 1 4 2 4 5 1 4 5 3 2 1 1 4 4 4 2 4 1 4 1 4 1 2 2 4 1 2 1 2 2 2\n [38] 5 2 4 1 3 4 4 2 3 4 1 2 2 5 4 2 2 5 2 4 4 2 2 4 4 2 1 2 4 2 1 2 4 4 2 2 4\n [75] 2 2 4 5 1 5 4 2 4 1 5 4 4 4 2 5 1 2 4 4 1 5 4 4 2 4 1 4 1 1 5 2 5 2 1 3 1\n[112] 4 3 3 4 2 2 4 4 4 4 5 4 2 1 5 5 4 4 5 1 3 2 1 4 1 2 4 1 1 2 1 1 3 4 1 1 1\n[149] 3 3 3 1 2 3 5 1 4 1 1 4 2 4 1 5 1 4 2 4 1 3 2 4 3 2 3 4 1 2 3 3 1 4 4 3 4\n[186] 5 1 3 2 2 3 3 3 1 1 2 4 3 4 2 2 4 1 4 2 4 4 2 4 4 4 5 1 1 2 2 2 2 4 4 3 1\n[223] 4 3 1 1 4 1 3 1 5 3 1 3 4 2 2 1 5 5 4 5 4 4 2 2 4 2 1 1 2 2 4 4 4 2 3 1 5\n[260] 1 2 1 2 1 4 3 2 1 5 1 5 4 2 5 2 4 2 5 4 4 1 4\n\nWithin cluster sum of squares by cluster:\n[1] 29.48504 27.95429 21.43397 27.33563 27.48363\n (between_SS / total_SS =  76.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n# attach cluster membership back to the original data\ndf_std$cluster &lt;- kmeans_result_n$cluster\ndf$cluster &lt;- kmeans_result_n$cluster\n\n# Calculate mean and standard deviation for each feature, grouped by cluster\ncluster_summary &lt;- df |&gt; \n  group_by(cluster) |&gt; \n  summarise(across(everything(),\n                   list(mean = ~mean(.), sd = ~sd(.)), \n                   .names = \"{.col}_{.fn}\"))\n\n# View the results: below instead\n# cluster_summary$volatility_mean\n# cluster_summary$volatility_sd\n# cluster_summary$TSR_1year_mean\n# cluster_summary$TSR_1year_sd\n\n# cross_tab &lt;- table(df_std$Sector, kmeans_result_n$cluster)\ntable(df_std$Sector, kmeans_result_n$cluster)\n\n                        \n                          1  2  3  4  5\n  Basic Materials         6  4  3  1  2\n  Communication Services  3  3  0  3  0\n  Consumer Cyclical       4  2  4  0  4\n  Consumer Defensive      2  8  1 13  0\n  Discretionary           0  1  0  0  0\n  Energy                  9  4 13  4  2\n  Financial Services     12 18  3 10  7\n  Healthcare              3  4  0 17  1\n  Industrials            14  8  0 17  4\n  Real Estate             3  4  0  3  0\n  Technology             10  7  6  3 10\n  Unknown                 1  0  0  1  0\n  Unknown Sector          1  3  0  1  0\n  Utilities               0  3  0 12  0\n\n\n\nVisualized\n\n# Plotting\nggplot(df_std, aes(x = volatility, y = TSR_1year, color = as.factor(cluster))) +\n    geom_point() +  # Add points\n    stat_ellipse(type = \"norm\", level = 0.95) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\") +\n    scale_color_manual(values = custom_colors) +  # Use custom color palette\n    theme_minimal() +  # Minimal theme\n    labs(color = \"Cluster\", \n         title = \"K-means clustering with only 2 features\",\n         subtitle = \"Features are scaled\",\n         x = \"Volatility\", \n         y = \"1 year TRS (TSR_1year) \")\n\n\n\nmodel_lm &lt;- lm(TSR_1year ~ volatility, data = df_std)\ncorr &lt;- cor(df_std$TSR_1year, df_std$volatility)\n\n\n\nUnscaled centroids\n\nselected_means &lt;- original_means[selected_features]\nselected_sds &lt;- original_sds[selected_features]\n\nscaled_centroids &lt;- kmeans_result_n$centers\n\n# Element-wise multiplication of each column by the corresponding standard deviation\n# Then, addition of each column by the corresponding mean\nunscaled_centroids &lt;- sweep(scaled_centroids, 2, selected_sds, FUN = \"*\")\nunscaled_centroids &lt;- sweep(unscaled_centroids, 2, selected_means, FUN = \"+\")\n\nunscaled_centroids_df &lt;- as.data.frame(unscaled_centroids)\nrownames(unscaled_centroids_df) &lt;- paste(\"Cluster\", 1:nrow(unscaled_centroids_df))\n\n# sort by volatility\nunscaled_centroids_df &lt;- unscaled_centroids_df[order(unscaled_centroids_df$volatility), ]\n\nprint(unscaled_centroids_df)\n\n          volatility   TSR_1year\nCluster 4  0.2438719 -0.04716554\nCluster 1  0.3173974  0.26064020\nCluster 2  0.3393562 -0.04832325\nCluster 5  0.4470738  0.55298364\nCluster 3  0.4877098  0.05756260"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FinEdTech by DH",
    "section": "",
    "text": "Clustering with k-means algorithm\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nUnsupervised learning groups observations by feature similarity\n\n\n\n\n\n\nDec 3, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nApplying my PPC data model\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nVisualizing correlation and mapping CMLs\n\n\n\n\n\n\nNov 26, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nPortfolio Possibilities Curve (PPC)\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nPPC with GPT4 as my coding partner\n\n\n\n\n\n\nNov 19, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nVolatility is a model\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nBeing unobservable, it cleaves to a subjective model\n\n\n\n\n\n\nNov 12, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nValue at Risk (VaR) Introduction\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nHistorical simulation (basic + bootstrap, MCS, and parametric)\n\n\n\n\n\n\nNov 5, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nWorst case scenario simulation (basic)\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nThe expected worst loss over 10,000 trials for different horizons\n\n\n\n\n\n\nNov 2, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nLogistic regression\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nSimulated insurance dataset (via simdata) and visualization of marginal effects (via ggeffects)\n\n\n\n\n\n\nOct 29, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nUnivariate regression\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nDescription to go here\n\n\n\n\n\n\nOct 22, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nNearest neighbors\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nThis lazy learning algorithm requires us to select k, but it’s a fast and intuitive classifer\n\n\n\n\n\n\nOct 13, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nDecision Trees (part 1)\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nJust some description here for now\n\n\n\n\n\n\nOct 7, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nThis is a Quarto website\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nPosit’s brilliant system executes either R or python code (and a GPT API example)\n\n\n\n\n\n\nSep 15, 2023\n\n\nDavid Harper\n\n\n\n\n\n\n  \n\n\n\n\nLogistic regression coefficients\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nFitting a logistic regression model is easy in R, but coefficient interpretation is non-trivial\n\n\n\n\n\n\nSep 4, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nSimulating the equity risk premium\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nThe implied ERP is very sensitive to assumptions, in particular G2\n\n\n\n\n\n\nAug 31, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To FinEdTech\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nDavid Harper\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About David",
    "section": "",
    "text": "Hello! I’m David Harper, CFA, FRM. I started bionicturtle.com in 2004 and built it into the #1 exam preparation provider (EPP) for the Financial Risk Manager (FRM). In February of 2021, I sold the business to CeriFi where I continue to run it as President, Bionic Turtle, a CeriFi company. And now I’m part of a great team that is building the world’s greatest platform for financial education at every stage of your career.\nI love learning about (and teaching) risk. I also write investing articles at Seeking Alpha, where I disclose my portfolio/trades and practice risk management. My focus is disruptive technologies, patient GARP-style with a portfolio risk overlay; e.g., diversification. The market wants growth but I’m a value technician at heart, consequently I am willing to wait for great companies to prove their worth to the general audience. I have a BA, Economics from UC, Berkeley. I earned my CFA in 2003 and my FRM in 2004.\nI’ve been training and practicing data science for several years. Of course, my preferred data tool is R (aka, #RStats, #rstat). The best thing about R is the amazing community, there is nothing quite like an RStudio conference. Online I try to keep sharp with Coursera, datacamp and perhaps the most talented data science instructor on the planet: Matt Dancho’s Business Science.\nI am extremely interested (and invested) in digital disruption and the future of work (FOW), including EdTech, talent marketplaces and work management (my favorite software is wrike).\nThis is a Quarto site using the built-in sandstone theme"
  },
  {
    "objectID": "posts/decision-tree-p1/index.html",
    "href": "posts/decision-tree-p1/index.html",
    "title": "Decision Trees (part 1)",
    "section": "",
    "text": "Contents\n\nTrain (and graph) dividend payer with rpart(), rpart.plot and C5.0\nLoan default train\nLoan default prediction\nAdding penalty matrix to make false negatives costly\nTrees are random but not too fragile\n\nTo write a PQ set for decision trees, I experimented below. First the libraries:\n\nmy_libraries &lt;- c(\"C50\", \"gmodels\", \"tidyverse\", \"openxlsx\", \n                  \"rattle\", \"rpart\", \"rpart.plot\")\nlapply(my_libraries, library, character.only = TRUE)\n\n\nPredicting dividend\nGARP’s motivating example is a super simple (n = 20) dataset of public companies the either pay or do not pay a Dividend. The my20firms dataframe (you can see) is slightly altered to achieve a tree that I liked better for purposes of a practice question:\n\n# my20firms &lt;- garp_data\n# my20firms$Dividend[1] &lt;- 0\n# my20firms$Dividend[9] &lt;- 0\n# my20firms$Dividend[12] &lt;- 1\n# my20firms$Dividend[13] &lt;- 0\n# my20firms$Dividend[15] &lt;- 0\n\n# colnames(my20firms)[colnames(my20firms) == \"Retail_investor\"] &lt;- \"Retail\"\n# colnames(my20firms)[colnames(my20firms) == \"Large_cap\"] &lt;- \"LargeCap\"\n\n# write.xlsx(my20firms, file = \"dividendExampleModified_v3.xlsx\")\n# my20firms &lt;- read.xlsx(\"dividendExampleModified_v3.xlsx\")\n# saveRDS(my20firms, file = \"my20firms-rds.RDS\")\n\nmy20firms &lt;- readRDS(\"my20firms-rds.RDS\")\nmy20firms\n\n   Dividend Earnings LargeCap Retail Tech\n1         0        0        1     40    1\n2         1        1        1     30    0\n3         1        1        1     20    0\n4         0        0        0     80    1\n5         1        0        1     20    0\n6         0        1        0     30    1\n7         0        1        0     40    0\n8         1        0        1     60    0\n9         0        1        1     20    1\n10        0        1        1     40    0\n11        0        0        0     20    1\n12        1        0        1     70    0\n13        0        1        0     30    1\n14        1        0        1     70    0\n15        0        0        1     50    1\n16        1        0        1     60    1\n17        1        1        1     30    0\n18        0        1        0     30    1\n19        0        0        0     40    0\n20        1        1        1     50    0\n\nfit2 &lt;- rpart(Dividend ~ ., data = my20firms, \n              parms = list(split = \"gini\"),\n              control = rpart.control(minsplit = 1, \n                                      minbucket = 1,\n                                      maxdepth = 4))\n\n# summary(fit2) printout is too long\nprint(fit2)\n\nn= 20 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 20 4.9500000 0.4500000  \n   2) LargeCap&lt; 0.5 7 0.0000000 0.0000000 *\n   3) LargeCap&gt;=0.5 13 2.7692310 0.6923077  \n     6) Tech&gt;=0.5 4 0.7500000 0.2500000  \n      12) Retail&lt; 55 3 0.0000000 0.0000000 *\n      13) Retail&gt;=55 1 0.0000000 1.0000000 *\n     7) Tech&lt; 0.5 9 0.8888889 0.8888889  \n      14) Earnings&gt;=0.5 5 0.8000000 0.8000000  \n        28) Retail&gt;=35 2 0.5000000 0.5000000 *\n        29) Retail&lt; 35 3 0.0000000 1.0000000 *\n      15) Earnings&lt; 0.5 4 0.0000000 1.0000000 *\n\nprintcp(fit2)\n\n\nRegression tree:\nrpart(formula = Dividend ~ ., data = my20firms, parms = list(split = \"gini\"), \n    control = rpart.control(minsplit = 1, minbucket = 1, maxdepth = 4))\n\nVariables actually used in tree construction:\n[1] Earnings LargeCap Retail   Tech    \n\nRoot node error: 4.95/20 = 0.2475\n\nn= 20 \n\n        CP nsplit rel error  xerror     xstd\n1 0.440559      0   1.00000 1.11610 0.056479\n2 0.228352      1   0.55944 1.12013 0.298518\n3 0.151515      2   0.33109 1.04063 0.309021\n4 0.039282      3   0.17957 0.81948 0.360160\n5 0.010000      5   0.10101 0.80808 0.361385\n\nrpart.plot(fit2, yesno = 2, left=FALSE, type=2, branch.lty = 3, nn= TRUE, \n           box.palette = \"BuGn\", leaf.round=0)\n\n\n\n# converting the target to factor\nmy20firms$Dividend &lt;- as_factor(my20firms$Dividend)\n\n\nfit3 &lt;- rpart(Dividend ~ ., data = my20firms, \n              parms = list(split = \"gini\"),\n              control = rpart.control(minsplit = 1, \n                                      minbucket = 1,\n                                      maxdepth = 4))\n\nprint(fit3)\n\nn= 20 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 20 9 0 (0.5500000 0.4500000)  \n   2) LargeCap&lt; 0.5 7 0 0 (1.0000000 0.0000000) *\n   3) LargeCap&gt;=0.5 13 4 1 (0.3076923 0.6923077)  \n     6) Tech&gt;=0.5 4 1 0 (0.7500000 0.2500000)  \n      12) Retail&lt; 55 3 0 0 (1.0000000 0.0000000) *\n      13) Retail&gt;=55 1 0 1 (0.0000000 1.0000000) *\n     7) Tech&lt; 0.5 9 1 1 (0.1111111 0.8888889) *\n\nrpart.plot(fit3, yesno = 2, left=FALSE, type=2, branch.lty = 3, nn= TRUE, \n           box.palette = \"BuGn\", leaf.round=0)\n\n\n\n\nI had to refresh my knowledge of decision trees, and for that I depended on the awesome book () that I will review in the future (almost done!). He uses C5.0 algorithm (per the C50 package) and I just wanted to see its defaults:\n\ntree_c5 &lt;- C5.0(Dividend ~ ., data = my20firms)\nplot(tree_c5)\n\n\n\n# set MinCases = 1\n\ntree_c5_v2 &lt;- C5.0(Dividend ~ ., \n                   control = C5.0Control(minCases = 1),\n                   data = my20firms)\nplot(tree_c5_v2)\n\n\n\n\n\n\nLoan default examples\nNow I will switch datasets, and use the same loan default dataset used in the book. But I will use the more familiar rpart() function to train the tree. The result is similar but not identical (and please not the difference is not due to sampling varation: my test sample is the same).\n\nset.seed(9829)\ntrain_sample &lt;- sample(1000, 900)\n\ncredit &lt;- read.csv(\"credit.csv\", stringsAsFactors = TRUE)\n\n# split the data frames\ncredit_train &lt;- credit[train_sample, ]\ncredit_test  &lt;- credit[-train_sample, ]\n\ncredit_train$credit_history &lt;- credit_train$credit_history |&gt; \n    fct_relevel(\"critical\", \"poor\", \"good\", \"very good\", \"perfect\")\n\ntree_credit_train &lt;- rpart(default ~ ., data = credit_train,\n                           parms = list(split = \"gini\"),\n                           control = rpart.control(minsplit = 1, \n                                                   minbucket = 1,\n                                                   maxdepth = 4))\n\nrpart.plot(tree_credit_train, yesno = 2, left=FALSE, type=2, branch.lty = 3, nn= TRUE, \n           box.palette = c(\"palegreen\", \"pink\"), leaf.round=0, extra = 101, digits = 4)\n\n\n\nprint(tree_credit_train)\n\nn= 900 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 900 265 no (0.7055556 0.2944444)  \n   2) checking_balance=&gt; 200 DM,unknown 415  55 no (0.8674699 0.1325301) *\n   3) checking_balance=&lt; 0 DM,1 - 200 DM 485 210 no (0.5670103 0.4329897)  \n     6) credit_history=critical,poor,good 426 167 no (0.6079812 0.3920188)  \n      12) months_loan_duration&lt; 27.5 324 105 no (0.6759259 0.3240741)  \n        24) amount&lt; 9899.5 317  98 no (0.6908517 0.3091483) *\n        25) amount&gt;=9899.5 7   0 yes (0.0000000 1.0000000) *\n      13) months_loan_duration&gt;=27.5 102  40 yes (0.3921569 0.6078431)  \n        26) dependents&gt;=1.5 14   4 no (0.7142857 0.2857143) *\n        27) dependents&lt; 1.5 88  30 yes (0.3409091 0.6590909) *\n     7) credit_history=very good,perfect 59  16 yes (0.2711864 0.7288136)  \n      14) age&lt; 22.5 3   0 no (1.0000000 0.0000000) *\n      15) age&gt;=22.5 56  13 yes (0.2321429 0.7678571) *\n\nprintcp(tree_credit_train)\n\n\nClassification tree:\nrpart(formula = default ~ ., data = credit_train, parms = list(split = \"gini\"), \n    control = rpart.control(minsplit = 1, minbucket = 1, maxdepth = 4))\n\nVariables actually used in tree construction:\n[1] age                  amount               checking_balance    \n[4] credit_history       dependents           months_loan_duration\n\nRoot node error: 265/900 = 0.29444\n\nn= 900 \n\n        CP nsplit rel error  xerror     xstd\n1 0.050943      0   1.00000 1.00000 0.051599\n2 0.026415      3   0.81509 0.83396 0.048726\n3 0.022642      4   0.78868 0.82642 0.048577\n4 0.011321      5   0.76604 0.81887 0.048425\n5 0.010000      6   0.75472 0.81887 0.048425\n\n\n\n\nDefault prediction\nBecause there is a 10% test set, we can test the decision tree. It’s not great. In terms of the mistake, notice that 28/35 actual defaulters were incorrectly predicted to repay; that’s terrible. Compare this to only 7/65 actual re-payers who were predicted to default.\n\ntree_credit_pred &lt;- predict(tree_credit_train, credit_test, type = \"class\")\n\n\nCrossTable(credit_test$default, tree_credit_pred,\n           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,\n           dnn = c('actual default', 'predicted default'))\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  100 \n\n \n               | predicted default \nactual default |        no |       yes | Row Total | \n---------------|-----------|-----------|-----------|\n            no |        58 |         7 |        65 | \n               |     0.580 |     0.070 |           | \n---------------|-----------|-----------|-----------|\n           yes |        28 |         7 |        35 | \n               |     0.280 |     0.070 |           | \n---------------|-----------|-----------|-----------|\n  Column Total |        86 |        14 |       100 | \n---------------|-----------|-----------|-----------|\n\n \n\n\n\n\nAdding a loss (aka, penalty, cost) matrix\nIt’s really easy to impose a penalty matrix. We will make the false negative three times more costly than a false positive. As desired, the false negatives flip with huge improvement: the updated model correctly traps 28/35 defaults with only 7/35 false negatives. But this comes with an equally huge trade-off: false positives jump from 7/65 to 27 out of 65 who are predicted to default but actually repay.\n\npenalty_matrix &lt;- matrix(c(0, 3,   # Actual: No\n                           1, 0),  # Actual: Yes\n                         ncol=2)\n\nrownames(penalty_matrix) &lt;- colnames(penalty_matrix) &lt;- c(\"No\", \"Yes\")\n\ntree_credit_cost_train &lt;- rpart(default ~ ., data = credit_train,\n                           parms = list(split = \"gini\", loss=penalty_matrix),\n                           control = rpart.control(minsplit = 1, \n                                                   minbucket = 1,\n                                                   maxdepth = 4))\n\ntree_credit_cost_pred &lt;- predict(tree_credit_cost_train, credit_test, type = \"class\")\n\nCrossTable(credit_test$default, tree_credit_cost_pred,\n           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,\n           dnn = c('actual default', 'predicted default'))\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  100 \n\n \n               | predicted default \nactual default |        no |       yes | Row Total | \n---------------|-----------|-----------|-----------|\n            no |        38 |        27 |        65 | \n               |     0.380 |     0.270 |           | \n---------------|-----------|-----------|-----------|\n           yes |         7 |        28 |        35 | \n               |     0.070 |     0.280 |           | \n---------------|-----------|-----------|-----------|\n  Column Total |        45 |        55 |       100 | \n---------------|-----------|-----------|-----------|\n\n \n\n\n\n\nCan I easily randomize?\nI’m interested in the fact that decision trees have random qualities (aside from sampling variation). Below I set a different seed and switched the split algo to entropy. But the ultimate tree is the same.\n\n# different see and switch gini to information; aka, entropy\nset.seed(448)\n\ntree_credit_train_2 &lt;- rpart(default ~ ., data = credit_train,\n                           parms = list(split = \"information\"),\n                           control = rpart.control(minsplit = 1, \n                                                   minbucket = 1,\n                                                   maxdepth = 4))\n\nrpart.plot(tree_credit_train_2, yesno = 2, left=FALSE, type=2, branch.lty = 3, nn= TRUE, \n           box.palette = c(\"palegreen\", \"pink\"), leaf.round=0, extra = 101, digits = 4)\n\n\n\nprint(tree_credit_train_2)\n\nn= 900 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 900 265 no (0.7055556 0.2944444)  \n   2) checking_balance=&gt; 200 DM,unknown 415  55 no (0.8674699 0.1325301) *\n   3) checking_balance=&lt; 0 DM,1 - 200 DM 485 210 no (0.5670103 0.4329897)  \n     6) credit_history=critical,poor,good 426 167 no (0.6079812 0.3920188)  \n      12) months_loan_duration&lt; 27.5 324 105 no (0.6759259 0.3240741)  \n        24) amount&lt; 9899.5 317  98 no (0.6908517 0.3091483) *\n        25) amount&gt;=9899.5 7   0 yes (0.0000000 1.0000000) *\n      13) months_loan_duration&gt;=27.5 102  40 yes (0.3921569 0.6078431)  \n        26) dependents&gt;=1.5 14   4 no (0.7142857 0.2857143) *\n        27) dependents&lt; 1.5 88  30 yes (0.3409091 0.6590909) *\n     7) credit_history=very good,perfect 59  16 yes (0.2711864 0.7288136)  \n      14) age&lt; 22.5 3   0 no (1.0000000 0.0000000) *\n      15) age&gt;=22.5 56  13 yes (0.2321429 0.7678571) *\n\nprintcp(tree_credit_train_2)\n\n\nClassification tree:\nrpart(formula = default ~ ., data = credit_train, parms = list(split = \"information\"), \n    control = rpart.control(minsplit = 1, minbucket = 1, maxdepth = 4))\n\nVariables actually used in tree construction:\n[1] age                  amount               checking_balance    \n[4] credit_history       dependents           months_loan_duration\n\nRoot node error: 265/900 = 0.29444\n\nn= 900 \n\n        CP nsplit rel error  xerror     xstd\n1 0.050943      0   1.00000 1.00000 0.051599\n2 0.026415      3   0.81509 0.87547 0.049518\n3 0.022642      4   0.78868 0.86038 0.049236\n4 0.011321      5   0.76604 0.86415 0.049307\n5 0.010000      6   0.75472 0.86038 0.049236\n\nidentical(tree_credit_train, tree_credit_train_2)\n\n[1] FALSE\n\nall.equal(tree_credit_train, tree_credit_train_2)\n\n[1] \"Component \\\"call\\\": target, current do not match when deparsed\"                                \n[2] \"Component \\\"cptable\\\": Mean relative difference: 0.04736418\"                                   \n[3] \"Component \\\"parms\\\": Component \\\"split\\\": Mean relative difference: 1\"                         \n[4] \"Component \\\"splits\\\": Attributes: &lt; Component \\\"dimnames\\\": Component 1: 4 string mismatches &gt;\"\n[5] \"Component \\\"splits\\\": Mean relative difference: 5.545437\"                                      \n[6] \"Component \\\"csplit\\\": Attributes: &lt; Component \\\"dim\\\": Mean relative difference: 0.04761905 &gt;\" \n[7] \"Component \\\"csplit\\\": Numeric: lengths (126, 120) differ\"                                      \n[8] \"Component \\\"variable.importance\\\": Names: 2 string mismatches\"                                 \n[9] \"Component \\\"variable.importance\\\": Mean relative difference: 0.1894674\""
  },
  {
    "objectID": "posts/logistic-coeff/index.html",
    "href": "posts/logistic-coeff/index.html",
    "title": "Logistic regression coefficients",
    "section": "",
    "text": "I wanted to shadow GARP’s logistic regression example, so I sampled from the same LendingClub database and performed a similar logistic regression. The key difference is practical: I’ll often re-sample from the data in order to get a result that lends itself to a good practice question. I’ve been writing practice questions for a long time, and there are many little details that go into this. For example, GARP’s logistic regression shows 10 independent variables, and I reduced that to seven merely because I don’t need to show all the variables to make the point.\nAfter I seeded the result that appealed to me, I wrote the practice question below (the published question sans answer is here). After fiddling with the four choices, I’m happy with the final question. It’s an “EXCEPT FOR” question, which is what I often use when I’m trying to blanked the concept more comprehensively than a “TRUE” question. This is a bit more work because each distractor must be carefully written.\n\n23.6.1. Darlene is a risk analyst who evaluate the creditworthiness of loan applicants at her financial institution. Her department is testing a new logistic regression model. If the model performs well in testing, it will be deployed to assist in the underwriting decision-making process. The training data is a sub-sample (n = 800) from the same LendingClub database used in reading. In the logistic regression, the dependent variable is a 0/1 for the terminal state of the loan being either zero (fully paid off) or one (deemed irrecoverable or defaulted). In the actual code, this dependent variable is labeled ‘outcome’.\nThe following are the features (aka, independent variables) as given by their textual labels: Amount, Term, Interest_rate, Installment, Employ_hist, Income, and Bankruptcies. In regard to units in the database, please note the following: Amount is thousands of dollars ($000s); Term is months; Interest_rate is effectively multiplied by one hundred such that 7 equates to 7% or 0.070; Installment is dollars; Employment_hist is years; Income is thousand of dollars ($000); and Bankruptcies is a whole number {0, 1, 2, …}.\nThe table below displays the logistic regression results:\n&lt;&lt;See regression output below; table will paste here&gt;&gt;\n\n\nIn regard to this logistic regression, each of the following statements is true EXCEPT which is false?\n\nA single additional bankruptcy increases the expected odds of default by almost 58 percent\nIf she requires significance at the 5% level or better, then two of the coefficients (in addition to the intercept) are significant\nEach +100 basis points increase in the interest rate (e.g., from 8.0% to 9.0%) implies an increase of about 14.0 basis points in the default probability\nIf the cost of making a bad loan is high, she can decrease the threshold (i.e., set Z to a low value such as 0.05), but this will reject more good borrowers\n\n\nHere is the code with some comments. The logistic model itself, a type of glm(), requires only a single line and the model is stored in logit_model_1 as a list object. Most of my code is re-coding the dataset, and then rendering the model’s coefficients with the awesome gt package. Posit’s Richard Iannone does an incredible job maintaining the package. If you think about it, generating tables are really important in data!\n\nlibrary(tidyverse) \nlibrary(gt)\n# library(labelled) Didn't use but helpful\n\n# set.seed(xzy)\nset.seed(374)\n\nsample_size &lt;- 800\nlcfeatures &lt;- read_csv(\"lcfeatures.csv\") \n# Same LendingClub dataset used in FRM Chapter 15 (Logistic Regression Example)\n# Located at https://www.kaggle.com/datasets/wordsforthewise/lending-club\n# But lcfeatures is a random sample of 10,000 which is too large for my need\n# So I just sample_n as random subset of the 10,000\nlcfeatures &lt;- lcfeatures |&gt; sample_n(sample_size)\n\n# recoding \nlcfeatures$emp_length_n &lt;- gsub(\"&lt; 1\", \"0\", lcfeatures$emp_length)\nlcfeatures$emp_length_n2 &lt;- parse_number(lcfeatures$emp_length_n)\nlcfeatures$term_n &lt;- parse_number(lcfeatures$term)\n\nlcfeatures$home_ownership_simpler &lt;- recode(lcfeatures$home_ownership,\n                                             \"MORTGAGE\" = \"OWN\",\n                                             \"ANY\" = \"RENT\",\n                                             \"NONE\" = \"RENT\")\n\nlcfeatures$mortgage_simpler &lt;- recode(lcfeatures$home_ownership,\n                                       \"OWN\" = \"NO\",\n                                       \"ANY\" = \"NO\",\n                                       \"NONE\" = \"NO\",\n                                       \"RENT\" = \"NO\",\n                                       \"MORTGAGE\" = \"YES\")\n\nlcfeatures$loan_status_coded &lt;- recode(lcfeatures$loan_status,\n                                        \"Charged Off\" = \"Default\",\n                                        \"Does not meet the credit policy. Status:Charged Off\" = \"Default\",\n                                        \"Late (31-120 days)\" = \"Default\",\n                                        .default = \"Paid\")\n\nlcfeatures$home_ownership_bern &lt;- recode(lcfeatures$home_ownership_simpler,\n                                          \"RENT\" = 0,\n                                          \"OWN\" = 1)\n\nlcfeatures$mortgage_bern &lt;- recode(lcfeatures$mortgage_simpler,\n                                          \"NO\" = 0,\n                                          \"YES\" = 1)\n\nlcfeatures$loan_status_bern &lt;- recode(lcfeatures$loan_status_coded,\n                                          \"Paid\" = 0,\n                                          \"Default\" = 1)\n\nlcfeatures$loan_amnt_000 &lt;- lcfeatures$loan_amnt / 1000\nlcfeatures$annual_inc_000 &lt;- lcfeatures$annual_inc / 1000\nlcfeatures$outcome &lt;- lcfeatures$loan_status_bern\n\n# This is logistic regression model\nlogit_model_1 &lt;- glm(formula = outcome ~ loan_amnt_000 + term_n + int_rate + installment + \n        emp_length_n2 + annual_inc_000 + pub_rec_bankruptcies,\n        family = binomial(link = \"logit\"), data = lcfeatures)\n\ncoef_table &lt;- coef(summary(logit_model_1)) \ncoef_tbl  &lt;-  as_tibble(coef_table)\nCoeff_labels &lt;- c(\"(Intercept)\", \"Amount\", \"Term\", \"Interest_rate\", \"Installment\", \n                 \"Employment_hist\", \"Income\",\"Bankruptcies\")\ncoef_tbl &lt;- cbind(Coeff_labels, coef_tbl)\n\n# Using gt() to render a table\ncoef_tbl_gt &lt;- coef_tbl %&gt;% gt() |&gt; \n    opt_table_font(stack = \"humanist\") |&gt;\n    fmt_number(columns = everything(),\n               decimals = 3)\ncoef_tbl_gt\n\n\n\n\n\n  \n    \n    \n      Coeff_labels\n      Estimate\n      Std. Error\n      z value\n      Pr(&gt;|z|)\n    \n  \n  \n    (Intercept)\n−2.329\n0.841\n−2.769\n0.006\n    Amount\n0.123\n0.092\n1.339\n0.181\n    Term\n−0.041\n0.027\n−1.519\n0.129\n    Interest_rate\n0.140\n0.034\n4.108\n0.000\n    Installment\n−0.003\n0.003\n−1.033\n0.302\n    Employment_hist\n−0.032\n0.031\n−1.025\n0.305\n    Income\n−0.003\n0.003\n−0.937\n0.349\n    Bankruptcies\n0.457\n0.230\n1.991\n0.046\n  \n  \n  \n\n\n\n\nIf we use predict() with type = “response”, then the logistic regression returns the vector of predicted probabilities (from zero to 100%). We can classify the Bernoulli prediction (0 = nondefault, 1 = default) as a function of our desired conservative/aggressive threshold. Below I show the number of rejections would increase as we lower the threshold.\n\npredicted_probs &lt;- predict(logit_model_1, lcfeatures, type = \"response\")\nthresholds &lt;- c(0.4, 0.3, 0.2, 0.1, 0.05, 0.010)\nthresholds |&gt; map_int(\\(x) sum(ifelse(predicted_probs &gt; x, 1, 0), na.rm = TRUE))\n\n[1]   5  30  86 383 689 748\n\n\nInspired by this blog post on color coding the {gt} table, I added some color to highlight the significant coefficients (obviously not in the actual Q&A, just here!):\n\ncoef_tbl_gt |&gt; \n    data_color(\n        columns = 'Pr(&gt;|z|)', \n        palette = c(\"#19F000\",\"#E4FF00\"),\n        domain = c(0,0.05),\n        na_color = \"lightgrey\"\n    )\n\n\n\n\n\n  \n    \n    \n      Coeff_labels\n      Estimate\n      Std. Error\n      z value\n      Pr(&gt;|z|)\n    \n  \n  \n    (Intercept)\n−2.329\n0.841\n−2.769\n0.006\n    Amount\n0.123\n0.092\n1.339\n0.181\n    Term\n−0.041\n0.027\n−1.519\n0.129\n    Interest_rate\n0.140\n0.034\n4.108\n0.000\n    Installment\n−0.003\n0.003\n−1.033\n0.302\n    Employment_hist\n−0.032\n0.031\n−1.025\n0.305\n    Income\n−0.003\n0.003\n−0.937\n0.349\n    Bankruptcies\n0.457\n0.230\n1.991\n0.046"
  },
  {
    "objectID": "posts/nearest-neighbors/index.html",
    "href": "posts/nearest-neighbors/index.html",
    "title": "Nearest neighbors",
    "section": "",
    "text": "Contents\n\nVisualizing nearest neighbors in two-dimensional feature space (simulated borrow default based on credit score and income)\nParallel coordinates plot to visualize nearest neighbors in multi-dimensional feature space (Wisconsin breast cancer dataset with 30 features)\n\nFirst the libraries:\n\nmy_libraries &lt;- c(\"tidyverse\", \"ggforce\", \"janitor\", \"openxlsx\", \"patchwork\", # for mine\n                  \"class\", \"GGally\", \"viridis\") # for Lantz's data\nlapply(my_libraries, library, character.only = TRUE)\n\nlibrary(class)\nlibrary(GGally)\nlibrary(viridis)\n\n\nPredicting loan default based on vote of nearest neighbors\nBecause it’s easy to visualize, my first example is simulated data in a two-dimensional feature space. The training set is 100 borrowers with credit scores and incomes. This is supervised learning: the borrowers either defaulted or repaid. The single test point (see blue triangle below) is a borrower with a credit score of 605 and income of $41,000.\n\nset.seed(743) \nn &lt;- 100 \n\ncredit_scores &lt;- rnorm(n, mean=650, sd=50)\nincomes &lt;- rnorm(n, mean=50000, sd=10000)\n\n# Default if credit score is below 600 OR income is below $40,000\nlabels &lt;- ifelse(credit_scores &lt; 600 | incomes &lt; 40000, \"default\", \"repay\")\n\n# But switch some \"repay\" to \"default\" to add noise\nrandom_indices &lt;- sample(1:n, n/10) # Arbitrary 10%\nlabels[random_indices] &lt;- \"default\"\n\ntrain &lt;- data.frame(credit_score=credit_scores, income=incomes, label=labels)\n# In k-nn, we should either standardize or normalize the data\nmu_credit &lt;- mean(train$credit_score); sig_credit &lt;- sd(train$credit_score)\nmu_income &lt;- mean(train$income); sig_income &lt;- sd(train$income)\ntrain$credit_score_std &lt;- (train$credit_score - mu_credit) / sig_credit\ntrain$income_std &lt;- (train$income - mu_income) / sig_income\n\n# The test point; then standardized\nx &lt;- 605\ny &lt;- 41000\nx_std &lt;- (x - mu_credit) / sig_credit\ny_std &lt;- (y - mu_income) / sig_income\n\n# Euclidean distance (from all points) to test point\ndistances_std &lt;- sqrt((train$credit_score_std - x_std)^2 + (train$income_std - y_std)^2)\n\n# The k-nearest neighbors are simply the k (=5 or =10 or =15, eg) smallest distances\nk05 &lt;- 5; k10 &lt;- 10; k15 &lt;- 15\n\nk_nearest_indices_std_05 &lt;- order(distances_std)[1:k05]\nk_nearest_indices_std_10 &lt;- order(distances_std)[1:k10]\nk_nearest_indices_std_15 &lt;- order(distances_std)[1:k15]\n\n# Add distances column and display  k-nearest neighbors with their distance\nk_nn &lt;- train[k_nearest_indices_std_15, ]\nnearest &lt;- distances_std[k_nearest_indices_std_15]\nk_nn$distance &lt;- nearest\n\n# k_nearest_neighbors\nk_nn |&gt; adorn_rounding(digits = 0, rounding = \"half up\", \n                                      all_of(c(\"credit_score\", \"income\"))) |&gt; \n    adorn_rounding(digits = 3, rounding = \"half up\", credit_score_std:distance)\n\n   credit_score income   label credit_score_std income_std distance\n8           611  41988   repay           -0.804     -0.759    0.145\n82          598  39398 default           -1.035     -1.016    0.201\n41          603  43184   repay           -0.951     -0.641    0.220\n4           592  39166 default           -1.150     -1.039    0.300\n64          604  44168   repay           -0.932     -0.543    0.315\n32          587  41994 default           -1.243     -0.759    0.345\n75          621  44367   repay           -0.618     -0.523    0.445\n53          627  38749 default           -0.513     -1.081    0.457\n1           583  46197 default           -1.307     -0.342    0.650\n45          598  34567 default           -1.045     -1.496    0.652\n11          639  43148   repay           -0.282     -0.644    0.665\n80          640  43630   repay           -0.263     -0.596    0.699\n84          643  43094   repay           -0.219     -0.650    0.723\n52          646  41064   repay           -0.156     -0.851    0.756\n99          639  45497   repay           -0.295     -0.411    0.761\n\n# Now the ggplots!\n# colors\nthree_colors &lt;- c(\"default\" = \"lightpink1\", \"repay\" = \"lightgreen\")\nfive_colors &lt;- c(\"default\" = \"lightpink1\", \"repay\" = \"lightgreen\", \"NN Default\" = \"red\", \"NN Repay\" = \"green4\")    \n\n# Base plots, with labels and without (and zoomed in per coord_cartesian)\np_base_lab &lt;- ggplot(train, aes(x=credit_score_std, y=income_std)) +\n    geom_point(aes(x=x_std, y=y_std), color=\"dodgerblue2\", shape=17, size=4) +\n    xlab(\"Credit Score (Standardized)\") +\n    ylab(\"Income (Standardized)\") +\n    theme_minimal()\n\np_base &lt;- ggplot(train, aes(x=credit_score_std, y=income_std)) + \n    geom_point(aes(x=x_std, y=y_std), color=\"dodgerblue2\", shape=17, size=4) +\n    theme_minimal() +\n    theme(legend.position = \"none\", axis.title = element_blank()) +\n    coord_cartesian(xlim = c(-1.75, 0), ylim = c(-1.75, 0))\n\np1_lab &lt;- p_base_lab + \n    geom_point(aes(color = label), size = 3) + \n    labs(title = paste(\"The majority of how many k neighbors?\"),\n         subtitle = paste(\"Blue triangle is Test point\"),\n         color = \"Borrower\") +\n    scale_color_manual(values = three_colors)\n\np3_lab &lt;- p_base_lab +\n    geom_point(aes(color = ifelse(row.names(train) %in% row.names(train[k_nearest_indices_std_10, ]),\n                                  ifelse(label == \"default\", \"NN Default\", \"NN Repay\"),\n                                  label)), size = 3) +\n    labs(title = paste(\"Let's ask k = 10 neighbors to vote\"),\n         subtitle = paste(\"Six defaulted and four repaid (radius is ~0.652)\"),\n         color = \"Borrower\") +\n    geom_circle(aes(x0 = x_std, y0 = y_std, r = 0.658), \n              color = \"blue\",linetype=\"dashed\", fill = NA) +\n    scale_color_manual(values = five_colors)\n\np1_lab\n\n\n\np3_lab\n\n\n\np1 &lt;- p_base + \n    geom_point(aes(color = label), size = 3) +\n    scale_color_manual(values = three_colors)\n\np2 &lt;- p_base + \n    geom_point(aes(color = ifelse(row.names(train) %in% row.names(train[k_nearest_indices_std_05, ]),\n                                  ifelse(label == \"default\", \"NN Default\", \"NN Repay\"),\n                                  label)), size = 3) +\n    geom_circle(aes(x0 = x_std, y0 = y_std, r = 0.330), \n              color = \"blue\",linetype=\"dashed\", fill = NA) +\n    scale_color_manual(values = five_colors) \n\n p3 &lt;- p_base +\n    geom_point(aes(color = ifelse(row.names(train) %in% row.names(train[k_nearest_indices_std_10, ]),\n                                  ifelse(label == \"default\", \"NN Default\", \"NN Repay\"),\n                                  label)), size = 3) +\n    geom_circle(aes(x0 = x_std, y0 = y_std, r = 0.658), \n              color = \"blue\",linetype=\"dashed\", fill = NA) +\n    scale_color_manual(values = five_colors)\n\n p4 &lt;- p_base +\n    geom_point(aes(color = ifelse(row.names(train) %in% row.names(train[k_nearest_indices_std_15, ]),\n                                  ifelse(label == \"default\", \"NN Default\", \"NN Repay\"),\n                                  label)), size = 3) +\n     geom_circle(aes(x0 = x_std, y0 = y_std, r = 0.763), \n              color = \"blue\",linetype=\"dashed\", fill = NA) +\n    scale_color_manual(values = five_colors) \n\n(p1 | p2) / (p3 | p4) + \n     plot_annotation(title = \"Top: None and k = 5, Bottom: k = 10 and k = 15\", \n                     subtitle = \"From repay (3/5) to default (6/10) to repay (9/15)\")\n\n\n\n\n\n\nPredicting default based on vote of nearest neighbors\nMost datasets have many features. I quickly tried a few experiments to visualize multidimensional neighbors. At this point, my favorite is the parallel coordinates plot below. I’ll use the dataset from my favorite machine learning introduction: Machine Learning with R by Brent Lantz, 4th Edition. He did not attempt to visualize this nearest neighbor’s example.\nWe’re using the Wisconsin Breast Cancer Dataset. The dataset has 569 observations and 30 numeric features that describe characteristics of the cell nuclei present in the image. The target variable is the diagnosis (benign or malignant).\nBrett Lantz parses the dataset into 469 training observation and 100 test observations. Please note that these features are normalized (i.e., on a zero to one scale) rather than standardized (as I did above). Below, I retrieve the first test instance and plot its two nearest (Euclidean) neighbors in the training set. Although this does not convey numerical distance (obviously), I think it’s a fine way to illustrate the proximity of the features.\n\nload(\"wbcd_dfs.RData\") # wbcd_train, wbcd_train_labels, wbcd_test, wbcd_test_labels\n# I previously retrieved the nearest neighbors to the single test instance\n# k_nearest neighbors &lt;- function(test_instance, train_data, k)\n# save(k_neighbors, file = \"k_neighbors.RData\")\nload(\"k_neighbors.RData\") # k_neighbors\n\nstr(wbcd_train)\n\n'data.frame':   469 obs. of  30 variables:\n $ radius_mean      : num  0.253 0.171 0.192 0.203 0.389 ...\n $ texture_mean     : num  0.0906 0.3125 0.2408 0.1245 0.1184 ...\n $ perimeter_mean   : num  0.242 0.176 0.187 0.202 0.372 ...\n $ area_mean        : num  0.136 0.0861 0.0974 0.1024 0.2411 ...\n $ smoothness_mean  : num  0.453 0.399 0.497 0.576 0.244 ...\n $ compactness_mean : num  0.155 0.292 0.18 0.289 0.153 ...\n $ concavity_mean   : num  0.0934 0.1496 0.0714 0.1086 0.0795 ...\n $ points_mean      : num  0.184 0.131 0.123 0.238 0.132 ...\n $ symmetry_mean    : num  0.454 0.435 0.33 0.359 0.334 ...\n $ dimension_mean   : num  0.202 0.315 0.283 0.227 0.115 ...\n $ radius_se        : num  0.0451 0.1228 0.0309 0.0822 0.0242 ...\n $ texture_se       : num  0.0675 0.1849 0.2269 0.2172 0.0116 ...\n $ perimeter_se     : num  0.043 0.1259 0.0276 0.0515 0.0274 ...\n $ area_se          : num  0.0199 0.0379 0.0126 0.0365 0.0204 ...\n $ smoothness_se    : num  0.215 0.196 0.117 0.325 0.112 ...\n $ compactness_se   : num  0.0717 0.252 0.0533 0.2458 0.0946 ...\n $ concavity_se     : num  0.0425 0.0847 0.0267 0.0552 0.0392 ...\n $ points_se        : num  0.235 0.259 0.142 0.372 0.173 ...\n $ symmetry_se      : num  0.16 0.382 0.131 0.111 0.121 ...\n $ dimension_se     : num  0.0468 0.0837 0.045 0.088 0.0301 ...\n $ radius_worst     : num  0.198 0.141 0.159 0.142 0.294 ...\n $ texture_worst    : num  0.0965 0.291 0.3843 0.0999 0.0989 ...\n $ perimeter_worst  : num  0.182 0.139 0.147 0.13 0.269 ...\n $ area_worst       : num  0.0894 0.0589 0.0703 0.0611 0.1558 ...\n $ smoothness_worst : num  0.445 0.331 0.434 0.433 0.274 ...\n $ compactness_worst: num  0.0964 0.2175 0.1173 0.1503 0.142 ...\n $ concavity_worst  : num  0.0992 0.153 0.0852 0.0692 0.1088 ...\n $ points_worst     : num  0.323 0.272 0.255 0.296 0.281 ...\n $ symmetry_worst   : num  0.249 0.271 0.282 0.106 0.182 ...\n $ dimension_worst  : num  0.0831 0.1366 0.1559 0.084 0.0828 ...\n\n# this knn() function is from the class package\n# and it classifies the test set; e.g., 1st is classified as Benign\nwbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test,\n                      cl = wbcd_train_labels, k = 21)\nwbcd_test_pred[1]\n\n[1] Benign\nLevels: Benign Malignant\n\n# inserting first instance at top of training set for graph\nwbcd_train &lt;- rbind(wbcd_test[1, ], wbcd_train) # 469 + 1 = 470\n\nwbcd_train$group &lt;- \"Others\"\nwbcd_train$group[1] &lt;- \"Test Instance\"\nobs_2_index &lt;- k_neighbors[1] + 1\nwbcd_train$group[obs_2_index] &lt;- \"Nearest #1\"\nobs_3_index &lt;- k_neighbors[2] + 1\nwbcd_train$group[obs_3_index] &lt;- \"Nearest #2\"\n\n# set.seed(479)\nset.seed(48514)\n\n# Set the row indices you want to include\nrow1 &lt;- 1\nrow2 &lt;- obs_2_index\nrow3 &lt;- obs_3_index\n\n# Number of random rows to sample\nn &lt;- 10\n\n# Sample without the specific rows, then combine with the specific rows\nsampled_indices &lt;- sample(setdiff(1:nrow(wbcd_train), c(row1, row2, row3)), n)\nfinal_sample &lt;- rbind(wbcd_train[c(row1, row2, row3), ], wbcd_train[sampled_indices, ])\n\nfinal_sample |&gt; ggparcoord(columns = 1:30, \n                           groupColumn = \"group\",\n                           showPoints = TRUE,\n                           alphaLines = 0.3,\n                           scale = \"uniminmax\") +\n    scale_color_manual(values = c(\"Test Instance\" = \"blue\",\n                                  \"Nearest #1\" = \"green4\",\n                                  \"Nearest #2\" = \"green4\",\n                                  \"Others\" = \"yellow\")) + \n    theme_minimal() +\n    labs(title = \"Parallel Coordinates Plot\") +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    coord_flip()"
  },
  {
    "objectID": "posts/ppc_v2/index.html",
    "href": "posts/ppc_v2/index.html",
    "title": "Applying my PPC data model",
    "section": "",
    "text": "Contents\nlibrary(tidyverse); library(patchwork); library (ggcorrplot); library(gt)\nlibrary(tidyquant)\nlibrary(MASS) # write.matrix\n# library(dplyr); library(tidyr); library(purrr)\n# library(ggplot)"
  },
  {
    "objectID": "posts/ppc_v2/index.html#define-the-potential-portfolios-sets-of-assets",
    "href": "posts/ppc_v2/index.html#define-the-potential-portfolios-sets-of-assets",
    "title": "Applying my PPC data model",
    "section": "Define the potential portfolios (= sets of assets)",
    "text": "Define the potential portfolios (= sets of assets)\nThe container in this approach is stock_sets, a dataframe we initialize (as our TOC) with three columns:\n\nset_id\ndescription\nsymbols: a list of tickers\n\nEach row of the stock_sets dataframe hold an entire portfolio including these identifiers plus the returns (via nested_data), and the analysis. The latter two are added as list-columns which are versatile objects: list-items can include dataframes, vectors, and even other lists.\n\n# First row of stock_sets is (portfolio of) sector ETFs:\nsector_11etf_list &lt;- c(\"XLK\",  # Technology\n                       \"XLV\",  # Health Care\n                       \"XLF\",  # Financials\n                       \"XLY\",  # Consumer Discretionary\n                       \"XLP\",  # Consumer Staples\n                       \"XLE\",  # Energy\n                       \"XLU\",  # Utilities\n                       \"XLI\",  # Industrials\n                       \"XLB\",  # Materials\n                       \"XLRE\", # Real Estate\n                       \"XLC\") # Communication Services\n\n# Second row of stock_sets is (portfolio of) style ETFs:\nsize_style_etfs &lt;- c(\"IWF\",  # Large-Cap Growth\n                     \"IWD\",  # Large-Cap Value\n                     \"SPY\",  # Large-Cap Blend\n                     \"IWP\",  # Mid-Cap Growth\n                     \"IWS\",  # Mid-Cap Value\n                     \"MDY\",  # Mid-Cap Blend\n                     \"IWO\",  # Small-Cap Growth\n                     \"IWN\",  # Small-Cap Value\n                     \"IWM\")  # Small-Cap Blend\n\n# Third row is largest (by market cap) company in each sector:\nlarge_mid_caps &lt;- read_csv(\"large_mid_caps.csv\")\nlarge_mid_caps &lt;- large_mid_caps |&gt; rename_at('Market Cap', ~ 'Capitalization')\nlarge_mid_caps$Industry &lt;- as_factor(large_mid_caps$Industry)\nlarge_mid_caps$Sector &lt;- as_factor(large_mid_caps$Sector)\n\n# select the largest (by Market Cap) in each Industry\ntop_in_sector &lt;- large_mid_caps |&gt; \n    group_by(Sector) |&gt; \n    arrange(desc(Capitalization)) |&gt; \n    slice(1)\nremove_rows &lt;- c(7, 8, 15, 17)\ntop_in_sector &lt;- top_in_sector[-remove_rows,]\n\n# This is the essential stock_sets dataframe\nstock_sets &lt;- tibble(\n    set_id = c(\"11_sectors\",\n               \"9_styles\",\n               \"13_top_in_sector\"),\n    \n    description = c(\"All 11 Sectors\",\n                    \"All 9 Styles\",\n                    \"Top Market Cap in each of 13 Sectors\"), \n    \n    # this is a list column, see https://adv-r.hadley.nz/vectors-chap.html#list-columns \n    symbols = list(sector_11etf_list, size_style_etfs, top_in_sector$Ticker)\n    )\n\ndate_start &lt;- \"2013-01-01\"\ndate_end   &lt;- \"2023-11-17\""
  },
  {
    "objectID": "posts/ppc_v2/index.html#retrieve-returns-for-each-frequency-aka-periodicity",
    "href": "posts/ppc_v2/index.html#retrieve-returns-for-each-frequency-aka-periodicity",
    "title": "Applying my PPC data model",
    "section": "Retrieve returns for each frequency; aka, periodicity",
    "text": "Retrieve returns for each frequency; aka, periodicity\nEach portfolio is a set of tickers/symbols. For each of the portfolio’s symbols (aka, tickers), the get_returns function retrieves log returns into three dataframes; one for each frequency:\n\ndaily\nweekly\nmonthly\n\nWe will call the get_returns function via map (purrr is one of my favorite packagse!) to create a new list column called nested_data. Each row of nested_data will contain a list of three dataframes, one for each period. These dataframes will contain the log returns for each ticker in the set.\n\n# This will add the nested_data list-column. For example, we\n# can use stock_sets$nested_data[[n]]$daily$daily.returns to\n# retrieve daily.returns for the nth portfolio\n\nget_returns &lt;- function(symbols, start_date, end_date) {\n    mult_stocks &lt;- tq_get(symbols, get = \"stock.prices\", \n                          from = start_date, to = end_date)\n    periods &lt;- c(\"daily\", \"weekly\", \"monthly\")\n    returns_list &lt;- lapply(periods, function(period) {\n        mult_stocks |&gt; \n            group_by(symbol) |&gt; \n            tq_transmute(select = adjusted,\n                         mutate_fun = periodReturn, \n                         period = period, \n                         type = \"log\")\n    })\n    \n    names(returns_list) &lt;- periods\n    return(returns_list)\n}\n\n# Nest return data for each stock set\nstock_sets &lt;- stock_sets |&gt; \n    mutate(nested_data = map(symbols, \n                             ~ get_returns(.x, date_start, date_end)))\n\nprint(stock_sets)\n\n# A tibble: 3 × 4\n  set_id           description                          symbols    nested_data \n  &lt;chr&gt;            &lt;chr&gt;                                &lt;list&gt;     &lt;list&gt;      \n1 11_sectors       All 11 Sectors                       &lt;chr [11]&gt; &lt;named list&gt;\n2 9_styles         All 9 Styles                         &lt;chr [9]&gt;  &lt;named list&gt;\n3 13_top_in_sector Top Market Cap in each of 13 Sectors &lt;chr [13]&gt; &lt;named list&gt;"
  },
  {
    "objectID": "posts/ppc_v2/index.html#add-analysis-list-column-volatility-avg-return-correlation-matrix",
    "href": "posts/ppc_v2/index.html#add-analysis-list-column-volatility-avg-return-correlation-matrix",
    "title": "Applying my PPC data model",
    "section": "Add analysis list-column: volatility, avg return, correlation matrix",
    "text": "Add analysis list-column: volatility, avg return, correlation matrix\nFor each portfolio (set of tickers) and periodicity, the analysis list column generates:\n\nvector of volatilities\nvector of average returns\ncorrelation matrix (diagonal is 1)\naverage correlation (as a rough measure of diversification)\n\n\n# For example, we can use stock_sets$analysis[[1]]$monthly$corr_matrix\n# to retrieve the correlation matrix for the nth portfolio\n\nperform_analysis &lt;- function(data, frequency) {\n    # Determine the column name for returns based on the frequency\n    returns_col &lt;- switch(frequency,\n                          \"daily\" = \"daily.returns\",\n                          \"weekly\" = \"weekly.returns\",\n                          \"monthly\" = \"monthly.returns\",\n                          stop(\"Invalid frequency\"))\n\n    # Calculate volatilities\n    volatilities &lt;- data |&gt; \n        group_by(symbol) |&gt; \n        summarise(volatility = sd(.data[[returns_col]], na.rm = TRUE)) |&gt; \n        ungroup()\n\n    # Calculate average returns\n    avg_returns &lt;- data |&gt; \n        group_by(symbol) |&gt; \n        summarise(avg_return = mean(.data[[returns_col]], na.rm = TRUE)) |&gt; \n        ungroup()\n\n    # Pivot to wide format for correlation matrix calculation\n    data_wide &lt;- data |&gt; \n        group_by(date) |&gt; \n        pivot_wider(names_from = symbol, values_from = .data[[returns_col]])\n    \n    # Write files to verify (in Excel) the correlation matrix\n    # write.csv(data_wide, paste0(\"data_wide_\", frequency, \".csv\"), row.names = FALSE)\n\n    # Calculate the correlation matrix\n    corr_matrix &lt;- cor(data_wide[ ,-1], use = \"complete.obs\")\n\n    # write.csv(as.data.frame(corr_matrix), paste0(\"corr_matrix_\", frequency, \".csv\"), row.names = TRUE) \n    \n    # Calculate average correlation\n    avg_corr &lt;- mean(corr_matrix[lower.tri(corr_matrix)])\n\n    return(list(volatilities = volatilities, \n                avg_returns = avg_returns, \n                corr_matrix = corr_matrix, \n                avg_corr = avg_corr))\n}\n\n# Applying the perform_analysis function to the stock_sets\nstock_sets &lt;- stock_sets |&gt; \n    mutate(analysis = map(nested_data, ~ {\n        list(\n            daily = perform_analysis(.x$daily, \"daily\"),\n            weekly = perform_analysis(.x$weekly, \"weekly\"),\n            monthly = perform_analysis(.x$monthly, \"monthly\")\n        )\n    }))\n\n# Examine data structure \nprint(stock_sets)\n\n# A tibble: 3 × 5\n  set_id           description                 symbols nested_data  analysis    \n  &lt;chr&gt;            &lt;chr&gt;                       &lt;list&gt;  &lt;list&gt;       &lt;list&gt;      \n1 11_sectors       All 11 Sectors              &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n2 9_styles         All 9 Styles                &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n3 13_top_in_sector Top Market Cap in each of … &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n\nglimpse(stock_sets)\n\nRows: 3\nColumns: 5\n$ set_id      &lt;chr&gt; \"11_sectors\", \"9_styles\", \"13_top_in_sector\"\n$ description &lt;chr&gt; \"All 11 Sectors\", \"All 9 Styles\", \"Top Market Cap in each …\n$ symbols     &lt;list&gt; &lt;\"XLK\", \"XLV\", \"XLF\", \"XLY\", \"XLP\", \"XLE\", \"XLU\", \"XLI\", \"…\n$ nested_data &lt;list&gt; [[&lt;grouped_df[28057 x 3]&gt;], [&lt;grouped_df[5819 x 3]&gt;], [&lt;g…\n$ analysis    &lt;list&gt; [[[&lt;tbl_df[11 x 2]&gt;], [&lt;tbl_df[11 x 2]&gt;], &lt;&lt;matrix[11 x 1…"
  },
  {
    "objectID": "posts/ppc_v2/index.html#correlation",
    "href": "posts/ppc_v2/index.html#correlation",
    "title": "Applying my PPC data model",
    "section": "Correlation",
    "text": "Correlation\n\nAverage average correlation\n\n# GPT4 basically wrote this entire chunk: I added it while writing substack\n# Prepare a dataframe for the average correlations\navg_corr_data &lt;- tibble(\n    Portfolio = character(),\n    Frequency = character(),\n    Average_Correlation = numeric()\n)\n\n# Loop through each portfolio and each frequency\nfor (i in seq_along(stock_sets$analysis)) {\n    portfolio_id &lt;- stock_sets$set_id[i]\n    for (freq in c(\"daily\", \"weekly\", \"monthly\")) {\n        avg_corr &lt;- stock_sets$analysis[[i]][[freq]]$avg_corr\n        avg_corr_data &lt;- avg_corr_data |&gt; \n            add_row(\n                Portfolio = portfolio_id,\n                Frequency = freq,\n                Average_Correlation = avg_corr\n            )\n    }\n}\n\n# Create a wider table format\navg_corr_wide &lt;- avg_corr_data |&gt; \n    pivot_wider(\n        names_from = Frequency,\n        values_from = Average_Correlation,\n        names_prefix = \"Avg_Corr_\"\n    )\n\n# Add a column for the overall average correlation\navg_corr_wide &lt;- avg_corr_wide |&gt; \n    rowwise() |&gt; \n    mutate(Avg_Corr_Total = mean(c_across(starts_with(\"Avg_Corr_\")), na.rm = TRUE)) |&gt; \n    ungroup()\n\navg_corr_wide |&gt; \n    gt() |&gt; \n    tab_header(title = \"Average Correlation per Frequency\") |&gt; \n    cols_label(\n        Portfolio = \"Portfolio\",\n        Avg_Corr_daily = \"Daily\",\n        Avg_Corr_weekly = \"Weekly\",\n        Avg_Corr_monthly = \"Monthly\",\n        Avg_Corr_Total = \"Avg Avg Corr\"\n    ) |&gt; \n    fmt_number(columns = 2:5, decimals = 3)\n\n\n\n\n\n  \n    \n      Average Correlation per Frequency\n    \n    \n    \n      Portfolio\n      Daily\n      Weekly\n      Monthly\n      Avg Avg Corr\n    \n  \n  \n    11_sectors\n0.676\n0.682\n0.677\n0.678\n    9_styles\n0.899\n0.902\n0.899\n0.900\n    13_top_in_sector\n0.371\n0.344\n0.325\n0.346\n  \n  \n  \n\n\n\n\n\n\nSingle stocks are better diversifiers\n\ncreate_avg_corr_table &lt;- function(monthly_data, title = \"Data Analysis\") {\n    corr_matrix &lt;- monthly_data$corr_matrix\n    symbols &lt;- rownames(corr_matrix)\n    \n    # average correlation for each stock excluding the stock itself\n    avg_correlations_per_stock &lt;- map_dbl(seq_len(nrow(corr_matrix)), function(i) {\n        row_correlations &lt;- corr_matrix[i, ]\n        mean(row_correlations[-i], na.rm = TRUE)\n    })\n    avg_correlations_df &lt;- data.frame(symbol = symbols, \n                                      avg_correlation = avg_correlations_per_stock)\n    combined_data &lt;- left_join(monthly_data$volatilities,\n                               monthly_data$avg_returns, by = \"symbol\") |&gt;\n        left_join(avg_correlations_df, by = \"symbol\")\n\n    gt_table &lt;- combined_data |&gt; \n        gt() |&gt; \n        tab_header(title = title) |&gt; \n        cols_label(\n            symbol = \"Symbol\",\n            volatility = \"Volatility\",\n            avg_return = \"Avg Return\",\n            avg_correlation = \"Correlation(*)\"\n        ) |&gt; \n        fmt_percent(columns = 2:3, decimals = 2) |&gt;\n        fmt_number(columns = 4, decimals = 2) |&gt; \n        tab_footnote(\"(*) Pairwise average(ρ): matrix row/column average excluding diagonal\")\n\n    return(gt_table)\n}\n\nmonthly_data &lt;- stock_sets$analysis[[1]]$monthly\ncreate_avg_corr_table(monthly_data, \"Sectors, Monthly Returns\")\n\n\n\n\n\n  \n    \n      Sectors, Monthly Returns\n    \n    \n    \n      Symbol\n      Volatility\n      Avg Return\n      Correlation(*)\n    \n  \n  \n    XLB\n5.30%\n0.73%\n0.76\n    XLC\n6.09%\n0.58%\n0.69\n    XLE\n8.35%\n0.40%\n0.54\n    XLF\n5.44%\n0.87%\n0.72\n    XLI\n5.12%\n0.91%\n0.77\n    XLK\n5.17%\n1.50%\n0.69\n    XLP\n3.64%\n0.72%\n0.66\n    XLRE\n5.01%\n0.46%\n0.71\n    XLU\n4.35%\n0.70%\n0.55\n    XLV\n3.99%\n1.01%\n0.66\n    XLY\n5.42%\n1.04%\n0.71\n  \n  \n  \n    \n       (*) Pairwise average(ρ): matrix row/column average excluding diagonal\n    \n  \n\n\n\nmonthly_data &lt;- stock_sets$analysis[[3]]$monthly\ncreate_avg_corr_table(monthly_data, \"Largest Stock in Sector, Monthly Returns\")\n\n\n\n\n\n  \n    \n      Largest Stock in Sector, Monthly Returns\n    \n    \n    \n      Symbol\n      Volatility\n      Avg Return\n      Correlation(*)\n    \n  \n  \n    AAPL\n8.06%\n1.85%\n0.35\n    AMZN\n8.68%\n1.84%\n0.29\n    BRK-B\n4.81%\n1.03%\n0.44\n    CMCSA\n6.50%\n0.77%\n0.38\n    LIN\n5.21%\n1.15%\n0.42\n    LLY\n6.47%\n2.08%\n0.11\n    LVMUY\n6.92%\n1.31%\n0.38\n    NEE\n5.59%\n1.11%\n0.21\n    NHYDY\n10.39%\n0.40%\n0.32\n    PLD\n6.40%\n1.07%\n0.37\n    UNP\n6.33%\n1.12%\n0.39\n    WMT\n5.14%\n0.80%\n0.27\n    XOM\n7.52%\n0.46%\n0.28\n  \n  \n  \n    \n       (*) Pairwise average(ρ): matrix row/column average excluding diagonal\n    \n  \n\n\n\n\n\n\nVisualize Correlation\nThese step took me too long because I couldn’t wrestle corrplot/ggcorrplot to my ends. I tried various packages but finally settled on the corrr package due to its elegant/tidy construction; e.g., if you use ggplot, you can guess its parameters. As the authors write,\n\nAt first, a correlation data frame might seem like an unnecessary complexity compared to the traditional matrix. However, the purpose of corrr is to help use explore these correlations, not to do mathematical or statistical operations. Thus, by having the correlations in a data frame, we can make use of packages that help us work with data frames like dplyr, tidyr, ggplot2, and focus on using data pipelines\n\n\nlibrary(corrr)\n\ncolors_corr_plot &lt;- colorRampPalette(c(\"firebrick3\", \"firebrick1\", \"yellow\", \"seagreen1\", \"seagreen4\"))\n\nplot_corr &lt;- function(matrix, title) {\n    matrix_df &lt;- as_cordf(matrix) |&gt; shave()\n    matrix_df |&gt; rplot(shape = 15, print_cor = TRUE,\n                       colors = colors_corr_plot(5),\n                       legend = FALSE) +\n        theme(axis.text.x = element_text(angle = 90, hjust = 1))\n}\n    \nget_corr_matrices &lt;- function(row_df){\n    analyze_this &lt;- row_df |&gt; \n        pull(analysis)\n\n    corr_daily_matrix   &lt;- analyze_this[[1]][[\"daily\"]]$corr_matrix\n    corr_weekly_matrix  &lt;- analyze_this[[1]][[\"weekly\"]]$corr_matrix\n    corr_monthly_matrix &lt;- analyze_this[[1]][[\"monthly\"]]$corr_matrix\n    \n    return(list(\n        corr_daily_matrix = corr_daily_matrix,\n        corr_weekly_matrix = corr_weekly_matrix,\n        corr_monthly_matrix = corr_monthly_matrix\n    ))\n}\n\nselect_row_1 &lt;- stock_sets |&gt; filter(set_id == \"9_styles\")\nthree_matrices &lt;- get_corr_matrices(select_row_1)\ncp1 &lt;- three_matrices$corr_daily_matrix |&gt; plot_corr(\"9_styles, daily\")\n# cp2 &lt;- three_matrices$corr_weekly_matrix |&gt; plot_corr(\"9_styles, weekly\")\ncp3 &lt;- three_matrices$corr_monthly_matrix |&gt; plot_corr(\"9_styles, monthly\")\n\nselect_row_2 &lt;- stock_sets |&gt; filter(set_id == \"11_sectors\")\nthree_matrices &lt;- get_corr_matrices(select_row_2)\ncp4 &lt;- three_matrices$corr_daily_matrix |&gt; plot_corr(\"11_sectors, daily\") + \n    theme(legend.position = \"right\")\n# cp5 &lt;- three_matrices$corr_weekly_matrix |&gt; plot_corr(\"11_sectors, weekly\")\ncp6 &lt;- three_matrices$corr_monthly_matrix |&gt; plot_corr(\"11_sectors, monthly\") + \n    theme(legend.position = \"right\")\n    \nselect_row_3 &lt;- stock_sets |&gt; filter(set_id == \"13_top_in_sector\")\nthree_matrices &lt;- get_corr_matrices(select_row_3)\ncp7 &lt;- three_matrices$corr_daily_matrix |&gt; plot_corr(\"13_top_in_sector, daily\")\n# cp8 &lt;- three_matrices$corr_weekly_matrix |&gt; plot_corr(\"13_top_in_sector, weekly\")\ncp9 &lt;- three_matrices$corr_monthly_matrix |&gt; plot_corr(\"13_top_in_sector, monthly\")\n\ncp1 + cp4 + plot_annotation(\"Daily frequency: Styles vs Sectors\")\n\n\n\ncp3 + cp6 + plot_annotation(\"Monthly frequency: Styles vs Sectors\")\n\n\n\ncp7 + cp9 + plot_annotation(\"Largest Stock in Each Sector: Daily vs Monthly\")"
  },
  {
    "objectID": "posts/ppc_v2/index.html#simulation",
    "href": "posts/ppc_v2/index.html#simulation",
    "title": "Applying my PPC data model",
    "section": "Simulation",
    "text": "Simulation\nThere are four steps here: select the portfolio, bias the random weights (if desired), setup the simulation, and run the simulation.\n\nSimulation: 1. Select the portfolio (set of tickers/symbols)\n\n# Selecting the desired set (e.g., \"Set 1\")\n\nselect_set &lt;- stock_sets |&gt; \n    filter(set_id == \"11_sectors\") |&gt; \n    pull(\"analysis\")\n\nanalyze_set &lt;- select_set[[1]]\nanalyze_period &lt;- analyze_set$monthly\n\n# Extracting components from the selected set\nexp_returns_period &lt;- analyze_period$avg_returns$avg_return\nnames(exp_returns_period) &lt;- analyze_period$avg_returns$symbol\n\nvolatilities_period &lt;- analyze_period$volatilities$volatility\nnames(volatilities_period) &lt;- analyze_period$volatilities$symbol\n\ncorr_matrix_period &lt;- analyze_period$corr_matrix\nnum_stocks_period &lt;- length(volatilities_period)\n\n\n\nSimulation: 2. Bias the random weights\nThis is an experiment to see if we can bias the random weights to get a better result. Rather than deduce a formula, I decided to ask, what is a more (or less) desirable feature of a diversifying asset? (in the mean-variance framework, it’s an asset with low marginal volatility). I decided to include the Sharpe ratio, expected utility (i.e., the expected rate of return minus half the risk aversion coeffient multiplied by the variance), and average correlation.\n\ncalculate_desirability &lt;- function(exp_returns, volatilities, corr_matrix, A, risk_free_rate, \n                                   utility_weight, sharpe_weight, correlation_weight) {\n\n    # Calculate the variance as the square of volatility\n    variance &lt;- volatilities^2\n    \n    # Calculate the utility for each sector\n    utility &lt;- exp_returns - 0.5 * A * variance\n    \n    # Calculate the Sharpe ratio for each sector\n    sharpe_ratio &lt;- (exp_returns - risk_free_rate) / volatilities\n    \n    # Calculate the average correlation for each sector\n    avg_correlation &lt;- apply(corr_matrix, 1, function(x) mean(x[-which(x == 1)], na.rm = TRUE))\n    write.matrix(corr_matrix, \"avg_correlation.csv\", sep = \",\")\n    \n    # Calculate the desirability score\n    desirability_score &lt;- utility_weight * (utility * 100) +\n        sharpe_weight * (sharpe_ratio * 3 ) -\n        correlation_weight * avg_correlation # Negative because lower correlation is better\n    \n    # Create a data frame for sector desirability\n    desirability_df &lt;- data.frame(sector = names(exp_returns),\n                                  exp_returns = exp_returns,\n                                  volatilities = volatilities,\n                                  utility = utility,\n                                  sharpe_ratio = sharpe_ratio,\n                                  avg_correlation = avg_correlation,\n                                  desirability_score = desirability_score)\n    \n    # Sort by desirability score\n    desirability_df &lt;- desirability_df[order(-desirability_df$desirability_score),]\n    \n    return(desirability_df)\n}\n\n# Example parameters\nA &lt;- 3  # Risk aversion coefficient\nrisk_free_rate &lt;- 0.0  # Risk-free rate\nutility_weight &lt;- 0.3\nsharpe_weight &lt;- 0.4\ncorrelation_weight &lt;- 0.3\n\n# Calculate desirability using the extracted components\nsector_desirability &lt;- calculate_desirability(exp_returns_period, volatilities_period, corr_matrix_period, A, risk_free_rate, utility_weight, sharpe_weight, correlation_weight)\n\nsector_desirability |&gt; gt() |&gt; \n    cols_label(sector = \"Ticker\",\n               exp_returns = \"Return\",\n               volatilities = \"Vol\",\n               utility = \"Utility\",\n               sharpe_ratio = \"Sharpe\",\n               avg_correlation = \"Avg Corr\",\n               desirability_score = \"Desirability\") |&gt;\n    fmt_percent(columns = 2:3) |&gt; \n    fmt_number(columns = 4:7, decimals = 4)\n\n\n\n\n\n  \n    \n    \n      Ticker\n      Return\n      Vol\n      Utility\n      Sharpe\n      Avg Corr\n      Desirability\n    \n  \n  \n    XLK\n1.50%\n5.17%\n0.0110\n0.2909\n0.5422\n0.5171\n    XLV\n1.01%\n3.99%\n0.0077\n0.2534\n0.7093\n0.3230\n    XLP\n0.72%\n3.64%\n0.0052\n0.1983\n0.5484\n0.2304\n    XLY\n1.04%\n5.42%\n0.0060\n0.1918\n0.6908\n0.2027\n    XLI\n0.91%\n5.12%\n0.0052\n0.1785\n0.6589\n0.1728\n    XLF\n0.87%\n5.44%\n0.0042\n0.1594\n0.7085\n0.1057\n    XLU\n0.70%\n4.35%\n0.0042\n0.1613\n0.7629\n0.0899\n    XLB\n0.73%\n5.30%\n0.0031\n0.1372\n0.6885\n0.0499\n    XLC\n0.58%\n6.09%\n0.0003\n0.0959\n0.6580\n−0.0738\n    XLRE\n0.46%\n5.01%\n0.0008\n0.0919\n0.7664\n−0.0944\n    XLE\n0.40%\n8.35%\n−0.0064\n0.0483\n0.7167\n−0.3499\n  \n  \n  \n\n\n\n\n\n\nSimulation: 3. Setup the simulation\nThe get_random_weights function returns a dataframe of random weights (that sum to 1). Each column is a set of weights for a single simulation.\nThis adds one innovation to the (my previous) naive approach. As previously, I still assume: the expected return for each stock is the average return for that stock over the entire period; the volatility for each stock is the average volatility for that stock over the entire period; and the correlation matrix (which obviously implies the covariance matrix) is computed from the pairwise (log) return vectors. In short, I’m assuming that the future will be like the past.\nThe innovation is that I bias the random weights to slightly favor the stocks with the highest “desirability” scores, as defined in the previous chunk. Desirability is just a weighted average of Utility, Sharpe ratio and (the inverse of) average correlation. Once random weights are selected, the portfolio return and volatility are calculated; and those don’t depend on the bias. The idea here is simply to (probably slightly) favor the simulation’s density toward the efficient segment.\n\n# returns a data frame of random weights\n# rows = weight per stock; columns = number of simulations\nget_random_weights &lt;- function(num_stocks, num_simulations, probabilities, row_names = NULL) {\n    set.seed(47391)\n    weights_df &lt;- matrix(nrow = num_stocks, ncol = num_simulations)\n\n    for (i in 1:num_simulations) {\n        # Generate weights influenced by probabilities\n        weights &lt;- runif(num_stocks) * probabilities\n        weights_df[, i] &lt;- weights / sum(weights)  # Normalize the weights\n    }\n    \n    weights_df &lt;- as.data.frame(weights_df)\n    \n    if (!is.null(row_names) && length(row_names) == num_stocks) {\n        rownames(weights_df) &lt;- row_names\n    }\n\n    return(weights_df)\n}\n\n# single simulation: given a set of weights, computes the expected return and volatility\nport_sim &lt;- function(exp_returns, volatilities, corr_matrix, weights) {\n    \n    cov_matrix &lt;- outer(volatilities, volatilities) * corr_matrix\n    port_variance &lt;- t(weights) %*% cov_matrix %*% weights\n    port_exp_return &lt;- sum(weights * exp_returns)\n\n    return(list(exp_returns = exp_returns, \n                volatilities = volatilities,\n                cov_matrix = cov_matrix, \n                corr_matrix = corr_matrix,\n                port_variance = port_variance,\n                port_exp_return = port_exp_return))\n}\n\n# runs a port_simulation for each column in the weights_df\nrun_sims &lt;- function(exp_returns, volatilities, corr_matrix, weights_df) {\n    simulations &lt;- map(1:ncol(weights_df), ~ {\n        weights_vector &lt;- weights_df[, .x]\n        port_sim(exp_returns, volatilities, corr_matrix, weights_vector)\n        })\n    \n    return(simulations)\n}\n\n\n\nSimulation: 4. Run the simulation (on a single set)\n\nnum_sims &lt;- 10000  # Set the number of simulations\n\nsector_desirability$desirability_score &lt;- pmax(sector_desirability$desirability_score, 0)\ntotal_score &lt;- sum(sector_desirability$desirability_score)\nprobabilities &lt;- sector_desirability$desirability_score / total_score\nnames(probabilities) &lt;- sector_desirability$sector\nordered_probabilities &lt;- probabilities[names(exp_returns_period)]\n\nrow_symbols &lt;- names(exp_returns_period)\nrandom_weights_df_period &lt;- get_random_weights(num_stocks_period, num_sims,\n                                               ordered_probabilities, row_names = row_symbols)\n# View first 5 columns (= simulations/trials)\nrandom_weights_df_period |&gt; dplyr::select(1:5)\n\n             V1         V2         V3         V4          V5\nXLB  0.02904022 0.05416135 0.02778490 0.02113182 0.068640582\nXLC  0.00000000 0.00000000 0.00000000 0.00000000 0.000000000\nXLE  0.00000000 0.00000000 0.00000000 0.00000000 0.000000000\nXLF  0.07715564 0.05391847 0.11792040 0.04413685 0.032498297\nXLI  0.07038140 0.14379002 0.10358717 0.15823493 0.106281860\nXLK  0.39217487 0.08140458 0.17201663 0.24347196 0.290418025\nXLP  0.10247948 0.30004556 0.16088140 0.19679704 0.079391443\nXLRE 0.00000000 0.00000000 0.00000000 0.00000000 0.000000000\nXLU  0.01658246 0.08766279 0.13995496 0.01986227 0.006779211\nXLV  0.23783284 0.04282421 0.04622476 0.22763656 0.189368775\nXLY  0.07435309 0.23619302 0.23162977 0.08872856 0.226621805\n\nsim_results_period &lt;- run_sims(exp_returns_period, \n                              volatilities_period, \n                              corr_matrix_period, \n                              random_weights_df_period)\n\n# Testing\n# print(sim_results_period[[1]])\n\nresults_df_period &lt;- map_dfr(sim_results_period, ~ data.frame(Exp_Return = .x$port_exp_return, \n                                                            Std_Dev = sqrt(.x$port_variance)))\n# Define the risk-free rate\nrisk_free_cml_1 &lt;- 0\nrisk_free_cml_2 &lt;- 0.005\n\n# Calculate the Sharpe Ratio for each simulation\nresults_df_period &lt;- results_df_period |&gt; \n    mutate(\n        Sharpe_Ratio_1 = (Exp_Return - risk_free_cml_1) / Std_Dev,\n        Sharpe_Ratio_2 = (Exp_Return - risk_free_cml_2) / Std_Dev\n        )\n\n# Find the row with the highest Sharpe Ratio\nbest_sim_1 &lt;- results_df_period[which.max(results_df_period$Sharpe_Ratio_1), ]\nbest_sim_2 &lt;- results_df_period[which.max(results_df_period$Sharpe_Ratio_2), ]\n\n# Print the best simulation result\nprint(best_sim_1)\n\n     Exp_Return    Std_Dev Sharpe_Ratio_1 Sharpe_Ratio_2\n5333 0.01181061 0.03765077      0.3136885      0.1808891\n\nprint(best_sim_2)\n\n     Exp_Return    Std_Dev Sharpe_Ratio_1 Sharpe_Ratio_2\n6340  0.0129082 0.04164321      0.3099714      0.1899038\n\n# View summarized results for daily returns\nprint(head(results_df_period))\n\n   Exp_Return    Std_Dev Sharpe_Ratio_1 Sharpe_Ratio_2\n1 0.011451646 0.03877509      0.2953351      0.1663864\n2 0.009070019 0.03843242      0.2359992      0.1059007\n3 0.009775922 0.03997072      0.2445771      0.1194855\n4 0.010426930 0.03721517      0.2801795      0.1458257\n5 0.011009263 0.03991819      0.2757956      0.1505394\n6 0.011803124 0.04004291      0.2947619      0.1698958"
  },
  {
    "objectID": "posts/ppc_v2/index.html#visualize-the-results",
    "href": "posts/ppc_v2/index.html#visualize-the-results",
    "title": "Applying my PPC data model",
    "section": "Visualize the results",
    "text": "Visualize the results\n\nlibrary(patchwork)\n\nresults_df &lt;- results_df_period # continuity from prior\nresults_df &lt;- results_df |&gt; \n    arrange(Std_Dev) |&gt; \n    mutate(is_efficient = Exp_Return &gt;= cummax(Exp_Return))\n\nefficient_portfolios &lt;- results_df |&gt; \n    arrange(Std_Dev)  |&gt; \n    mutate(cummax_return = cummax(Exp_Return)) |&gt; \n    filter(Exp_Return &gt;= cummax_return)\n\nefficient_model &lt;- lm(Exp_Return ~ poly(Std_Dev, 2), data = efficient_portfolios)\n\np1 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return, color = is_efficient)) +\n    geom_point() +\n    scale_color_manual(values = c(\"azure2\", \"springgreen4\")) + \n    theme_minimal() +\n    theme(\n        axis.title = element_blank(),\n        legend.position = \"none\"\n    )\n\np2 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return)) +\n    geom_point(aes(color = is_efficient), size = 1) +  # Default size for all points\n    geom_point(data = filter(results_df, is_efficient), \n               aes(color = is_efficient), size = 2) +  # Larger size for efficient points\n    scale_color_manual(values = c(\"azure2\", \"springgreen4\")) +\n    theme_minimal() +\n    geom_line(data = efficient_portfolios, aes(x = Std_Dev, y = Exp_Return), colour = \"springgreen2\") +\n    theme(\n        axis.title = element_blank(),\n        legend.position = \"none\"\n    )\n\np3 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return)) +\n    geom_point(color = \"azure2\") +\n    geom_smooth(data = efficient_portfolios, method = \"lm\", formula = y ~ poly(x, 2), \n                se = FALSE, colour = \"springgreen4\", linewidth = 1.5) +\n    labs(x = \"Std Dev (Risk)\",\n         y = \"Return\") +\n    coord_cartesian(xlim = c(0.0330, 0.050), ylim = c(0.0075, 0.0140)) + \n    theme_minimal()\n\n# Calculate a metric, Sharpe, that can inform COLOR fill\n# ... but it will only be based on the first risk-free rate!\n\nresults_df &lt;- results_df %&gt;%\n    mutate(efficiency = (Exp_Return - risk_free_cml_1)/ Std_Dev)\n\nslope_cml_1 &lt;- (best_sim_1$Exp_Return - risk_free_cml_1) / best_sim_1$Std_Dev\nslope_cml_2 &lt;- (best_sim_2$Exp_Return - risk_free_cml_2) / best_sim_2$Std_Dev\n\nextended_std_dev &lt;- max(results_df_period$Std_Dev) * 1.2  # For example, 20% beyond the max std dev in the data\nextended_exp_return_1 &lt;- risk_free_cml_1 + slope_cml_1 * extended_std_dev\nextended_exp_return_2 &lt;- risk_free_cml_2 + slope_cml_2 * extended_std_dev\n\np4 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return, color = efficiency)) +\n    geom_point() +\n    scale_color_gradientn(colors = c(\"firebrick1\", \"lightgoldenrod1\", \"springgreen2\"),\n                          values = scales::rescale(c(min(results_df$efficiency), \n                                                     max(results_df$efficiency)))) +\n    geom_segment(aes(x = 0, y = risk_free_cml_1, xend = extended_std_dev, yend = extended_exp_return_1), \n                 color = \"dodgerblue2\", linetype = \"dashed\", linewidth = 1) +\n    theme_minimal() +\n    coord_cartesian(xlim = c(0.0330, 0.050), ylim = c(0.0075, 0.0140)) + \n    labs(x = \"Std Dev (Risk)\",\n         y = \"Return\", \n         color = \"efficiency\")\n\n(p1 + p2) / (p3 + p4 )\n\n\n\n\n\nAdd the second CML\n\np5 &lt;- p4 + geom_segment(aes(x = 0, y = risk_free_cml_2, xend = extended_std_dev, yend = extended_exp_return_2), \n                 color = \"purple2\", linetype = \"dashed\", linewidth = 1) \np5\n\n\n\n\n\n\nThis is for the social thumb\n\ncp9_thumb &lt;- cp9 + theme(legend.position = \"right\")\n\n(cp7 + cp9_thumb) / (p2 + p4)"
  },
  {
    "objectID": "posts/sim-equity-risk-premium/index.html",
    "href": "posts/sim-equity-risk-premium/index.html",
    "title": "Simulating the equity risk premium",
    "section": "",
    "text": "The following implements the implied ERP approach in Professor Damodaran’s post on the The Price of Risk. My intention is to briefly explore its sensitivity to assumptions.\n\nlibrary(tidyverse)\nlibrary(scales)\n\nsolve_for_R &lt;- function(RF, ER_vector, CP_vector, G2, PV) {\n  \n  # Calculate cash flow vector\n  CF_vector &lt;- ER_vector * CP_vector\n  \n  # Define the objective function\n  objective_function &lt;- function(R) {\n    # This is effectively two-stage dividend discount model except the initial stage is explicated\n    # such that there exists no G1 and G2 refers to the subsequent period of growth\n    \n    PV_calculated &lt;- sum(CF_vector[1:5] / (1 + R)^(1:5)) + CF_vector[6] / ((R - G2) * (1 + R)^5)\n    return((PV - PV_calculated)^2)\n  }\n  \n  # Use the optim function to minimize the objective function\n  result &lt;- optim(par = RF, fn = objective_function, method = \"Brent\", lower = -1, upper = 2)\n  \n  return(result$par)\n}\n\nRF &lt;- 0.04 # I have rounded his riskfree rate of 3.97% to 4.00%\nER_vector &lt;- c(217.8, 245.2, 273.7, 295.1, 308.9, 324.9) # A. Damodaran's earnings vector\nCP_vector &lt;- c(0.84, 0.82, 0.80, 0.78, 0.77, 0.77) # Cash payout ratios\nG2 &lt;- 0.04 # His model sets the stable growth equal to the RF rate\nPV &lt;- 4600 # I rounded 4588.96 to 4,600\n\nimplied_equity &lt;- solve_for_R(RF, ER_vector, CP_vector, G2, PV)\nimplied_ERP &lt;- implied_equity - RF\n\n# Number of simulations\nn_simulations &lt;- 10000\ncoeff_variation &lt;- 0.10 # Arbitrarily suggesting that COV of 10% is tight\n\n# Assumed means and standard deviations for inputs\nmean_RF &lt;- RF; sd_RF &lt;- RF * coeff_variation\nmean_ER &lt;- ER_vector; sd_ER &lt;- ER_vector * coeff_variation\nmean_CP &lt;- CP_vector; sd_CP &lt;- CP_vector * coeff_variation\nmean_G2 &lt;- G2; sd_G2 &lt;- G2 * coeff_variation\nmean_PV &lt;- PV; sd_PV &lt;- PV * coeff_variation\n\n# MC simulation\nset.seed(379)\nR_values &lt;- replicate(n_simulations, \n  solve_for_R(\n    # RF = rnorm(1, mean_RF, sd_RF),\n    RF = RF,\n    ER_vector = rnorm(6, mean_ER, sd_ER),\n    CP_vector = rnorm(6, mean_CP, sd_CP),\n    G2 = rnorm(1, mean_G2, sd_G2),\n    PV = PV\n  )\n)\n\n# Histogram to visualize the distribution of R values\nR_values &lt;- R_values[R_values &gt; 0]\nERP_values &lt;- R_values - RF\nERP_values_mean &lt;- mean(ERP_values)\nERP_values_df &lt;- as_data_frame(ERP_values)\n\nERP_values_df %&gt;% ggplot(aes(value)) +\n    geom_histogram(color = \"darkblue\", fill = \"lightblue\") +\n    geom_vline(aes(xintercept = ERP_values_mean), color = \"darkgreen\", size = 1.5) +\n    scale_x_continuous(labels = percent_format(0.01)) +\n    labs(title = \"Implied equity risk premium, ERP (n = 10,000 sims)\",\n         subtitle = \"Under tight assumption dispersion (CV = σ/μ =10%). Green vertical line is the mean.\",\n         y = \"Count\") +\n    # xlab(\"X label\") + \n    # ylab(\"Count\") +\n    theme_classic() +\n    theme(axis.title = element_blank(),\n          axis.text = element_text(size = 12, face = \"bold\"))\n\n\n\n\nQuick check on the distribution:\n\nlibrary(moments)\nskewness(ERP_values_df$value)\n\n[1] 0.08903814\n\nkurtosis(ERP_values_df$value)\n\n[1] 2.997992\n\nquantiles_v &lt;- c(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975, 0.99)\nquantile(ERP_values_df$value, probs = quantiles_v)\n\n        1%       2.5%         5%        10%        25%        50%        75% \n0.03041180 0.03248300 0.03433620 0.03649075 0.04004356 0.04410500 0.04832614 \n       90%        95%      97.5%        99% \n0.05209407 0.05447292 0.05638444 0.05881702 \n\n\nWhat is the relationship between the sustainable growth rate, G2, and the ERP?\n\nG2_values &lt;- seq(from = 0.02, to = 0.06, by = 0.001)\nR_values &lt;- map_dbl(G2_values, function(G2) {\n  solve_for_R(\n    RF = RF,\n    ER_vector = ER_vector,\n    CP_vector = CP_vector,\n    G2 = G2,\n    PV = PV\n  )\n})\n\nERP_values &lt;- R_values - RF\n\nG_vs_ERP &lt;-  tibble(\n  G2 = G2_values,\n  ERP = ERP_values\n)\n\nG_vs_ERP %&gt;% ggplot(aes(x = G2, y = ERP)) + \n  geom_point() + \n  coord_cartesian(ylim = c(.02, .08)) + \n  labs(title = \"Implied ERP as function of sustainable growth rate, G2\",\n       subtitle = \"Unlike prior/next visualization, predicted vectors are not randomized\")\n\n\n\n\nAnd just for fun, let’s add randomness to the earnings and cash payout vectors:\n\nG2_values &lt;- seq(from = 0.02, to = 0.06, by = 0.001)\n\nR_values &lt;- map(G2_values, function(G2) {\n  replicate(30, {\n    solve_for_R(\n      RF = RF,\n      ER_vector = rnorm(6, mean_ER, sd_ER),\n      CP_vector = rnorm(6, mean_CP, sd_CP),\n      G2 = G2,\n      PV = PV\n    ) - RF # subtracting RF here inside replicat\n  })\n})\n\ndf &lt;- tibble(\n  G2 = G2_values,\n  ERP = R_values\n) %&gt;% unnest()\n\nmodel_line &lt;- lm(ERP ~ G2, data = df)\nrsq &lt;- summary(model_line)$r.squared\nlabel_R2 &lt;- sprintf(\"R^2 = %.2f\", rsq)\n\ndf %&gt;% ggplot(aes(x = G2, y = ERP)) + \n  geom_point() +\n  coord_cartesian(ylim = c(.02, .08)) + \n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Restores 10% CV randomness to earnings and payout vectors\") +\n  annotate(\"text\", x=0.025, y=0.065, label=label_R2, fontface=\"bold\", hjust=0)\n\n\n\n  # geom_text(aes(label = label_R2))"
  },
  {
    "objectID": "posts/var_intro/index.html",
    "href": "posts/var_intro/index.html",
    "title": "Value at Risk (VaR) Introduction",
    "section": "",
    "text": "Historical Simulation: Basic and Bootstrap\nMonte Carlo\nParametric; aka, analytical"
  },
  {
    "objectID": "posts/var_intro/index.html#historical-simulation-hs",
    "href": "posts/var_intro/index.html#historical-simulation-hs",
    "title": "Value at Risk (VaR) Introduction",
    "section": "Historical simulation (HS)",
    "text": "Historical simulation (HS)\n\nBasic HS\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(patchwork)\nlibrary(scales)\n\nsymbols &lt;- c(\"PG\", \"JPM\", \"NVDA\")\nmult_stocks &lt;- tq_get(symbols, get = \"stock.prices\", from = \"2012-12-31\", to = \"2022-12-31\")\nmult_stocks$symbol &lt;- mult_stocks$symbol &lt;- factor(mult_stocks$symbol, levels = c(\"PG\", \"JPM\", \"NVDA\"))\n\n# tq_mutate_fun_options() returns list of compatible mutate functions by pkg \nall_returns_daily &lt;- mult_stocks |&gt; \n    group_by(symbol) |&gt;\n    tq_transmute(select     = adjusted,\n                 mutate_fun = periodReturn, \n                 period     = \"daily\", \n                 type       = \"log\")\n\nall_returns_monthly &lt;- mult_stocks |&gt; \n    group_by(symbol) |&gt; \n    tq_transmute(select     = adjusted, \n                 mutate_fun = periodReturn, \n                 period     = \"monthly\",\n                 type       = \"log\")\n\n# reframe() has apparently replaced summarize()\nquantiles_daily &lt;- all_returns_daily |&gt; \n    group_by(symbol) |&gt; \n    reframe(quantiles = quantile(daily.returns, probs = c(0.05, 0.95)))\n\nquantiles_monthly &lt;- all_returns_monthly |&gt; \n    group_by(symbol) |&gt; \n    reframe(quantiles = quantile(monthly.returns, probs = c(0.05, 0.95)))\n\n# 5% quantile for each stock, DAILY\nPG_05d &lt;- quantiles_daily$quantiles[1]\nJPM_05d &lt;- quantiles_daily$quantiles[3]\nNVDA_05d &lt;- quantiles_daily$quantiles[5]\nmean_d &lt;- mean(all_returns_daily$daily.returns) \n\n# 5% quantile for each stock, MONTHLY\nPG_05m &lt;- quantiles_monthly$quantiles[1]\nJPM_05m &lt;- quantiles_monthly$quantiles[3]\nNVDA_05m &lt;- quantiles_monthly$quantiles[5]\nmean_m &lt;- mean(all_returns_monthly$monthly.returns)\n\n# I probably spend too much time tinkering with colors\n# col_ticker_fills &lt;- c(\"PG\" = \"blue\", \"JPM\" = \"yellow\", \"NVDA\" = \"red\")\n# col_ticker_fills &lt;- c(\"PG\" = \"#90EE90\", \"JPM\" = \"#ff6347\", \"NVDA\" = \"#8B0000\")\ncol_ticker_fills &lt;- c(\"PG\" = \"chartreuse2\", \"JPM\" = \"dodgerblue2\", \"NVDA\" = \"coral1\")\ncol_ticker_colors &lt;- c(\"PG\" = \"chartreuse3\", \"JPM\" = \"dodgerblue3\", \"NVDA\" = \"coral3\")\ncol_PG_line &lt;- \"chartreuse3\"; col_JPM_line &lt;- \"dodgerblue3\"; col_NVDA_line &lt;- \"coral3\"\n\np_hist_daily &lt;- all_returns_daily |&gt; \n    ggplot(aes(x = daily.returns, fill = symbol, color = symbol)) +\n    geom_density(alpha = 0.50) +\n    geom_vline(xintercept = PG_05d, color = col_PG_line, linetype = \"dashed\", linewidth = 1) +\n    geom_vline(xintercept = JPM_05d, color = col_JPM_line, linetype = \"dashed\", linewidth = 1) + \n    geom_vline(xintercept = NVDA_05d, color = col_NVDA_line, linetype = \"dashed\", linewidth = 1) +\n    geom_vline(xintercept = mean_d, color = \"black\", linewidth = 0.4) +\n    scale_fill_manual(values = col_ticker_fills) +\n    scale_color_manual(values = col_ticker_fills) +\n    theme_minimal() +\n    coord_cartesian(xlim = c(-0.25, 0.25)) +\n        theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n    )\n\np_hist_monthly &lt;- all_returns_monthly |&gt; \n    ggplot(aes(x = monthly.returns, fill = symbol, color = symbol)) +\n    geom_density(alpha = 0.50) +\n    geom_vline(xintercept = PG_05m, color = col_PG_line, linetype = \"dashed\", linewidth = 1) +\n    geom_vline(xintercept = JPM_05m, color = col_JPM_line, linetype = \"dashed\", linewidth = 1) + \n    geom_vline(xintercept = NVDA_05m, color = col_NVDA_line, linetype = \"dashed\", linewidth = 1) +\n    geom_vline(xintercept = mean_m, color = \"black\", linewidth = 0.4) +\n    scale_fill_manual(values = col_ticker_fills) +\n    scale_color_manual(values = col_ticker_fills) +\n    theme_minimal() + \n    coord_cartesian(xlim = c(-0.25, 0.25)) +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n    )\n\np_hs &lt;- p_hist_daily / p_hist_monthly\np_hs\n\n\n\n\n\n\nBootstrap HS\nFirst let’s simulate a single one-year forward (+ 12 months) path\n\n# all_returns_monthly is tidy, but the simulation wants a wide format\nall_returns_monthly_wide &lt;- all_returns_monthly |&gt; \n  pivot_wider(names_from = symbol, values_from = monthly.returns)\n\n# Initial investment (into each stock)\ninitial_investment &lt;- 100\nportfolio &lt;- setNames(data.frame(t(rep(initial_investment, times = length(symbols)))), symbols)\nmonths_to_simulate &lt;- 12 # Set X months for the simulation\n\n# Simulate one forward month\nsimulate_one_month &lt;- function(portfolio, historical_returns_wide) {\n    # Randomly sample one month's returns (with replacement)\n    sampled_returns &lt;- historical_returns_wide |&gt;  \n        sample_n(1, replace = TRUE) |&gt; select(-date) \n    # Apply the sampled log returns to the current portfolio value\n    updated_portfolio &lt;- portfolio * exp(sampled_returns)\n    # FOR TESTING: print(sampled_returns[1,]); print(updated_portfolio[1,])\n    return(updated_portfolio)\n}\n\n# Run the simulation for X months\nset.seed(123) # For reproducibility of results\nsimulation_results &lt;- tibble(Month = 0, TotalValue = sum(portfolio))\nfor (i in 1:months_to_simulate) {\n    portfolio &lt;- simulate_one_month(portfolio, all_returns_monthly_wide)\n    simulation_results &lt;- simulation_results |&gt;  \n        add_row(Month = i, TotalValue = sum(portfolio))\n}\n\nprint(simulation_results)\n\n# A tibble: 13 × 2\n   Month TotalValue\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1     0       300 \n 2     1       294.\n 3     2       325.\n 4     3       329.\n 5     4       316.\n 6     5       314.\n 7     6       349.\n 8     7       356.\n 9     8       312.\n10     9       313.\n11    10       326.\n12    11       349.\n13    12       336.\n\n\nNext let’s add a loop to run the simulation multiple times; e.g. 20 trials\n\nlibrary(RColorBrewer)\n# library(ggside) couldn't pull it off! \n\n# Function to simulate portfolio over X months, where each trail has a trial_id\nsimulate_portfolio &lt;- function(months_to_simulate, historical_returns_wide, initial_investment, trial_id) {\n    # Initialize portfolio\n    portfolio &lt;- setNames(data.frame(t(rep(initial_investment, times = length(symbols)))), symbols)\n    # Initialize results data frame with Month 0\n    simulation_results &lt;- tibble(Month = 0, TotalValue = sum(portfolio), Trial = as.factor(trial_id))\n    for (i in 1:months_to_simulate) {\n        portfolio &lt;- simulate_one_month(portfolio, historical_returns_wide)\n        simulation_results &lt;- simulation_results |&gt; \n            add_row(Month = i, TotalValue = sum(portfolio), Trial = as.factor(trial_id))\n    }\n  return(simulation_results)\n}\n\nmonths_to_simulate &lt;- 12 \nnum_trials &lt;- 20 # Number of trials\n\nset.seed(123) \nall_trials &lt;- map_df(1:num_trials, \n                     ~simulate_portfolio(months_to_simulate, all_returns_monthly_wide, initial_investment, .x), \n                     .id = \"Trial_ID\")\n\nfinal_month_values_df &lt;- all_trials |&gt; \n    filter(Month == max(all_trials$Month))\n\n# Plot the results using ggplot2\np_forward_sim &lt;- ggplot(all_trials, aes(x = Month, y = TotalValue, group = Trial, color = Trial)) +\n    geom_line() +\n    scale_color_viridis_d(option = \"plasma\", direction = -1) +\n    theme_minimal() +\n    scale_x_continuous(breaks = 1:12, limits = c(0,12)) +\n    labs(x = \"Month\",\n         y = \"Portfolio Value\") +\n    theme(legend.position = \"none\") \n\ndensity_plot &lt;- final_month_values_df |&gt; ggplot(aes(x = TotalValue)) +\n    geom_density(fill = \"#933fbd\", alpha = 0.5) +\n        theme_minimal() + \n    theme(\n        axis.title = element_blank(),\n        axis.text = element_blank()\n    ) +\n    coord_flip()\n\np_boot &lt;- p_forward_sim + density_plot + \n    plot_layout(ncol = 2, widths = c(3, 1))\np_boot\n\n\n\nfinal_month_values_vct &lt;- final_month_values_df |&gt; \n    pull(TotalValue) # 'pull' extracts the column as a vector\n\n# Calculate and print the quantiles for the final month's values\nquantiles_final_month &lt;- quantile(final_month_values_vct, probs = c(0, 0.01, 0.05, 0.50, 1.0))\nprint(quantiles_final_month)\n\n      0%       1%       5%      50%     100% \n218.1581 233.4910 294.8226 371.7791 567.0854 \n\n\n\n\nMonte carlos simulation (MCS)\nFirst a single path:\n\nsim_gbm &lt;- function(price, mu, sigma, periods) {\n  dt &lt;- 1/52  # we'll assume period is one week\n  prices &lt;- numeric(periods)\n  prices[1] &lt;- price\n\n    for(t in 2:periods) {\n    Z &lt;- rnorm(1) # random standar normal quantile\n    \n    # GBM\n    prices[t] &lt;- prices[t-1]*exp((mu -0.5*sigma^2)*dt +sigma*sqrt(dt)*Z)\n    }\n  return(prices)\n}\n\nset.seed(952347)\nsim_prices &lt;- sim_gbm(100, 0.09, 0.3, 50)\nsim_prices_df &lt;- as_tibble(sim_prices) |&gt; rownames_to_column(\"period\")\nsim_prices_df$period &lt;- as.integer(sim_prices_df$period)\n\n# Plot the simulated stock price path\nsim_prices_df |&gt; \n    ggplot(aes(x = period, y = value)) + \n    geom_line() + \n    theme_minimal() \n\n\n\n\nNow multiple (say 20) trials:\n\nsim_gbm_matrix &lt;- function(price, mu, sigma, periods, simulations) {\n  dt &lt;- 1/52  \n  prices &lt;- matrix(price, nrow = periods, ncol = simulations)\n  for(t in 2:periods) {\n    Z &lt;- rnorm(simulations)\n    prices[t, ] &lt;- prices[t-1, ] * exp((mu - 0.5 * sigma^2) * dt + sigma * sqrt(dt) * Z)\n  }\n  return(prices)\n}\n\nset.seed(84923)\nsimulations &lt;- 20\nsim_prices_matrix &lt;- sim_gbm_matrix(100, 0.09, 0.3, 50, simulations)\n\nsim_prices_df &lt;- as_tibble(sim_prices_matrix, .name_repair = \"minimal\")\nnames(sim_prices_df) &lt;- 1:ncol(sim_prices_df)\nsim_prices_df &lt;- sim_prices_df |&gt; rownames_to_column(\"period\")\nsim_prices_df$period &lt;- as.integer(sim_prices_df$period)\n\nsim_prices_long &lt;- sim_prices_df |&gt; \n  pivot_longer(cols = !period, names_to = \"Trial\", values_to = \"Price\")\n\np_mcs &lt;- sim_prices_long |&gt; ggplot(aes(x = period, y = Price, group = Trial, color = Trial)) +\n    geom_line() +\n    labs(x = \"Week\",\n         y = \"Price\") +\n    theme_minimal() +\n    theme(\n        legend.position = \"none\"\n    ) +\n    scale_color_viridis_d()\n\np_mcs + ggtitle(\"GBM MCS: 20 trials x 50 weeks\")\n\n\n\n\n\n\nParametric: normally distributed arithmetic returns\nThe basic starting point: normal, arithmetic returns and scaling per the square root rule (SRR) which asumes i.i.d. returns.\n\nlibrary(ggplot2)\n\n# Parameters\nmu_pa &lt;- 0.09\nsigma_pa &lt;- 0.20\nhorizon &lt;- 12/52\n\nmu &lt;- mu_pa * horizon  # horizon return\nsigma &lt;- sigma_pa * sqrt(horizon)  # horizon volatility\nconfidence_level &lt;- 0.95\ninvestment_value &lt;- 100  # Initial investment\n\n# Find the Z-score that corresponds to the confidence level\nz_score &lt;- qnorm(confidence_level)\nVaR &lt;- (-mu + sigma * z_score) * investment_value\n\n# Create a dataframe of returns for plotting\nreturns &lt;- data.frame(Returns = seq(-0.30, 0.30, by = 0.001))\nreturns$Density &lt;- dnorm(returns$Returns, mean = mu, sd = sigma)\n\n# Plot with ggplot\np_pVaR &lt;- ggplot(returns, aes(x = Returns, y = Density)) +\n    geom_line(color = \"dodgerblue\", linewidth = 1.3) +\n    geom_vline(xintercept = -VaR / investment_value, color = \"darkred\", linetype = \"dashed\", linewidth = 1.3) +\n    geom_area(data = subset(returns, Returns &lt; -VaR / investment_value),\n              aes(x = Returns, y = Density), fill = \"coral1\", alpha = 0.5) +\n    geom_vline(xintercept = mu, color = \"chartreuse3\", linetype = \"dashed\", linewidth = 1.3) + \n    geom_vline(xintercept = 0, color = \"black\", linewidth = 0.4) +\n    scale_x_continuous(labels = percent_format(),\n                       breaks = seq(-0.30, 0.30, by = 0.1)) +\n    theme_minimal() +\n    theme(\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank()\n    ) + \n    annotate(\"text\", x = -VaR / investment_value + .02, \n             y = max(returns$Density)/3,\n             label = paste(\"VaR at\", scales::percent(confidence_level), \"=\", round(VaR, 3)),\n             hjust = 1.2, vjust = 0, size = 4, color = \"darkred\", fontface = \"bold\")\n\np_pVaR + ggtitle(\"Normal, arithmetic i.i.d. returns\")\n\n\n\n\nFor fun: Using patchwork to print the social thumbnail!\n\n# layout &lt;- \"\n# AABBCC\n# AABBCC\n# DDDDEE\n# DDDDEE\n# \"\n# \n# p_hs + p_pVaR + p_boot + p_mcs +\n#    plot_layout(design = layout)\n\np_hs / p_boot / (p_mcs + p_pVaR)"
  },
  {
    "objectID": "posts/wcs_basic/index.html",
    "href": "posts/wcs_basic/index.html",
    "title": "Worst case scenario simulation (basic)",
    "section": "",
    "text": "Here is GARP’s lame explainer (source: GARP):\n\n\n\n\n\n\n2.11 WORST CASE ANALYSIS:\n\n\n\nOccasionally, when there are repeated trials, an analyst will calculate statistics for worst-case results. For example, if a portfolio manager reports results every week, he or she might ask what the worst result will be over a period of 52 weeks. If the distribution of the returns in one week is known, a Monte Carlo simulation can be used to calculate statistics for this worst-case result. For example, one can calculate the expected worst-case result over 52 weeks, the 95th percentile of the worst-case result, and so on.\n\n\nBelow simulations_list will contain five matrices\n\nThe first matrix is 10,000 rows by 1 column (H = 1 day; not realistic just illustrative)\nThe second matrix is 10,000 rows by 5 columns (H = 5 days)\nThe third matrix is 10,000 rows by 20 columns (H = 20 days)\n\nNote Linda Allen:\n\nIn contrast to VaR, WCS focuses on the distribution of the loss during the worst trading period (“period” being, e.g., one day or two weeks), over a given horizon (“horizon” being, e.g., 100 days or one year). The key point is that a worst period will occur with probability one.\n\nSo if each row is a trial (i.e., 10,000 rows = 10,000 trials), then we’re retrieving a vector of the worst period within the horizon for each of 10,000 trials. In this simulation, all periods are one day. So, the second matrix will retrieve (a vector of length 10,000 of) the worst one-day period in a five-day horizon. The third matrix will retrieve the worst one-day period in a 20-day horizon. The first matrix has only column, so the worst is the only value in the row: the statistics are the same.\nThe key function is worst_returns &lt;- map_dbl(1:nrow(simulation), ~ min(simulation[.x, ])). Because it finds the minimum (ie, worst) value in each of the 10,000 rows. That’s the worst_returns vector.\nHere is my interpretation, and the numbers are very similar to Linda Allen’s table.\n\nlibrary(tidyverse)\n# includes purrr, dplyr, tidyr, ggplot2, tibble\nlibrary(gt)\n\nset.seed(73914)\n\n# Vector of different numbers of days\ndays_vector &lt;- c(1, 5, 20, 100, 250)\n\n# Number of trials\nY &lt;- 10000 # trials; aka, sims\n\n# A full experiment; e.g., 2nd experiment will be 10,000 rows * 5 columns(= 5 days)\n# Each experiment has 10,000 rows but they have 5 | 20 | 100 | 250 columns\nsimulate_trials &lt;- function(X) {\n    simulations &lt;- matrix(rnorm(X * Y), nrow=Y)\n    return(simulations)\n}\n\n# List to of NULLs to store five simulations: 1, 5, 20, 100, 250 days\nsimulations_list &lt;- setNames(vector(\"list\", length(days_vector)), days_vector)\n\n# Do an experiment for each number of days\nsimulations_list &lt;- map(days_vector, ~ simulate_trials(.x))\n# This first LIST item is a matrix with 10,000 rows and 1 column\n# This second LIST item is a matrix with 10,000 rows and 5 (= horizon days) column\nstr(simulations_list[[1]])\n\n num [1:10000, 1] 0.21 0.337 -0.248 -0.744 -2.258 ...\n\nstr(simulations_list[[2]])\n\n num [1:10000, 1:5] -0.279 0.752 1.203 1.138 -0.157 ...\n\n# Function: Get the worst return for each row (trial)\nget_worst_returns &lt;- function(simulation) {\n    \n    # .x is the current row index in the iteration\n    # simulation[.x, ] selects the entire row because [x., ] is all columns\n    # such that ~ min(simulation[.x, ]) is the minimum value in the row\n    worst_returns &lt;- map_dbl(1:nrow(simulation), ~ min(simulation[.x, ]))\n    return(worst_returns)\n}\n\n# Get the worst returns for each set of days\nworst_returns_list &lt;- map(simulations_list, ~ get_worst_returns(.x))\n\n# Function: Get percentiles and mean\nget_percentiles_and_mean &lt;- function(returns) {\n    percentiles &lt;- quantile(returns, probs = c(0.01, 0.05, 0.1, 0.25, 0.5))\n    mean_val &lt;- mean(returns)\n    c(percentiles, mean = mean_val)\n}\n\n# Get them \npercentiles_and_mean_list &lt;- map(worst_returns_list, ~ get_percentiles_and_mean(.x))\n\n# Print percentiles and mean\npercentiles_and_mean_list\n\n[[1]]\n           1%            5%           10%           25%           50% \n-2.3408352635 -1.6258636416 -1.2747263559 -0.6772910421  0.0009027502 \n         mean \n-0.0080410293 \n\n[[2]]\n       1%        5%       10%       25%       50%      mean \n-2.901285 -2.337821 -2.049299 -1.598782 -1.133643 -1.169856 \n\n[[3]]\n       1%        5%       10%       25%       50%      mean \n-3.273848 -2.768015 -2.545114 -2.187286 -1.818529 -1.866486 \n\n[[4]]\n       1%        5%       10%       25%       50%      mean \n-3.735132 -3.284581 -3.072783 -2.760331 -2.462535 -2.506729 \n\n[[5]]\n       1%        5%       10%       25%       50%      mean \n-3.931741 -3.526280 -3.330623 -3.045503 -2.773540 -2.816351 \n\n# Name the list elements\nnames(percentiles_and_mean_list) &lt;- days_vector\n# has_rownames(percentiles_and_mean_list) # FALSE\n\n# The rest is awesome gt table stuff\npercentiles_df &lt;- as_tibble(percentiles_and_mean_list)\ndescriptive_stat &lt;- c(\"1 %ile\", \"5 %ile\", \"10 %ile\", \"25 %ile\", \"50 %ile\", \"Exp WCS (mean)\")\n\npercentiles_df &lt;- add_column(percentiles_df, descriptive_stat, .before = 1)\n\npercentiles_df_gt &lt;- percentiles_df |&gt; \n    gt(rowname_col = \"descriptive_stat\") |&gt; \n    fmt_number(\n        columns = c(`1`, `5`, `20`, `100`, `250`),\n        decimals = 2\n    ) |&gt;\n    tab_stubhead(label = \"Descriptive Stat\") |&gt; \n    tab_spanner(\n        label = \"Horizon in Days\",\n        columns = c('1', '5', '20', '100', '250')\n    ) |&gt; \n    tab_style(\n        style = cell_text(weight = \"bold\"),\n        locations = list(cells_column_labels(),\n                         cells_stubhead(),\n                         cells_column_spanners())\n    ) |&gt; \n    tab_options(\n        table.font.size = 14\n    ) |&gt; \n    data_color(\n        rows = 6,\n        palette = \"lightcyan1\"\n    ) |&gt; \n    data_color(\n        columns = 2,\n        palette = \"lightgrey\"\n    )\n    \npercentiles_df_gt\n\n\n\n\n\n  \n    \n    \n      Descriptive Stat\n      \n        Horizon in Days\n      \n    \n    \n      1\n      5\n      20\n      100\n      250\n    \n  \n  \n    1 %ile\n−2.34\n−2.90\n−3.27\n−3.74\n−3.93\n    5 %ile\n−1.63\n−2.34\n−2.77\n−3.28\n−3.53\n    10 %ile\n−1.27\n−2.05\n−2.55\n−3.07\n−3.33\n    25 %ile\n−0.68\n−1.60\n−2.19\n−2.76\n−3.05\n    50 %ile\n0.00\n−1.13\n−1.82\n−2.46\n−2.77\n    Exp WCS (mean)\n−0.01\n−1.17\n−1.87\n−2.51\n−2.82"
  },
  {
    "objectID": "posts/wcs_basic/index.html#worst-case-scenario-v1",
    "href": "posts/wcs_basic/index.html#worst-case-scenario-v1",
    "title": "Worst case scenario simulation (basic)",
    "section": "",
    "text": "Here is GARP’s lame explainer (source: GARP):\n\n\n\n\n\n\n2.11 WORST CASE ANALYSIS:\n\n\n\nOccasionally, when there are repeated trials, an analyst will calculate statistics for worst-case results. For example, if a portfolio manager reports results every week, he or she might ask what the worst result will be over a period of 52 weeks. If the distribution of the returns in one week is known, a Monte Carlo simulation can be used to calculate statistics for this worst-case result. For example, one can calculate the expected worst-case result over 52 weeks, the 95th percentile of the worst-case result, and so on.\n\n\nBelow simulations_list will contain five matrices\n\nThe first matrix is 10,000 rows by 1 column (H = 1 day; not realistic just illustrative)\nThe second matrix is 10,000 rows by 5 columns (H = 5 days)\nThe third matrix is 10,000 rows by 20 columns (H = 20 days)\n\nNote Linda Allen:\n\nIn contrast to VaR, WCS focuses on the distribution of the loss during the worst trading period (“period” being, e.g., one day or two weeks), over a given horizon (“horizon” being, e.g., 100 days or one year). The key point is that a worst period will occur with probability one.\n\nSo if each row is a trial (i.e., 10,000 rows = 10,000 trials), then we’re retrieving a vector of the worst period within the horizon for each of 10,000 trials. In this simulation, all periods are one day. So, the second matrix will retrieve (a vector of length 10,000 of) the worst one-day period in a five-day horizon. The third matrix will retrieve the worst one-day period in a 20-day horizon. The first matrix has only column, so the worst is the only value in the row: the statistics are the same.\nThe key function is worst_returns &lt;- map_dbl(1:nrow(simulation), ~ min(simulation[.x, ])). Because it finds the minimum (ie, worst) value in each of the 10,000 rows. That’s the worst_returns vector.\nHere is my interpretation, and the numbers are very similar to Linda Allen’s table.\n\nlibrary(tidyverse)\n# includes purrr, dplyr, tidyr, ggplot2, tibble\nlibrary(gt)\n\nset.seed(73914)\n\n# Vector of different numbers of days\ndays_vector &lt;- c(1, 5, 20, 100, 250)\n\n# Number of trials\nY &lt;- 10000 # trials; aka, sims\n\n# A full experiment; e.g., 2nd experiment will be 10,000 rows * 5 columns(= 5 days)\n# Each experiment has 10,000 rows but they have 5 | 20 | 100 | 250 columns\nsimulate_trials &lt;- function(X) {\n    simulations &lt;- matrix(rnorm(X * Y), nrow=Y)\n    return(simulations)\n}\n\n# List to of NULLs to store five simulations: 1, 5, 20, 100, 250 days\nsimulations_list &lt;- setNames(vector(\"list\", length(days_vector)), days_vector)\n\n# Do an experiment for each number of days\nsimulations_list &lt;- map(days_vector, ~ simulate_trials(.x))\n# This first LIST item is a matrix with 10,000 rows and 1 column\n# This second LIST item is a matrix with 10,000 rows and 5 (= horizon days) column\nstr(simulations_list[[1]])\n\n num [1:10000, 1] 0.21 0.337 -0.248 -0.744 -2.258 ...\n\nstr(simulations_list[[2]])\n\n num [1:10000, 1:5] -0.279 0.752 1.203 1.138 -0.157 ...\n\n# Function: Get the worst return for each row (trial)\nget_worst_returns &lt;- function(simulation) {\n    \n    # .x is the current row index in the iteration\n    # simulation[.x, ] selects the entire row because [x., ] is all columns\n    # such that ~ min(simulation[.x, ]) is the minimum value in the row\n    worst_returns &lt;- map_dbl(1:nrow(simulation), ~ min(simulation[.x, ]))\n    return(worst_returns)\n}\n\n# Get the worst returns for each set of days\nworst_returns_list &lt;- map(simulations_list, ~ get_worst_returns(.x))\n\n# Function: Get percentiles and mean\nget_percentiles_and_mean &lt;- function(returns) {\n    percentiles &lt;- quantile(returns, probs = c(0.01, 0.05, 0.1, 0.25, 0.5))\n    mean_val &lt;- mean(returns)\n    c(percentiles, mean = mean_val)\n}\n\n# Get them \npercentiles_and_mean_list &lt;- map(worst_returns_list, ~ get_percentiles_and_mean(.x))\n\n# Print percentiles and mean\npercentiles_and_mean_list\n\n[[1]]\n           1%            5%           10%           25%           50% \n-2.3408352635 -1.6258636416 -1.2747263559 -0.6772910421  0.0009027502 \n         mean \n-0.0080410293 \n\n[[2]]\n       1%        5%       10%       25%       50%      mean \n-2.901285 -2.337821 -2.049299 -1.598782 -1.133643 -1.169856 \n\n[[3]]\n       1%        5%       10%       25%       50%      mean \n-3.273848 -2.768015 -2.545114 -2.187286 -1.818529 -1.866486 \n\n[[4]]\n       1%        5%       10%       25%       50%      mean \n-3.735132 -3.284581 -3.072783 -2.760331 -2.462535 -2.506729 \n\n[[5]]\n       1%        5%       10%       25%       50%      mean \n-3.931741 -3.526280 -3.330623 -3.045503 -2.773540 -2.816351 \n\n# Name the list elements\nnames(percentiles_and_mean_list) &lt;- days_vector\n# has_rownames(percentiles_and_mean_list) # FALSE\n\n# The rest is awesome gt table stuff\npercentiles_df &lt;- as_tibble(percentiles_and_mean_list)\ndescriptive_stat &lt;- c(\"1 %ile\", \"5 %ile\", \"10 %ile\", \"25 %ile\", \"50 %ile\", \"Exp WCS (mean)\")\n\npercentiles_df &lt;- add_column(percentiles_df, descriptive_stat, .before = 1)\n\npercentiles_df_gt &lt;- percentiles_df |&gt; \n    gt(rowname_col = \"descriptive_stat\") |&gt; \n    fmt_number(\n        columns = c(`1`, `5`, `20`, `100`, `250`),\n        decimals = 2\n    ) |&gt;\n    tab_stubhead(label = \"Descriptive Stat\") |&gt; \n    tab_spanner(\n        label = \"Horizon in Days\",\n        columns = c('1', '5', '20', '100', '250')\n    ) |&gt; \n    tab_style(\n        style = cell_text(weight = \"bold\"),\n        locations = list(cells_column_labels(),\n                         cells_stubhead(),\n                         cells_column_spanners())\n    ) |&gt; \n    tab_options(\n        table.font.size = 14\n    ) |&gt; \n    data_color(\n        rows = 6,\n        palette = \"lightcyan1\"\n    ) |&gt; \n    data_color(\n        columns = 2,\n        palette = \"lightgrey\"\n    )\n    \npercentiles_df_gt\n\n\n\n\n\n  \n    \n    \n      Descriptive Stat\n      \n        Horizon in Days\n      \n    \n    \n      1\n      5\n      20\n      100\n      250\n    \n  \n  \n    1 %ile\n−2.34\n−2.90\n−3.27\n−3.74\n−3.93\n    5 %ile\n−1.63\n−2.34\n−2.77\n−3.28\n−3.53\n    10 %ile\n−1.27\n−2.05\n−2.55\n−3.07\n−3.33\n    25 %ile\n−0.68\n−1.60\n−2.19\n−2.76\n−3.05\n    50 %ile\n0.00\n−1.13\n−1.82\n−2.46\n−2.77\n    Exp WCS (mean)\n−0.01\n−1.17\n−1.87\n−2.51\n−2.82"
  }
]