[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To FinEdTech",
    "section": "",
    "text": "Welcome to my data science blog!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/logistic-coeff/index.html",
    "href": "posts/logistic-coeff/index.html",
    "title": "Logistic regression coefficients",
    "section": "",
    "text": "I wanted to shadow GARP’s logistic regression example, so I sampled from the same LendingClub database and performed a similar logistic regression. The key difference is practical: I’ll often re-sample from the data in order to get a result that lends itself to a good practice question. I’ve been writing practice questions for a long time, and there are many little details that go into this. For example, GARP’s logistic regression shows 10 independent variables, and I reduced that to seven merely because I don’t need to show all the variables to make the point.\nAfter I seeded the result that appealed to me, I wrote the practice question below (the published question sans answer is here). After fiddling with the four choices, I’m happy with the final question. It’s an “EXCEPT FOR” question, which is what I often use when I’m trying to blanked the concept more comprehensively than a “TRUE” question. This is a bit more work because each distractor must be carefully written.\n\n23.6.1. Darlene is a risk analyst who evaluate the creditworthiness of loan applicants at her financial institution. Her department is testing a new logistic regression model. If the model performs well in testing, it will be deployed to assist in the underwriting decision-making process. The training data is a sub-sample (n = 800) from the same LendingClub database used in reading. In the logistic regression, the dependent variable is a 0/1 for the terminal state of the loan being either zero (fully paid off) or one (deemed irrecoverable or defaulted). In the actual code, this dependent variable is labeled ‘outcome’.\nThe following are the features (aka, independent variables) as given by their textual labels: Amount, Term, Interest_rate, Installment, Employ_hist, Income, and Bankruptcies. In regard to units in the database, please note the following: Amount is thousands of dollars ($000s); Term is months; Interest_rate is effectively multiplied by one hundred such that 7 equates to 7% or 0.070; Installment is dollars; Employment_hist is years; Income is thousand of dollars ($000); and Bankruptcies is a whole number {0, 1, 2, …}.\nThe table below displays the logistic regression results:\n&lt;&lt;See regression output below; table will paste here&gt;&gt;\n\n\nIn regard to this logistic regression, each of the following statements is true EXCEPT which is false?\n\nA single additional bankruptcy increases the expected odds of default by almost 58 percent\nIf she requires significance at the 5% level or better, then two of the coefficients (in addition to the intercept) are significant\nEach +100 basis points increase in the interest rate (e.g., from 8.0% to 9.0%) implies an increase of about 14.0 basis points in the default probability\nIf the cost of making a bad loan is high, she can decrease the threshold (i.e., set Z to a low value such as 0.05), but this will reject more good borrowers\n\n\nHere is the code with some comments. The logistic model itself, a type of glm(), requires only a single line and the model is stored in logit_model_1 as a list object. Most of my code is re-coding the dataset, and then rendering the model’s coefficients with the awesome gt package. Posit’s Richard Iannone does an incredible job maintaining the package. If you think about it, generating tables are really important in data!\n\nlibrary(tidyverse) \nlibrary(gt)\n# library(labelled) Didn't use but helpful\n\n# set.seed(xzy)\nset.seed(374)\n\nsample_size &lt;- 800\nlcfeatures &lt;- read_csv(\"lcfeatures.csv\") \n# Same LendingClub dataset used in FRM Chapter 15 (Logistic Regression Example)\n# Located at https://www.kaggle.com/datasets/wordsforthewise/lending-club\n# But lcfeatures is a random sample of 10,000 which is too large for my need\n# So I just sample_n as random subset of the 10,000\nlcfeatures &lt;- lcfeatures |&gt; sample_n(sample_size)\n\n# recoding \nlcfeatures$emp_length_n &lt;- gsub(\"&lt; 1\", \"0\", lcfeatures$emp_length)\nlcfeatures$emp_length_n2 &lt;- parse_number(lcfeatures$emp_length_n)\nlcfeatures$term_n &lt;- parse_number(lcfeatures$term)\n\nlcfeatures$home_ownership_simpler &lt;- recode(lcfeatures$home_ownership,\n                                             \"MORTGAGE\" = \"OWN\",\n                                             \"ANY\" = \"RENT\",\n                                             \"NONE\" = \"RENT\")\n\nlcfeatures$mortgage_simpler &lt;- recode(lcfeatures$home_ownership,\n                                       \"OWN\" = \"NO\",\n                                       \"ANY\" = \"NO\",\n                                       \"NONE\" = \"NO\",\n                                       \"RENT\" = \"NO\",\n                                       \"MORTGAGE\" = \"YES\")\n\nlcfeatures$loan_status_coded &lt;- recode(lcfeatures$loan_status,\n                                        \"Charged Off\" = \"Default\",\n                                        \"Does not meet the credit policy. Status:Charged Off\" = \"Default\",\n                                        \"Late (31-120 days)\" = \"Default\",\n                                        .default = \"Paid\")\n\nlcfeatures$home_ownership_bern &lt;- recode(lcfeatures$home_ownership_simpler,\n                                          \"RENT\" = 0,\n                                          \"OWN\" = 1)\n\nlcfeatures$mortgage_bern &lt;- recode(lcfeatures$mortgage_simpler,\n                                          \"NO\" = 0,\n                                          \"YES\" = 1)\n\nlcfeatures$loan_status_bern &lt;- recode(lcfeatures$loan_status_coded,\n                                          \"Paid\" = 0,\n                                          \"Default\" = 1)\n\nlcfeatures$loan_amnt_000 &lt;- lcfeatures$loan_amnt / 1000\nlcfeatures$annual_inc_000 &lt;- lcfeatures$annual_inc / 1000\nlcfeatures$outcome &lt;- lcfeatures$loan_status_bern\n\n# This is logistic regression model\nlogit_model_1 &lt;- glm(formula = outcome ~ loan_amnt_000 + term_n + int_rate + installment + \n        emp_length_n2 + annual_inc_000 + pub_rec_bankruptcies,\n        family = binomial(link = \"logit\"), data = lcfeatures)\n\ncoef_table &lt;- coef(summary(logit_model_1)) \ncoef_tbl  &lt;-  as_tibble(coef_table)\nCoeff_labels &lt;- c(\"(Intercept)\", \"Amount\", \"Term\", \"Interest_rate\", \"Installment\", \n                 \"Employment_hist\", \"Income\",\"Bankruptcies\")\ncoef_tbl &lt;- cbind(Coeff_labels, coef_tbl)\n\n# Using gt() to render a table\ncoef_tbl_gt &lt;- coef_tbl %&gt;% gt() |&gt; \n    opt_table_font(stack = \"humanist\") |&gt;\n    fmt_number(columns = everything(),\n               decimals = 3)\ncoef_tbl_gt\n\n\n\n\n\n  \n    \n    \n      Coeff_labels\n      Estimate\n      Std. Error\n      z value\n      Pr(&gt;|z|)\n    \n  \n  \n    (Intercept)\n−2.329\n0.841\n−2.769\n0.006\n    Amount\n0.123\n0.092\n1.339\n0.181\n    Term\n−0.041\n0.027\n−1.519\n0.129\n    Interest_rate\n0.140\n0.034\n4.108\n0.000\n    Installment\n−0.003\n0.003\n−1.033\n0.302\n    Employment_hist\n−0.032\n0.031\n−1.025\n0.305\n    Income\n−0.003\n0.003\n−0.937\n0.349\n    Bankruptcies\n0.457\n0.230\n1.991\n0.046\n  \n  \n  \n\n\n\n\nIf we use predict() with type = “response”, then the logistic regression returns the vector of predicted probabilities (from zero to 100%). We can classify the Bernoulli prediction (0 = nondefault, 1 = default) as a function of our desired conservative/aggressive threshold. Below I show the number of rejections would increase as we lower the threshold.\n\npredicted_probs &lt;- predict(logit_model_1, lcfeatures, type = \"response\")\nthresholds &lt;- c(0.4, 0.3, 0.2, 0.1, 0.05, 0.010)\nthresholds |&gt; map_int(\\(x) sum(ifelse(predicted_probs &gt; x, 1, 0), na.rm = TRUE))\n\n[1]   5  30  86 383 689 748"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About David",
    "section": "",
    "text": "Hello! I’m David Harper, CFA, FRM. I started bionicturtle.com in 2004 and built it into the #1 exam preparation provider (EPP) for the Financial Risk Manager (FRM). In February of 2021, I sold the business to CeriFi where I continue to run it as President, Bionic Turtle, a CeriFi company. And now I’m part of a great team that is building the world’s greatest platform for financial education at every stage of your career.\nI love learning about (and teaching) risk. I also write investing articles at Seeking Alpha, where I disclose my portfolio/trades and practice risk management. My focus is disruptive technologies, patient GARP-style with a portfolio risk overlay; e.g., diversification. The market wants growth but I’m a value technician at heart, consequently I am willing to wait for great companies to prove their worth to the general audience. I have a BA, Economics from UC, Berkeley. I earned my CFA in 2003 and my FRM in 2004.\nI’ve been training and practicing data science for several years. Of course, my preferred data tool is R (aka, #RStats, #rstat). The best thing about R is the amazing community, there is nothing quite like an RStudio conference. Online I try to keep sharp with Coursera, datacamp and perhaps the most talented data science instructor on the planet: Matt Dancho’s Business Science.\nI am extremely interested (and invested) in digital disruption and the future of work (FOW), including EdTech, talent marketplaces and work management (my favorite software is wrike).\nThis is a Quarto site using the built-in sandstone theme"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FinEdTech by DH",
    "section": "",
    "text": "Logistic regression coefficients\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nFitting a logistic regression model is easy in R, but coefficient interpretation is non-trivial\n\n\n\n\n\n\nSep 4, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nSimulating the equity risk premium\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nThe implied ERP is very sensitive to assumptions, in particular G2\n\n\n\n\n\n\nAug 31, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To FinEdTech\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nDavid Harper\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/sim-equity-risk-premium/index.html",
    "href": "posts/sim-equity-risk-premium/index.html",
    "title": "Simulating the equity risk premium",
    "section": "",
    "text": "The following implements the implied ERP approach in Professor Damodaran’s post on the The Price of Risk. My intention is to briefly explore its sensitivity to assumptions.\n\nlibrary(tidyverse)\nlibrary(scales)\n\nsolve_for_R &lt;- function(RF, ER_vector, CP_vector, G2, PV) {\n  \n  # Calculate cash flow vector\n  CF_vector &lt;- ER_vector * CP_vector\n  \n  # Define the objective function\n  objective_function &lt;- function(R) {\n    # This is effectively two-stage dividend discount model except the initial stage is explicated\n    # such that there exists no G1 and G2 refers to the subsequent period of growth\n    \n    PV_calculated &lt;- sum(CF_vector[1:5] / (1 + R)^(1:5)) + CF_vector[6] / ((R - G2) * (1 + R)^5)\n    return((PV - PV_calculated)^2)\n  }\n  \n  # Use the optim function to minimize the objective function\n  result &lt;- optim(par = RF, fn = objective_function, method = \"Brent\", lower = -1, upper = 2)\n  \n  return(result$par)\n}\n\nRF &lt;- 0.04 # I have rounded his riskfree rate of 3.97% to 4.00%\nER_vector &lt;- c(217.8, 245.2, 273.7, 295.1, 308.9, 324.9) # A. Damodaran's earnings vector\nCP_vector &lt;- c(0.84, 0.82, 0.80, 0.78, 0.77, 0.77) # Cash payout ratios\nG2 &lt;- 0.04 # His model sets the stable growth equal to the RF rate\nPV &lt;- 4600 # I rounded 4588.96 to 4,600\n\nimplied_equity &lt;- solve_for_R(RF, ER_vector, CP_vector, G2, PV)\nimplied_ERP &lt;- implied_equity - RF\n\n# Number of simulations\nn_simulations &lt;- 10000\ncoeff_variation &lt;- 0.10 # Arbitrarily suggesting that COV of 10% is tight\n\n# Assumed means and standard deviations for inputs\nmean_RF &lt;- RF; sd_RF &lt;- RF * coeff_variation\nmean_ER &lt;- ER_vector; sd_ER &lt;- ER_vector * coeff_variation\nmean_CP &lt;- CP_vector; sd_CP &lt;- CP_vector * coeff_variation\nmean_G2 &lt;- G2; sd_G2 &lt;- G2 * coeff_variation\nmean_PV &lt;- PV; sd_PV &lt;- PV * coeff_variation\n\n# MC simulation\nset.seed(379)\nR_values &lt;- replicate(n_simulations, \n  solve_for_R(\n    # RF = rnorm(1, mean_RF, sd_RF),\n    RF = RF,\n    ER_vector = rnorm(6, mean_ER, sd_ER),\n    CP_vector = rnorm(6, mean_CP, sd_CP),\n    G2 = rnorm(1, mean_G2, sd_G2),\n    PV = PV\n  )\n)\n\n# Histogram to visualize the distribution of R values\nR_values &lt;- R_values[R_values &gt; 0]\nERP_values &lt;- R_values - RF\nERP_values_mean &lt;- mean(ERP_values)\nERP_values_df &lt;- as_data_frame(ERP_values)\n\nERP_values_df %&gt;% ggplot(aes(value)) +\n    geom_histogram(color = \"darkblue\", fill = \"lightblue\") +\n    geom_vline(aes(xintercept = ERP_values_mean), color = \"darkgreen\", size = 1.5) +\n    scale_x_continuous(labels = percent_format(0.01)) +\n    labs(title = \"Implied equity risk premium, ERP (n = 10,000 sims)\",\n         subtitle = \"Under tight assumption dispersion (CV = σ/μ =10%). Green vertical line is the mean.\",\n         y = \"Count\") +\n    # xlab(\"X label\") + \n    # ylab(\"Count\") +\n    theme_classic() +\n    theme(axis.title = element_blank(),\n          axis.text = element_text(size = 12, face = \"bold\"))\n\n\n\n\nQuick check on the distribution:\n\nlibrary(moments)\nskewness(ERP_values_df$value)\n\n[1] 0.08903814\n\nkurtosis(ERP_values_df$value)\n\n[1] 2.997992\n\nquantiles_v &lt;- c(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975, 0.99)\nquantile(ERP_values_df$value, probs = quantiles_v)\n\n        1%       2.5%         5%        10%        25%        50%        75% \n0.03041180 0.03248300 0.03433620 0.03649075 0.04004356 0.04410500 0.04832614 \n       90%        95%      97.5%        99% \n0.05209407 0.05447292 0.05638444 0.05881702 \n\n\nWhat is the relationship between the sustainable growth rate, G2, and the ERP?\n\nG2_values &lt;- seq(from = 0.02, to = 0.06, by = 0.001)\nR_values &lt;- map_dbl(G2_values, function(G2) {\n  solve_for_R(\n    RF = RF,\n    ER_vector = ER_vector,\n    CP_vector = CP_vector,\n    G2 = G2,\n    PV = PV\n  )\n})\n\nERP_values &lt;- R_values - RF\n\nG_vs_ERP &lt;-  tibble(\n  G2 = G2_values,\n  ERP = ERP_values\n)\n\nG_vs_ERP %&gt;% ggplot(aes(x = G2, y = ERP)) + \n  geom_point() + \n  coord_cartesian(ylim = c(.02, .08)) + \n  labs(title = \"Implied ERP as function of sustainable growth rate, G2\",\n       subtitle = \"Unlike prior/next visualization, predicted vectors are not randomized\")\n\n\n\n\nAnd just for fun, let’s add randomness to the earnings and cash payout vectors:\n\nG2_values &lt;- seq(from = 0.02, to = 0.06, by = 0.001)\n\nR_values &lt;- map(G2_values, function(G2) {\n  replicate(30, {\n    solve_for_R(\n      RF = RF,\n      ER_vector = rnorm(6, mean_ER, sd_ER),\n      CP_vector = rnorm(6, mean_CP, sd_CP),\n      G2 = G2,\n      PV = PV\n    ) - RF # subtracting RF here inside replicat\n  })\n})\n\ndf &lt;- tibble(\n  G2 = G2_values,\n  ERP = R_values\n) %&gt;% unnest()\n\nmodel_line &lt;- lm(ERP ~ G2, data = df)\nrsq &lt;- summary(model_line)$r.squared\nlabel_R2 &lt;- sprintf(\"R^2 = %.2f\", rsq)\n\ndf %&gt;% ggplot(aes(x = G2, y = ERP)) + \n  geom_point() +\n  coord_cartesian(ylim = c(.02, .08)) + \n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Restores 10% CV randomness to earnings and payout vectors\") +\n  annotate(\"text\", x=0.025, y=0.065, label=label_R2, fontface=\"bold\", hjust=0)\n\n\n\n  # geom_text(aes(label = label_R2))"
  }
]