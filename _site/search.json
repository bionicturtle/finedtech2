[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FinEdTech by DH",
    "section": "",
    "text": "PPC v2\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nDescription here\n\n\n\n\n\n\nNov 22, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nPortfolio Possibilities Curve (PPC)\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nPPC with GPT4 as my coding partner\n\n\n\n\n\n\nNov 19, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nVolatility is a model\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nDescription here\n\n\n\n\n\n\nNov 12, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nValue at Risk (VaR) Introduction\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nHistorical simulation (basic + bootstrap, MCS, and parametric)\n\n\n\n\n\n\nNov 5, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nWorst case scenario simulation (basic)\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nThe expected worst loss over 10,000 trials for different horizons\n\n\n\n\n\n\nNov 2, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nLogistic regression\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nSimulated insurance dataset (via simdata) and visualization of marginal effects (via ggeffects)\n\n\n\n\n\n\nOct 29, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nUnivariate regression\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nDescription to go here\n\n\n\n\n\n\nOct 22, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nNearest neighbors\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nThis lazy learning algorithm requires us to select k, but it’s a fast and intuitive classifer\n\n\n\n\n\n\nOct 13, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nDecision Trees (part 1)\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nJust some description here for now\n\n\n\n\n\n\nOct 7, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nThis is a Quarto website\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nPosit’s brilliant system executes either R or python code (and a GPT API example)\n\n\n\n\n\n\nSep 15, 2023\n\n\nDavid Harper\n\n\n\n\n\n\n  \n\n\n\n\nLogistic regression coefficients\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nFitting a logistic regression model is easy in R, but coefficient interpretation is non-trivial\n\n\n\n\n\n\nSep 4, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nSimulating the equity risk premium\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nThe implied ERP is very sensitive to assumptions, in particular G2\n\n\n\n\n\n\nAug 31, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To FinEdTech\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nDavid Harper\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ppc/index.html",
    "href": "posts/ppc/index.html",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "",
    "text": "Contents\nload libraries\nlibrary(tidyverse)\nlibrary(tidyquant)\n# library(dplyr); library(tidyr); library(purrr)\n# library(ggplot)"
  },
  {
    "objectID": "posts/ppc/index.html#define-the-sets-of-stocksindices",
    "href": "posts/ppc/index.html#define-the-sets-of-stocksindices",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Define the SETS of stocks/indices",
    "text": "Define the SETS of stocks/indices\nThe container in this approach is stock_sets, a dataframe the we initialize (our TOC) with three columns:\n\nset_id\ndescription\nsymbols: a list of tickers\n\n\nsector_3eft_list &lt;- c( \"XLK\", \"XLV\", \"XLP\") # Tech, Health, Staples\n# sector_4etf_list &lt;- c( \"XLK\", \"XLV\", \"XLP\",  \"XLE\",) # Tech, Health, Staples, Energy\nsector_5etf_list &lt;- c( \"XLK\", \"XLV\", \"XLP\", \"XLE\", \"XLF\") # Tech, Health, Staples, Energy, Financials\n# sector_7etf_list &lt;- c(sector_5etf_list, \"XLI\", \"XLU\")\nsector_11etf_list &lt;- c(\"XLK\",  # Technology\n                      \"XLV\",  # Health Care\n                      \"XLF\",  # Financials\n                      \"XLY\",  # Consumer Discretionary\n                      \"XLP\",  # Consumer Staples\n                      \"XLE\",  # Energy\n                      \"XLU\",  # Utilities\n                      \"XLI\",  # Industrials\n                      \"XLB\",  # Materials\n                      \"XLRE\", # Real Estate\n                      \"XLC\") # Communication Services\n\nsize_etfs &lt;- c(\"SPY\", \"MDY\", \"IWM\") # Large, Mid, Small\n# size_style_etfs &lt;- c(\"IWF\",  # Large-Cap Growth\n#                      \"IWD\",  # Large-Cap Value\n#                      \"SPY\",  # Large-Cap Blend\n#                      \"IWP\",  # Mid-Cap Growth\n#                      \"IWS\",  # Mid-Cap Value\n#                      \"MDY\",  # Mid-Cap Blend\n#                      \"IWO\",  # Small-Cap Growth\n#                      \"IWN\",  # Small-Cap Value\n#                      \"IWM\")  # Small-Cap Blend\n\nstock_sets &lt;- tibble(\n    set_id = c(\"3_sectors\",\n               \"5_sectors\", \n               \"11_sectors\",\n               \"3_sizes\"),\n    \n    description = c(\"3 Sectors picked by GPT-4: Tech, Health, Staples\",\n                    \"5 Sectors picked by GPT-4: above + Energy + Financials\",\n                    \"All  11 Sectors\",\n                    \"Size: Large, Mid, Small--Blend\"),\n    \n    # this is a list column, see https://adv-r.hadley.nz/vectors-chap.html#list-columns \n    symbols = list(sector_3eft_list, sector_5etf_list, sector_11etf_list, size_etfs)\n    )\n\ndate_start &lt;- \"2013-01-01\"\ndate_end   &lt;- \"2023-11-17\""
  },
  {
    "objectID": "posts/ppc/index.html#retrieve-returns-for-each-periodicity-aka-frequency",
    "href": "posts/ppc/index.html#retrieve-returns-for-each-periodicity-aka-frequency",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Retrieve returns for each periodicity; aka, frequency",
    "text": "Retrieve returns for each periodicity; aka, frequency\nFor each SET of tickers, get_returns will retrieve log returns for each of three periods:\n\ndaily\nweekly\nmonthly\n\nThen we will call the get_returns function via map (my favorite function) to create a new list column called nested_data. Each row of nested_data will contain a list of three dataframes, one for each period. These dataframes will contain the log returns for each ticker in the set.\n\nget_returns &lt;- function(symbols, start_date, end_date) {\n    mult_stocks &lt;- tq_get(symbols, get = \"stock.prices\", \n                          from = start_date, to = end_date)\n    \n    periods &lt;- c(\"daily\", \"weekly\", \"monthly\")\n    returns_list &lt;- lapply(periods, function(period) {\n        mult_stocks |&gt; \n            group_by(symbol) |&gt; \n            tq_transmute(select = adjusted,\n                         mutate_fun = periodReturn, \n                         period = period, \n                         type = \"log\")\n    })\n    \n    names(returns_list) &lt;- periods\n    return(returns_list)\n}\n\n# Nest return data for each stock set\nstock_sets &lt;- stock_sets |&gt; \n    mutate(nested_data = map(symbols, \n                             ~ get_returns(.x, date_start, date_end)))\n\nprint(stock_sets)\n\n# A tibble: 4 × 4\n  set_id     description                                    symbols nested_data \n  &lt;chr&gt;      &lt;chr&gt;                                          &lt;list&gt;  &lt;list&gt;      \n1 3_sectors  3 Sectors picked by GPT-4: Tech, Health, Stap… &lt;chr&gt;   &lt;named list&gt;\n2 5_sectors  5 Sectors picked by GPT-4: above + Energy + F… &lt;chr&gt;   &lt;named list&gt;\n3 11_sectors All  11 Sectors                                &lt;chr&gt;   &lt;named list&gt;\n4 3_sizes    Size: Large, Mid, Small--Blend                 &lt;chr&gt;   &lt;named list&gt;"
  },
  {
    "objectID": "posts/ppc/index.html#analyze-add-the-analysis-list-column",
    "href": "posts/ppc/index.html#analyze-add-the-analysis-list-column",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Analyze: add the analysis list column",
    "text": "Analyze: add the analysis list column\nFor each set and periodicity, the analysis list column generates:\n\nvector of volatilities\nvector of average returns\ncorrelation matrix (diagonal is 1)\naverage correlation (as a rough measure of diversification)\n\n\nperform_analysis &lt;- function(data, returns_column) {\n    volatilities &lt;- data |&gt;  \n        group_by(symbol) |&gt;  \n        summarise(volatility = sd(.data[[returns_column]], na.rm = TRUE)) |&gt;  \n        ungroup()\n    \n    avg_returns &lt;- data |&gt;  \n        group_by(symbol) |&gt;  \n        summarise(avg_return = mean(.data[[returns_column]], na.rm = TRUE)) |&gt;  \n        ungroup()\n    \n    data_wide &lt;- data |&gt;  \n        pivot_wider(names_from = symbol, values_from = .data[[returns_column]])\n    corr_matrix &lt;- cor(select(data_wide, -date), use = \"complete.obs\")\n    avg_corr &lt;- mean(corr_matrix[lower.tri(corr_matrix)])\n    \n    return(list(volatilities = volatilities, avg_returns = avg_returns, corr_matrix = corr_matrix, avg_corr = avg_corr))\n}\n\n# Applying the perform_analysis function to the stock_sets\nstock_sets &lt;- stock_sets |&gt; \n    mutate(analysis = map(nested_data, ~ {\n        data_daily &lt;- .x$daily\n        data_weekly &lt;- .x$weekly\n        data_monthly &lt;- .x$monthly\n        \n        analysis_daily &lt;- perform_analysis(data_daily, \"daily.returns\")\n        analysis_weekly &lt;- perform_analysis(data_weekly, \"weekly.returns\")\n        analysis_monthly &lt;- perform_analysis(data_monthly, \"monthly.returns\")\n        \n        list(daily = analysis_daily, weekly = analysis_weekly, monthly = analysis_monthly)\n    }))\n\n# Examine data structure \nprint(stock_sets) # Notice the analysis list column has been added\n\n# A tibble: 4 × 5\n  set_id     description                       symbols nested_data  analysis    \n  &lt;chr&gt;      &lt;chr&gt;                             &lt;list&gt;  &lt;list&gt;       &lt;list&gt;      \n1 3_sectors  3 Sectors picked by GPT-4: Tech,… &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n2 5_sectors  5 Sectors picked by GPT-4: above… &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n3 11_sectors All  11 Sectors                   &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n4 3_sizes    Size: Large, Mid, Small--Blend    &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n\nglimpse(stock_sets)\n\nRows: 4\nColumns: 5\n$ set_id      &lt;chr&gt; \"3_sectors\", \"5_sectors\", \"11_sectors\", \"3_sizes\"\n$ description &lt;chr&gt; \"3 Sectors picked by GPT-4: Tech, Health, Staples\", \"5 Sec…\n$ symbols     &lt;list&gt; &lt;\"XLK\", \"XLV\", \"XLP\"&gt;, &lt;\"XLK\", \"XLV\", \"XLP\", \"XLE\", \"XLF\"&gt;…\n$ nested_data &lt;list&gt; [[&lt;grouped_df[8217 x 3]&gt;], [&lt;grouped_df[1704 x 3]&gt;], [&lt;gr…\n$ analysis    &lt;list&gt; [[[&lt;tbl_df[3 x 2]&gt;], [&lt;tbl_df[3 x 2]&gt;], &lt;&lt;matrix[3 x 3]&gt;&gt;…\n\nstock_sets$analysis[[1]] # first row is the first stock set\n\n$daily\n$daily$volatilities\n# A tibble: 3 × 2\n  symbol volatility\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK       0.0138 \n2 XLP       0.00906\n3 XLV       0.0105 \n\n$daily$avg_returns\n# A tibble: 3 × 2\n  symbol avg_return\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK      0.000719\n2 XLP      0.000345\n3 XLV      0.000484\n\n$daily$corr_matrix\n          XLK       XLV       XLP\nXLK 1.0000000 0.7254977 0.6357681\nXLV 0.7254977 1.0000000 0.7182649\nXLP 0.6357681 0.7182649 1.0000000\n\n$daily$avg_corr\n[1] 0.6931769\n\n\n$weekly\n$weekly$volatilities\n# A tibble: 3 × 2\n  symbol volatility\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK        0.0270\n2 XLP        0.0194\n3 XLV        0.0227\n\n$weekly$avg_returns\n# A tibble: 3 × 2\n  symbol avg_return\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK       0.00347\n2 XLP       0.00167\n3 XLV       0.00233\n\n$weekly$corr_matrix\n          XLK       XLV       XLP\nXLK 1.0000000 0.6976196 0.6515028\nXLV 0.6976196 1.0000000 0.7046562\nXLP 0.6515028 0.7046562 1.0000000\n\n$weekly$avg_corr\n[1] 0.6845929\n\n\n$monthly\n$monthly$volatilities\n# A tibble: 3 × 2\n  symbol volatility\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK        0.0517\n2 XLP        0.0364\n3 XLV        0.0399\n\n$monthly$avg_returns\n# A tibble: 3 × 2\n  symbol avg_return\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK       0.0150 \n2 XLP       0.00722\n3 XLV       0.0101 \n\n$monthly$corr_matrix\n          XLK       XLV       XLP\nXLK 1.0000000 0.6136564 0.5679385\nXLV 0.6136564 1.0000000 0.6780583\nXLP 0.5679385 0.6780583 1.0000000\n\n$monthly$avg_corr\n[1] 0.6198844\n\n\nHere is a class diagram of the stock_sets data structure rendered by DiagrammeR via Graphviz."
  },
  {
    "objectID": "posts/ppc/index.html#setup-the-simulation",
    "href": "posts/ppc/index.html#setup-the-simulation",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Setup the simulation",
    "text": "Setup the simulation\nThe get_random_weights function returns a dataframe of random weights. Each column is a set of weights for a single simulation. Each row is the weight for a single stock. The weights are normalized so that they sum to 1.\nSo I’m starting with an incredibly naive approach to the simulation. I’m going to assume that the expected return for each stock is the average return for that stock over the entire period. I’m also going to assume that the volatility for each stock is the average volatility for that stock over the entire period. Most importantly, the only randomness in the simulation is the weights and they are totally naive because they are independent of the analysis. We can’t expect anything like an efficient frontier from the raw scatterplot. However, this will illustrate the future risk/reward trade-off faced by a “totally naive” investor!\n\n# returns a data frame of random weights\n# rows = weight per stock; columns = number of simulations\nget_random_weights &lt;- function(num_stocks, num_simulations) {\n    set.seed(123)\n    weights_df &lt;- matrix(nrow = num_stocks, ncol = num_simulations)\n\n    for (i in 1:num_simulations) {\n        weights &lt;- runif(num_stocks)\n        weights_df[, i] &lt;- weights / sum(weights)\n    }\n\n    return(as.data.frame(weights_df))\n}\n\n# single simulation: given a set of weights, computes the expected return and volatility\nport_sim &lt;- function(exp_returns, volatilities, corr_matrix, weights) {\n    \n    cov_matrix &lt;- outer(volatilities, volatilities) * corr_matrix\n    port_variance &lt;- t(weights) %*% cov_matrix %*% weights\n    port_exp_return &lt;- sum(weights * exp_returns)\n\n    return(list(exp_returns = exp_returns, \n                volatilities = volatilities,\n                cov_matrix = cov_matrix, \n                corr_matrix = corr_matrix,\n                port_variance = port_variance,\n                port_exp_return = port_exp_return))\n}\n\n# runs a port_simulation for each column in the weights_df\nrun_sims &lt;- function(exp_returns, volatilities, corr_matrix, weights_df) {\n    simulations &lt;- map(1:ncol(weights_df), ~ {\n        weights_vector &lt;- weights_df[, .x]\n        port_sim(exp_returns, volatilities, corr_matrix, weights_vector)\n        })\n    \n    return(simulations)\n}"
  },
  {
    "objectID": "posts/ppc/index.html#run-the-simulation-on-a-single-set",
    "href": "posts/ppc/index.html#run-the-simulation-on-a-single-set",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Run the simulation (on a single set)",
    "text": "Run the simulation (on a single set)\n\n# Selecting the desired set (e.g., \"Set 1\")\nselect_set &lt;- stock_sets |&gt; \n    filter(set_id == \"5_sectors\") |&gt; \n    pull(\"analysis\")\n\nanalyze_set &lt;- select_set[[1]]\nanalyze_period &lt;- analyze_set$monthly\n\n# Extracting components from the selected set\nexp_returns_period &lt;- analyze_period$avg_returns$avg_return\nvolatilities_period &lt;- analyze_period$volatilities$volatility\ncorr_matrix_period &lt;- analyze_period$corr_matrix\nnum_stocks_period &lt;- length(volatilities_period)\n\nnum_sims &lt;- 20000  # Set the number of simulations\nrandom_weights_df_period &lt;- get_random_weights(num_stocks_period, num_sims)\nsim_results_period &lt;- run_sims(exp_returns_period, \n                              volatilities_period, \n                              corr_matrix_period, \n                              random_weights_df_period)\n\n# Print results of the first simulation\nprint(sim_results_period[[1]])\n\n$exp_returns\n[1] 0.004030846 0.008677509 0.015025342 0.007222011 0.010114014\n\n$volatilities\n[1] 0.08350623 0.05444006 0.05165270 0.03642462 0.03991410\n\n$cov_matrix\n            XLK          XLV          XLP          XLE         XLF\nXLK 0.006973290 0.0027897366 0.0024497012 0.0013125262 0.002137823\nXLV 0.002789737 0.0029637200 0.0019066845 0.0008771254 0.001387132\nXLP 0.002449701 0.0019066845 0.0026680010 0.0007372934 0.001139679\nXLE 0.001312526 0.0008771254 0.0007372934 0.0013267528 0.001034452\nXLF 0.002137823 0.0013871321 0.0011396791 0.0010344519 0.001593135\n\n$corr_matrix\n          XLK       XLV       XLP       XLE       XLF\nXLK 1.0000000 0.6136571 0.5679384 0.4315132 0.6413965\nXLV 0.6136571 1.0000000 0.6780586 0.4423317 0.6383707\nXLP 0.5679384 0.6780586 1.0000000 0.3918793 0.5527939\nXLE 0.4315132 0.4423317 0.3918793 1.0000000 0.7115231\nXLF 0.6413965 0.6383707 0.5527939 0.7115231 1.0000000\n\n$port_variance\n            [,1]\n[1,] 0.001517294\n\n$port_exp_return\n[1] 0.009078192\n\nresults_df_period &lt;- map_dfr(sim_results_period, ~ data.frame(Exp_Return = .x$port_exp_return, \n                                                            Std_Dev = sqrt(.x$port_variance)))\n# View summarized results for daily returns\nprint(head(results_df_period))\n\n   Exp_Return    Std_Dev\n1 0.009078192 0.03895245\n2 0.010822326 0.03906639\n3 0.008376929 0.04953396\n4 0.007454897 0.04938544\n5 0.008529608 0.04357777\n6 0.008795629 0.04981343\n\nresults_df &lt;- results_df_period"
  },
  {
    "objectID": "posts/ppc/index.html#visualize-the-results",
    "href": "posts/ppc/index.html#visualize-the-results",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Visualize the results",
    "text": "Visualize the results\n\nlibrary(patchwork)\n\nresults_df &lt;- results_df |&gt; \n    arrange(Std_Dev) |&gt; \n    mutate(is_efficient = Exp_Return &gt;= cummax(Exp_Return))\n\nefficient_portfolios &lt;- results_df |&gt; \n    arrange(Std_Dev)  |&gt; \n    mutate(cummax_return = cummax(Exp_Return)) |&gt; \n    filter(Exp_Return &gt;= cummax_return)\n\nefficient_model &lt;- lm(Exp_Return ~ poly(Std_Dev, 2), data = efficient_portfolios)\n\np1 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return, color = is_efficient)) +\n    geom_point() +\n    scale_color_manual(values = c(\"azure2\", \"springgreen4\")) + \n    theme_minimal() +\n    theme(\n        axis.title = element_blank(),\n        legend.position = \"none\"\n    )\n\np2 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return)) +\n    geom_point(aes(color = is_efficient), size = 1) +  # Default size for all points\n    geom_point(data = filter(results_df, is_efficient), \n               aes(color = is_efficient), size = 2) +  # Larger size for efficient points\n    scale_color_manual(values = c(\"azure2\", \"springgreen4\")) +\n    theme_minimal() +\n    geom_line(data = efficient_portfolios, aes(x = Std_Dev, y = Exp_Return), colour = \"springgreen2\") +\n    theme(\n        axis.title = element_blank(),\n        legend.position = \"none\"\n    )\n\np3 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return)) +\n    geom_point(color = \"azure2\") +\n    geom_smooth(data = efficient_portfolios, method = \"lm\", formula = y ~ poly(x, 2), \n                se = FALSE, colour = \"springgreen4\", linewidth = 1.5) +\n    labs(x = \"Std Dev (Risk)\",\n         y = \"Return\") +\n    theme_minimal()\n\n# Calculate a color metric based on Exp_Return and Std_Dev\n\nRiskFree_temp &lt;- 0.0\nresults_df &lt;- results_df %&gt;%\n    mutate(efficiency = (Exp_Return - RiskFree_temp)/ Std_Dev)\n\n# Create a scatterplot with color gradient based on the color_metric\n# p4 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return, color = color_metric)) +\n#     geom_point() +\n#     scale_color_gradient2(low = \"azure3\", high = \"springgreen1\", mid = \"yellow\", \n#                           midpoint = median(results_df$color_metric)) +\n#     theme_minimal() +\n#     labs(color = \"Color Metric\")\n\n# Assuming results_df and color_metric are already defined appropriately\n\np4 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return, color = efficiency)) +\n    geom_point() +\n    scale_color_gradientn(colors = c(\"azure4\", \"lightgoldenrod1\", \"springgreen2\"),\n                          values = scales::rescale(c(min(results_df$efficiency), \n                                                     max(results_df$efficiency)))) +\n    theme_minimal() +\n    labs(x = \"Std Dev (Risk)\",\n         y = \"Return\", \n         color = \"efficiency\") \n\n(p1 + p2) / (p3 + p4 )"
  },
  {
    "objectID": "posts/ppc_v2/index.html",
    "href": "posts/ppc_v2/index.html",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "",
    "text": "Contents\nload libraries\nlibrary(tidyverse)\nlibrary(tidyquant)\n# library(dplyr); library(tidyr); library(purrr)\n# library(ggplot)"
  },
  {
    "objectID": "posts/ppc_v2/index.html#define-the-sets-of-stocksindices",
    "href": "posts/ppc_v2/index.html#define-the-sets-of-stocksindices",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Define the SETS of stocks/indices",
    "text": "Define the SETS of stocks/indices\nThe container in this approach is stock_sets, a dataframe the we initialize (our TOC) with three columns:\n\nset_id\ndescription\nsymbols: a list of tickers\n\n\nsector_3eft_list &lt;- c( \"XLK\", \"XLV\", \"XLP\") # Tech, Health, Staples\n# sector_4etf_list &lt;- c( \"XLK\", \"XLV\", \"XLP\",  \"XLE\",) # Tech, Health, Staples, Energy\nsector_5etf_list &lt;- c( \"XLK\", \"XLV\", \"XLP\", \"XLE\", \"XLF\") # Tech, Health, Staples, Energy, Financials\n# sector_7etf_list &lt;- c(sector_5etf_list, \"XLI\", \"XLU\")\nsector_11etf_list &lt;- c(\"XLK\",  # Technology\n                      \"XLV\",  # Health Care\n                      \"XLF\",  # Financials\n                      \"XLY\",  # Consumer Discretionary\n                      \"XLP\",  # Consumer Staples\n                      \"XLE\",  # Energy\n                      \"XLU\",  # Utilities\n                      \"XLI\",  # Industrials\n                      \"XLB\",  # Materials\n                      \"XLRE\", # Real Estate\n                      \"XLC\") # Communication Services\n\nsize_etfs &lt;- c(\"SPY\", \"MDY\", \"IWM\") # Large, Mid, Small\n# size_style_etfs &lt;- c(\"IWF\",  # Large-Cap Growth\n#                      \"IWD\",  # Large-Cap Value\n#                      \"SPY\",  # Large-Cap Blend\n#                      \"IWP\",  # Mid-Cap Growth\n#                      \"IWS\",  # Mid-Cap Value\n#                      \"MDY\",  # Mid-Cap Blend\n#                      \"IWO\",  # Small-Cap Growth\n#                      \"IWN\",  # Small-Cap Value\n#                      \"IWM\")  # Small-Cap Blend\n\nstock_sets &lt;- tibble(\n    set_id = c(\"3_sectors\",\n               \"5_sectors\", \n               \"11_sectors\",\n               \"3_sizes\"),\n    \n    description = c(\"3 Sectors picked by GPT-4: Tech, Health, Staples\",\n                    \"5 Sectors picked by GPT-4: above + Energy + Financials\",\n                    \"All  11 Sectors\",\n                    \"Size: Large, Mid, Small--Blend\"),\n    \n    # this is a list column, see https://adv-r.hadley.nz/vectors-chap.html#list-columns \n    symbols = list(sector_3eft_list, sector_5etf_list, sector_11etf_list, size_etfs)\n    )\n\ndate_start &lt;- \"2013-01-01\"\ndate_end   &lt;- \"2023-11-17\""
  },
  {
    "objectID": "posts/ppc_v2/index.html#retrieve-returns-for-each-periodicity-aka-frequency",
    "href": "posts/ppc_v2/index.html#retrieve-returns-for-each-periodicity-aka-frequency",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Retrieve returns for each periodicity; aka, frequency",
    "text": "Retrieve returns for each periodicity; aka, frequency\nFor each SET of tickers, get_returns will retrieve log returns for each of three periods:\n\ndaily\nweekly\nmonthly\n\nThen we will call the get_returns function via map (my favorite function) to create a new list column called nested_data. Each row of nested_data will contain a list of three dataframes, one for each period. These dataframes will contain the log returns for each ticker in the set.\n\nget_returns &lt;- function(symbols, start_date, end_date) {\n    mult_stocks &lt;- tq_get(symbols, get = \"stock.prices\", \n                          from = start_date, to = end_date)\n    \n    periods &lt;- c(\"daily\", \"weekly\", \"monthly\")\n    returns_list &lt;- lapply(periods, function(period) {\n        mult_stocks |&gt; \n            group_by(symbol) |&gt; \n            tq_transmute(select = adjusted,\n                         mutate_fun = periodReturn, \n                         period = period, \n                         type = \"log\")\n    })\n    \n    names(returns_list) &lt;- periods\n    return(returns_list)\n}\n\n# Nest return data for each stock set\nstock_sets &lt;- stock_sets |&gt; \n    mutate(nested_data = map(symbols, \n                             ~ get_returns(.x, date_start, date_end)))\n\nprint(stock_sets)\n\n# A tibble: 4 × 4\n  set_id     description                                    symbols nested_data \n  &lt;chr&gt;      &lt;chr&gt;                                          &lt;list&gt;  &lt;list&gt;      \n1 3_sectors  3 Sectors picked by GPT-4: Tech, Health, Stap… &lt;chr&gt;   &lt;named list&gt;\n2 5_sectors  5 Sectors picked by GPT-4: above + Energy + F… &lt;chr&gt;   &lt;named list&gt;\n3 11_sectors All  11 Sectors                                &lt;chr&gt;   &lt;named list&gt;\n4 3_sizes    Size: Large, Mid, Small--Blend                 &lt;chr&gt;   &lt;named list&gt;"
  },
  {
    "objectID": "posts/ppc_v2/index.html#analyze-add-the-analysis-list-column",
    "href": "posts/ppc_v2/index.html#analyze-add-the-analysis-list-column",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Analyze: add the analysis list column",
    "text": "Analyze: add the analysis list column\nFor each set and periodicity, the analysis list column generates:\n\nvector of volatilities\nvector of average returns\ncorrelation matrix (diagonal is 1)\naverage correlation (as a rough measure of diversification)\n\n\nperform_analysis &lt;- function(data, returns_column) {\n    # Sort the data by symbol and date\n    data_sorted &lt;- data |&gt; arrange(symbol, date)\n    \n    volatilities &lt;- data_sorted |&gt;  \n        group_by(symbol) |&gt;  \n        summarise(volatility = sd(.data[[returns_column]], na.rm = TRUE)) |&gt;  \n        ungroup()\n    \n    avg_returns &lt;- data_sorted |&gt;  \n        group_by(symbol) |&gt;  \n        summarise(avg_return = mean(.data[[returns_column]], na.rm = TRUE)) |&gt;  \n        ungroup()\n    \n    # Pivot to wide format for correlation matrix calculation\n    # Ensuring the data is sorted correctly helps in keeping the order of symbols consistent\n    data_wide &lt;- data_sorted |&gt;  \n        pivot_wider(names_from = symbol, values_from = .data[[returns_column]])\n    corr_matrix &lt;- cor(select(data_wide, -date), use = \"complete.obs\")\n    \n    # Calculate average correlation\n    avg_corr &lt;- mean(corr_matrix[lower.tri(corr_matrix)])\n    \n    return(list(volatilities = volatilities, avg_returns = avg_returns, corr_matrix = corr_matrix, avg_corr = avg_corr))\n}\n\n# Applying the perform_analysis function to the stock_sets\nstock_sets &lt;- stock_sets |&gt; \n    mutate(analysis = map(nested_data, ~ {\n        data_daily &lt;- .x$daily\n        data_weekly &lt;- .x$weekly\n        data_monthly &lt;- .x$monthly\n        \n        analysis_daily &lt;- perform_analysis(data_daily, \"daily.returns\")\n        analysis_weekly &lt;- perform_analysis(data_weekly, \"weekly.returns\")\n        analysis_monthly &lt;- perform_analysis(data_monthly, \"monthly.returns\")\n        \n        list(daily = analysis_daily, weekly = analysis_weekly, monthly = analysis_monthly)\n    }))\n\n# Examine data structure \nprint(stock_sets) # Notice the analysis list column has been added\n\n# A tibble: 4 × 5\n  set_id     description                       symbols nested_data  analysis    \n  &lt;chr&gt;      &lt;chr&gt;                             &lt;list&gt;  &lt;list&gt;       &lt;list&gt;      \n1 3_sectors  3 Sectors picked by GPT-4: Tech,… &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n2 5_sectors  5 Sectors picked by GPT-4: above… &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n3 11_sectors All  11 Sectors                   &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n4 3_sizes    Size: Large, Mid, Small--Blend    &lt;chr&gt;   &lt;named list&gt; &lt;named list&gt;\n\nglimpse(stock_sets)\n\nRows: 4\nColumns: 5\n$ set_id      &lt;chr&gt; \"3_sectors\", \"5_sectors\", \"11_sectors\", \"3_sizes\"\n$ description &lt;chr&gt; \"3 Sectors picked by GPT-4: Tech, Health, Staples\", \"5 Sec…\n$ symbols     &lt;list&gt; &lt;\"XLK\", \"XLV\", \"XLP\"&gt;, &lt;\"XLK\", \"XLV\", \"XLP\", \"XLE\", \"XLF\"&gt;…\n$ nested_data &lt;list&gt; [[&lt;grouped_df[8217 x 3]&gt;], [&lt;grouped_df[1704 x 3]&gt;], [&lt;gr…\n$ analysis    &lt;list&gt; [[[&lt;tbl_df[3 x 2]&gt;], [&lt;tbl_df[3 x 2]&gt;], &lt;&lt;matrix[3 x 3]&gt;&gt;…\n\nstock_sets$analysis[[1]] # first row is the first stock set\n\n$daily\n$daily$volatilities\n# A tibble: 3 × 2\n  symbol volatility\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK       0.0138 \n2 XLP       0.00906\n3 XLV       0.0105 \n\n$daily$avg_returns\n# A tibble: 3 × 2\n  symbol avg_return\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK      0.000719\n2 XLP      0.000345\n3 XLV      0.000484\n\n$daily$corr_matrix\n          XLK       XLP       XLV\nXLK 1.0000000 0.6357679 0.7254975\nXLP 0.6357679 1.0000000 0.7182656\nXLV 0.7254975 0.7182656 1.0000000\n\n$daily$avg_corr\n[1] 0.693177\n\n\n$weekly\n$weekly$volatilities\n# A tibble: 3 × 2\n  symbol volatility\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK        0.0270\n2 XLP        0.0194\n3 XLV        0.0227\n\n$weekly$avg_returns\n# A tibble: 3 × 2\n  symbol avg_return\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK       0.00347\n2 XLP       0.00167\n3 XLV       0.00233\n\n$weekly$corr_matrix\n          XLK       XLP       XLV\nXLK 1.0000000 0.6515029 0.6976196\nXLP 0.6515029 1.0000000 0.7046559\nXLV 0.6976196 0.7046559 1.0000000\n\n$weekly$avg_corr\n[1] 0.6845928\n\n\n$monthly\n$monthly$volatilities\n# A tibble: 3 × 2\n  symbol volatility\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK        0.0517\n2 XLP        0.0364\n3 XLV        0.0399\n\n$monthly$avg_returns\n# A tibble: 3 × 2\n  symbol avg_return\n  &lt;chr&gt;       &lt;dbl&gt;\n1 XLK       0.0150 \n2 XLP       0.00722\n3 XLV       0.0101 \n\n$monthly$corr_matrix\n          XLK       XLP       XLV\nXLK 1.0000000 0.5679382 0.6136564\nXLP 0.5679382 1.0000000 0.6780580\nXLV 0.6136564 0.6780580 1.0000000\n\n$monthly$avg_corr\n[1] 0.6198842\n\n\n\n# Selecting the desired set (e.g., \"Set 1\")\n\nselect_set &lt;- stock_sets |&gt; \n    filter(set_id == \"11_sectors\") |&gt; \n    pull(\"analysis\")\n\nanalyze_set &lt;- select_set[[1]]\nanalyze_period &lt;- analyze_set$monthly\n\n# Extracting components from the selected set\nexp_returns_period &lt;- analyze_period$avg_returns$avg_return\nnames(exp_returns_period) &lt;- analyze_period$avg_returns$symbol\n\nvolatilities_period &lt;- analyze_period$volatilities$volatility\nnames(volatilities_period) &lt;- analyze_period$volatilities$symbol\n\ncorr_matrix_period &lt;- analyze_period$corr_matrix\nnum_stocks_period &lt;- length(volatilities_period)\n\nCorrelation\n\nlibrary(ggcorrplot)\nlibrary(MASS)\nlibrary(patchwork)\n\ncp1 &lt;- corr_matrix_period |&gt; ggcorrplot(\n    method = \"square\", \n    type = \"lower\", \n    lab = TRUE, \n    colors = c(\"red\", \"white\", \"springgreen\"),\n    title = \"Correlation Matrix of Daily Returns\")\n\ncp1\n\n\n\n\nDesirability\n\ncalculate_desirability &lt;- function(exp_returns, volatilities, corr_matrix, A, risk_free_rate, utility_weight, sharpe_weight, correlation_weight) {\n    if(is.null(names(exp_returns)) || is.null(names(volatilities))) {\n        stop(\"Expected returns and volatilities must have names.\")\n    }\n    \n    # Ensure the lengths match\n    if(length(exp_returns) != length(volatilities) || length(exp_returns) != nrow(corr_matrix)) {\n        stop(\"Lengths of returns, volatilities, and correlation matrix do not match.\")\n    }\n    # Ensure the lengths match\n    if(length(exp_returns) != length(volatilities) || length(exp_returns) != nrow(corr_matrix)) {\n        stop(\"Lengths of returns, volatilities, and correlation matrix do not match.\")\n    }\n    \n    # Calculate the variance as the square of volatility\n    variance &lt;- volatilities^2\n    \n    # Calculate the utility for each sector\n    utility &lt;- exp_returns - 0.5 * A * variance\n    \n    # Calculate the Sharpe ratio for each sector\n    sharpe_ratio &lt;- (exp_returns - risk_free_rate) / volatilities\n    \n    # Calculate the average correlation for each sector\n    avg_correlation &lt;- apply(corr_matrix, 1, function(x) mean(x[-which(x == 1)], na.rm = TRUE))\n    write.matrix(corr_matrix, \"avg_correlation.csv\", sep = \",\")\n    \n    # Calculate the desirability score\n    desirability_score &lt;- utility_weight * (utility * 100) +\n        sharpe_weight * (sharpe_ratio * 3 ) -\n        correlation_weight * avg_correlation # Negative because lower correlation is better\n    \n    # Create a data frame for sector desirability\n    desirability_df &lt;- data.frame(sector = names(exp_returns),\n                                  exp_returns = exp_returns,\n                                  volatilities = volatilities,\n                                  utility = utility,\n                                  sharpe_ratio = sharpe_ratio,\n                                  avg_correlation = avg_correlation,\n                                  desirability_score = desirability_score)\n    \n    # Sort by desirability score\n    desirability_df &lt;- desirability_df[order(-desirability_df$desirability_score),]\n    \n    return(desirability_df)\n}\n\n# Example parameters\nA &lt;- 3  # Risk aversion coefficient\nrisk_free_rate &lt;- 0.0  # Risk-free rate\nutility_weight &lt;- 0.3\nsharpe_weight &lt;- 0.4\ncorrelation_weight &lt;- 0.3\n\n# Calculate desirability using the extracted components\nsector_desirability &lt;- calculate_desirability(exp_returns_period, volatilities_period, corr_matrix_period, A, risk_free_rate, utility_weight, sharpe_weight, correlation_weight)\n\nsector_desirability\n\n     sector exp_returns volatilities       utility sharpe_ratio avg_correlation\nXLK     XLK 0.015025342   0.05165266  0.0110233459   0.29089192       0.6885434\nXLV     XLV 0.010114015   0.03991406  0.0077243160   0.25339477       0.6580335\nXLY     XLY 0.010399520   0.05421595  0.0059904665   0.19181662       0.7084905\nXLP     XLP 0.007222011   0.03642463  0.0052318807   0.19827274       0.6588569\nXLU     XLU 0.007008193   0.04345244  0.0041760216   0.16128423       0.5483552\nXLI     XLI 0.009141406   0.05120984  0.0052077349   0.17850879       0.7663516\nXLF     XLF 0.008677509   0.05444007  0.0042319278   0.15939564       0.7166628\nXLB     XLB 0.007271933   0.05298609  0.0030606447   0.13724231       0.7629429\nXLRE   XLRE 0.004606752   0.05011872  0.0008389219   0.09191678       0.7092791\nXLC     XLC 0.005837405   0.06085228  0.0002829058   0.09592747       0.6907898\nXLE     XLE 0.004030844   0.08350624 -0.0064290935   0.04826997       0.5421771\n     desirability_score\nXLK          0.47320766\nXLV          0.33839315\nXLY          0.19734680\nXLP          0.19722663\nXLU          0.15431518\nXLI          0.14053713\nXLF          0.10323376\nXLB          0.02762725\nXLRE        -0.07731594\nXLC         -0.08363680\nXLE         -0.29760195"
  },
  {
    "objectID": "posts/ppc_v2/index.html#setup-the-simulation",
    "href": "posts/ppc_v2/index.html#setup-the-simulation",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Setup the simulation",
    "text": "Setup the simulation\nThe get_random_weights function returns a dataframe of random weights. Each column is a set of weights for a single simulation. Each row is the weight for a single stock. The weights are normalized so that they sum to 1.\nSo I’m starting with an incredibly naive approach to the simulation. I’m going to assume that the expected return for each stock is the average return for that stock over the entire period. I’m also going to assume that the volatility for each stock is the average volatility for that stock over the entire period. Most importantly, the only randomness in the simulation is the weights and they are totally naive because they are independent of the analysis. We can’t expect anything like an efficient frontier from the raw scatterplot. However, this will illustrate the future risk/reward trade-off faced by a “totally naive” investor!\n\n# returns a data frame of random weights\n# rows = weight per stock; columns = number of simulations\nget_random_weights &lt;- function(num_stocks, num_simulations, probabilities) {\n    set.seed(123)\n    weights_df &lt;- matrix(nrow = num_stocks, ncol = num_simulations)\n\n    for (i in 1:num_simulations) {\n        # Generate weights influenced by probabilities\n        weights &lt;- runif(num_stocks) * probabilities\n        weights_df[, i] &lt;- weights / sum(weights)  # Normalize the weights\n    }\n\n    return(as.data.frame(weights_df))\n}\n\n# single simulation: given a set of weights, computes the expected return and volatility\nport_sim &lt;- function(exp_returns, volatilities, corr_matrix, weights) {\n    \n    cov_matrix &lt;- outer(volatilities, volatilities) * corr_matrix\n    port_variance &lt;- t(weights) %*% cov_matrix %*% weights\n    port_exp_return &lt;- sum(weights * exp_returns)\n\n    return(list(exp_returns = exp_returns, \n                volatilities = volatilities,\n                cov_matrix = cov_matrix, \n                corr_matrix = corr_matrix,\n                port_variance = port_variance,\n                port_exp_return = port_exp_return))\n}\n\n# runs a port_simulation for each column in the weights_df\nrun_sims &lt;- function(exp_returns, volatilities, corr_matrix, weights_df) {\n    simulations &lt;- map(1:ncol(weights_df), ~ {\n        weights_vector &lt;- weights_df[, .x]\n        port_sim(exp_returns, volatilities, corr_matrix, weights_vector)\n        })\n    \n    return(simulations)\n}"
  },
  {
    "objectID": "posts/ppc_v2/index.html#run-the-simulation-on-a-single-set",
    "href": "posts/ppc_v2/index.html#run-the-simulation-on-a-single-set",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Run the simulation (on a single set)",
    "text": "Run the simulation (on a single set)\n\nnum_sims &lt;- 10000  # Set the number of simulations\n\nsector_desirability$desirability_score &lt;- pmax(sector_desirability$desirability_score, 0)\ntotal_score &lt;- sum(sector_desirability$desirability_score)\nprobabilities &lt;- sector_desirability$desirability_score / total_score\n\n\nrandom_weights_df_period &lt;- get_random_weights(num_stocks_period, num_sims, probabilities)\nsim_results_period &lt;- run_sims(exp_returns_period, \n                              volatilities_period, \n                              corr_matrix_period, \n                              random_weights_df_period)\n\n# Print results of the first simulation\nprint(sim_results_period[[1]])\n\n$exp_returns\n        XLB         XLC         XLE         XLF         XLI         XLK \n0.007271933 0.005837405 0.004030844 0.008677509 0.009141406 0.015025342 \n        XLP        XLRE         XLU         XLV         XLY \n0.007222011 0.004606752 0.007008193 0.010114015 0.010399520 \n\n$volatilities\n       XLB        XLC        XLE        XLF        XLI        XLK        XLP \n0.05298609 0.06085228 0.08350624 0.05444007 0.05120984 0.05165266 0.03642463 \n      XLRE        XLU        XLV        XLY \n0.05011872 0.04345244 0.03991406 0.05421595 \n\n$cov_matrix\n             XLB         XLC         XLE         XLF         XLI         XLK\nXLB  0.002807526 0.002445591 0.002939209 0.002453076 0.002449652 0.002039863\nXLC  0.002445591 0.003703000 0.002729728 0.002525212 0.002395695 0.002707615\nXLE  0.002939209 0.002729728 0.006973292 0.003519155 0.002980982 0.002003833\nXLF  0.002453076 0.002525212 0.003519155 0.002963721 0.002500637 0.001908759\nXLI  0.002449652 0.002395695 0.002980982 0.002500637 0.002622447 0.002071542\nXLK  0.002039863 0.002707615 0.002003833 0.001908759 0.002071542 0.002667998\nXLP  0.001440331 0.001273703 0.001455723 0.001279402 0.001394055 0.001159082\nXLRE 0.002086998 0.002209362 0.001960762 0.001904553 0.001926469 0.001842359\nXLU  0.001410705 0.001306091 0.001132947 0.001157953 0.001280432 0.001077269\nXLV  0.001605595 0.001455557 0.001672473 0.001400932 0.001527509 0.001348071\nXLY  0.002309809 0.002730008 0.002369834 0.002144644 0.002204521 0.002493967\n             XLP        XLRE         XLU         XLV         XLY\nXLB  0.001440331 0.002086998 0.001410705 0.001605595 0.002309809\nXLC  0.001273703 0.002209362 0.001306091 0.001455557 0.002730008\nXLE  0.001455723 0.001960762 0.001132947 0.001672473 0.002369834\nXLF  0.001279402 0.001904553 0.001157953 0.001400932 0.002144644\nXLI  0.001394055 0.001926469 0.001280432 0.001527509 0.002204521\nXLK  0.001159082 0.001842359 0.001077269 0.001348071 0.002493967\nXLP  0.001326754 0.001352455 0.001134192 0.001080211 0.001145343\nXLRE 0.001352455 0.002511886 0.001581032 0.001387531 0.002155425\nXLU  0.001134192 0.001581032 0.001888114 0.001013965 0.001160315\nXLV  0.001080211 0.001387531 0.001013965 0.001593132 0.001412943\nXLY  0.001145343 0.002155425 0.001160315 0.001412943 0.002939369\n\n$corr_matrix\n           XLB       XLC       XLE       XLF       XLI       XLK       XLP\nXLB  1.0000000 0.7584819 0.6642778 0.8504140 0.9027950 0.7453263 0.7462862\nXLC  0.7584819 1.0000000 0.5371847 0.7622585 0.7687785 0.8614248 0.5746405\nXLE  0.6642778 0.5371847 1.0000000 0.7741067 0.6970871 0.4645686 0.4785911\nXLF  0.8504140 0.7622585 0.7741067 1.0000000 0.8969715 0.6787967 0.6451982\nXLI  0.9027950 0.7687785 0.6970871 0.8969715 1.0000000 0.7831547 0.7473627\nXLK  0.7453263 0.8614248 0.4645686 0.6787967 0.7831547 1.0000000 0.6160648\nXLP  0.7462862 0.5746405 0.4785911 0.6451982 0.7473627 0.6160648 1.0000000\nXLRE 0.7858870 0.7244194 0.4684961 0.6980305 0.7506003 0.7116746 0.7408458\nXLU  0.6127171 0.4939495 0.3122313 0.4895063 0.5754254 0.4799737 0.7166010\nXLV  0.7591859 0.5992755 0.5017812 0.6447219 0.7473162 0.6538742 0.7429981\nXLY  0.8040577 0.8274848 0.5234461 0.7266236 0.7940243 0.8905759 0.5799807\n          XLRE       XLU       XLV       XLY\nXLB  0.7858870 0.6127171 0.7591859 0.8040577\nXLC  0.7244194 0.4939495 0.5992755 0.8274848\nXLE  0.4684961 0.3122313 0.5017812 0.5234461\nXLF  0.6980305 0.4895063 0.6447219 0.7266236\nXLI  0.7506003 0.5754254 0.7473162 0.7940243\nXLK  0.7116746 0.4799737 0.6538742 0.8905759\nXLP  0.7408458 0.7166010 0.7429981 0.5799807\nXLRE 1.0000000 0.7259829 0.6936121 0.7932422\nXLU  0.7259829 1.0000000 0.5846324 0.4925322\nXLV  0.6936121 0.5846324 1.0000000 0.6529376\nXLY  0.7932422 0.4925322 0.6529376 1.0000000\n\n$port_variance\n            [,1]\n[1,] 0.002567196\n\n$port_exp_return\n[1] 0.007106525\n\nresults_df_period &lt;- map_dfr(sim_results_period, ~ data.frame(Exp_Return = .x$port_exp_return, \n                                                            Std_Dev = sqrt(.x$port_variance)))\n# View summarized results for daily returns\nprint(head(results_df_period))\n\n   Exp_Return    Std_Dev\n1 0.007106525 0.05066750\n2 0.007058208 0.05249469\n3 0.007349765 0.05167136\n4 0.007666882 0.05147461\n5 0.009318553 0.04969520\n6 0.008069111 0.05307626\n\nresults_df &lt;- results_df_period"
  },
  {
    "objectID": "posts/ppc_v2/index.html#visualize-the-results",
    "href": "posts/ppc_v2/index.html#visualize-the-results",
    "title": "Portfolio Possibilities Curve (PPC)",
    "section": "Visualize the results",
    "text": "Visualize the results\n\nlibrary(patchwork)\n\nresults_df &lt;- results_df |&gt; \n    arrange(Std_Dev) |&gt; \n    mutate(is_efficient = Exp_Return &gt;= cummax(Exp_Return))\n\nefficient_portfolios &lt;- results_df |&gt; \n    arrange(Std_Dev)  |&gt; \n    mutate(cummax_return = cummax(Exp_Return)) |&gt; \n    filter(Exp_Return &gt;= cummax_return)\n\nefficient_model &lt;- lm(Exp_Return ~ poly(Std_Dev, 2), data = efficient_portfolios)\n\np1 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return, color = is_efficient)) +\n    geom_point() +\n    scale_color_manual(values = c(\"azure2\", \"springgreen4\")) + \n    theme_minimal() +\n    theme(\n        axis.title = element_blank(),\n        legend.position = \"none\"\n    )\n\np2 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return)) +\n    geom_point(aes(color = is_efficient), size = 1) +  # Default size for all points\n    geom_point(data = filter(results_df, is_efficient), \n               aes(color = is_efficient), size = 2) +  # Larger size for efficient points\n    scale_color_manual(values = c(\"azure2\", \"springgreen4\")) +\n    theme_minimal() +\n    geom_line(data = efficient_portfolios, aes(x = Std_Dev, y = Exp_Return), colour = \"springgreen2\") +\n    theme(\n        axis.title = element_blank(),\n        legend.position = \"none\"\n    )\n\np3 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return)) +\n    geom_point(color = \"azure2\") +\n    geom_smooth(data = efficient_portfolios, method = \"lm\", formula = y ~ poly(x, 2), \n                se = FALSE, colour = \"springgreen4\", linewidth = 1.5) +\n    labs(x = \"Std Dev (Risk)\",\n         y = \"Return\") +\n    theme_minimal()\n\n# Calculate a color metric based on Exp_Return and Std_Dev\n\nRiskFree_temp &lt;- 0.0\nresults_df &lt;- results_df %&gt;%\n    mutate(efficiency = (Exp_Return - RiskFree_temp)/ Std_Dev)\n\n# Create a scatterplot with color gradient based on the color_metric\n# p4 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return, color = color_metric)) +\n#     geom_point() +\n#     scale_color_gradient2(low = \"azure3\", high = \"springgreen1\", mid = \"yellow\", \n#                           midpoint = median(results_df$color_metric)) +\n#     theme_minimal() +\n#     labs(color = \"Color Metric\")\n\n# Assuming results_df and color_metric are already defined appropriately\n\np4 &lt;- ggplot(results_df, aes(x = Std_Dev, y = Exp_Return, color = efficiency)) +\n    geom_point() +\n    scale_color_gradientn(colors = c(\"azure4\", \"lightgoldenrod1\", \"springgreen2\"),\n                          values = scales::rescale(c(min(results_df$efficiency), \n                                                     max(results_df$efficiency)))) +\n    theme_minimal() +\n    labs(x = \"Std Dev (Risk)\",\n         y = \"Return\", \n         color = \"efficiency\") \n\n(p1 + p2) / (p3 + p4 )"
  }
]