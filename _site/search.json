[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To FinEdTech",
    "section": "",
    "text": "Welcome to my data science blog!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/sim-equity-risk-premium/index.html",
    "href": "posts/sim-equity-risk-premium/index.html",
    "title": "Simulating the equity risk premium",
    "section": "",
    "text": "The following implements the implied ERP approach in Professor Damodaran’s post on the The Price of Risk. My intention is to briefly explore its sensitivity to assumptions.\n\nlibrary(tidyverse)\nlibrary(scales)\n\nsolve_for_R &lt;- function(RF, ER_vector, CP_vector, G2, PV) {\n  \n  # Calculate cash flow vector\n  CF_vector &lt;- ER_vector * CP_vector\n  \n  # Define the objective function\n  objective_function &lt;- function(R) {\n    # This is effectively two-stage dividend discount model except the initial stage is explicated\n    # such that there exists no G1 and G2 refers to the subsequent period of growth\n    \n    PV_calculated &lt;- sum(CF_vector[1:5] / (1 + R)^(1:5)) + CF_vector[6] / ((R - G2) * (1 + R)^5)\n    return((PV - PV_calculated)^2)\n  }\n  \n  # Use the optim function to minimize the objective function\n  result &lt;- optim(par = RF, fn = objective_function, method = \"Brent\", lower = -1, upper = 2)\n  \n  return(result$par)\n}\n\nRF &lt;- 0.04 # I have rounded his riskfree rate of 3.97% to 4.00%\nER_vector &lt;- c(217.8, 245.2, 273.7, 295.1, 308.9, 324.9) # A. Damodaran's earnings vector\nCP_vector &lt;- c(0.84, 0.82, 0.80, 0.78, 0.77, 0.77) # Cash payout ratios\nG2 &lt;- 0.04 # His model sets the stable growth equal to the RF rate\nPV &lt;- 4600 # I rounded 4588.96 to 4,600\n\nimplied_equity &lt;- solve_for_R(RF, ER_vector, CP_vector, G2, PV)\nimplied_ERP &lt;- implied_equity - RF\n\n# Number of simulations\nn_simulations &lt;- 10000\ncoeff_variation &lt;- 0.10 # Arbitrarily suggesting that COV of 10% is tight\n\n# Assumed means and standard deviations for inputs\nmean_RF &lt;- RF; sd_RF &lt;- RF * coeff_variation\nmean_ER &lt;- ER_vector; sd_ER &lt;- ER_vector * coeff_variation\nmean_CP &lt;- CP_vector; sd_CP &lt;- CP_vector * coeff_variation\nmean_G2 &lt;- G2; sd_G2 &lt;- G2 * coeff_variation\nmean_PV &lt;- PV; sd_PV &lt;- PV * coeff_variation\n\n# MC simulation\nset.seed(379)\nR_values &lt;- replicate(n_simulations, \n  solve_for_R(\n    # RF = rnorm(1, mean_RF, sd_RF),\n    RF = RF,\n    ER_vector = rnorm(6, mean_ER, sd_ER),\n    CP_vector = rnorm(6, mean_CP, sd_CP),\n    G2 = rnorm(1, mean_G2, sd_G2),\n    PV = PV\n  )\n)\n\n# Histogram to visualize the distribution of R values\nR_values &lt;- R_values[R_values &gt; 0]\nERP_values &lt;- R_values - RF\nERP_values_mean &lt;- mean(ERP_values)\nERP_values_df &lt;- as_data_frame(ERP_values)\n\nERP_values_df %&gt;% ggplot(aes(value)) +\n    geom_histogram(color = \"darkblue\", fill = \"lightblue\") +\n    geom_vline(aes(xintercept = ERP_values_mean), color = \"darkgreen\", size = 1.5) +\n    scale_x_continuous(labels = percent_format(0.01)) +\n    labs(title = \"Implied equity risk premium, ERP (n = 10,000 sims)\",\n         subtitle = \"Under tight assumption dispersion (CV = σ/μ =10%). Green vertical line is the mean.\",\n         y = \"Count\") +\n    # xlab(\"X label\") + \n    # ylab(\"Count\") +\n    theme_classic() +\n    theme(axis.title = element_blank(),\n          axis.text = element_text(size = 12, face = \"bold\"))\n\n\n\n\nQuick check on the distribution:\n\nlibrary(moments)\nskewness(ERP_values_df$value)\n\n[1] 0.08903814\n\nkurtosis(ERP_values_df$value)\n\n[1] 2.997992\n\nquantiles_v &lt;- c(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975, 0.99)\nquantile(ERP_values_df$value, probs = quantiles_v)\n\n        1%       2.5%         5%        10%        25%        50%        75% \n0.03041180 0.03248300 0.03433620 0.03649075 0.04004356 0.04410500 0.04832614 \n       90%        95%      97.5%        99% \n0.05209407 0.05447292 0.05638444 0.05881702 \n\n\nWhat is the relationship between the sustainable growth rate, G2, and the ERP?\n\nG2_values &lt;- seq(from = 0.02, to = 0.06, by = 0.001)\nR_values &lt;- map_dbl(G2_values, function(G2) {\n  solve_for_R(\n    RF = RF,\n    ER_vector = ER_vector,\n    CP_vector = CP_vector,\n    G2 = G2,\n    PV = PV\n  )\n})\n\nERP_values &lt;- R_values - RF\n\nG_vs_ERP &lt;-  tibble(\n  G2 = G2_values,\n  ERP = ERP_values\n)\n\nG_vs_ERP %&gt;% ggplot(aes(x = G2, y = ERP)) + \n  geom_point() + \n  coord_cartesian(ylim = c(.02, .08)) + \n  labs(title = \"Implied ERP as function of sustainable growth rate, G2\",\n       subtitle = \"Unlike prior/next visualization, predicted vectors are not randomized\")\n\n\n\n\nAnd just for fun, let’s add randomness to the earnings and cash payout vectors:\n\nG2_values &lt;- seq(from = 0.02, to = 0.06, by = 0.001)\n\nR_values &lt;- map(G2_values, function(G2) {\n  replicate(30, {\n    solve_for_R(\n      RF = RF,\n      ER_vector = rnorm(6, mean_ER, sd_ER),\n      CP_vector = rnorm(6, mean_CP, sd_CP),\n      G2 = G2,\n      PV = PV\n    ) - RF # subtracting RF here inside replicat\n  })\n})\n\ndf &lt;- tibble(\n  G2 = G2_values,\n  ERP = R_values\n) %&gt;% unnest()\n\nmodel_line &lt;- lm(ERP ~ G2, data = df)\nrsq &lt;- summary(model_line)$r.squared\nlabel_R2 &lt;- sprintf(\"R^2 = %.2f\", rsq)\n\ndf %&gt;% ggplot(aes(x = G2, y = ERP)) + \n  geom_point() +\n  coord_cartesian(ylim = c(.02, .08)) + \n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Restores 10% CV randomness to earnings and payout vectors\") +\n  annotate(\"text\", x=0.025, y=0.065, label=label_R2, fontface=\"bold\", hjust=0)\n\n\n\n  # geom_text(aes(label = label_R2))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FinEdTech by DH",
    "section": "",
    "text": "This is a Quarto website\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nPosit’s brilliant system executes either R or python code (and a GPT API example)\n\n\n\n\n\n\nSep 15, 2023\n\n\nDavid Harper\n\n\n\n\n\n\n  \n\n\n\n\nLogistic regression coefficients\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nFitting a logistic regression model is easy in R, but coefficient interpretation is non-trivial\n\n\n\n\n\n\nSep 4, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nSimulating the equity risk premium\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\nThe implied ERP is very sensitive to assumptions, in particular G2\n\n\n\n\n\n\nAug 31, 2023\n\n\nDavid Harper, CFA, FRM\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To FinEdTech\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nDavid Harper\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About David",
    "section": "",
    "text": "Hello! I’m David Harper, CFA, FRM. I started bionicturtle.com in 2004 and built it into the #1 exam preparation provider (EPP) for the Financial Risk Manager (FRM). In February of 2021, I sold the business to CeriFi where I continue to run it as President, Bionic Turtle, a CeriFi company. And now I’m part of a great team that is building the world’s greatest platform for financial education at every stage of your career.\nI love learning about (and teaching) risk. I also write investing articles at Seeking Alpha, where I disclose my portfolio/trades and practice risk management. My focus is disruptive technologies, patient GARP-style with a portfolio risk overlay; e.g., diversification. The market wants growth but I’m a value technician at heart, consequently I am willing to wait for great companies to prove their worth to the general audience. I have a BA, Economics from UC, Berkeley. I earned my CFA in 2003 and my FRM in 2004.\nI’ve been training and practicing data science for several years. Of course, my preferred data tool is R (aka, #RStats, #rstat). The best thing about R is the amazing community, there is nothing quite like an RStudio conference. Online I try to keep sharp with Coursera, datacamp and perhaps the most talented data science instructor on the planet: Matt Dancho’s Business Science.\nI am extremely interested (and invested) in digital disruption and the future of work (FOW), including EdTech, talent marketplaces and work management (my favorite software is wrike).\nThis is a Quarto site using the built-in sandstone theme"
  },
  {
    "objectID": "posts/logistic-coeff/index.html",
    "href": "posts/logistic-coeff/index.html",
    "title": "Logistic regression coefficients",
    "section": "",
    "text": "I wanted to shadow GARP’s logistic regression example, so I sampled from the same LendingClub database and performed a similar logistic regression. The key difference is practical: I’ll often re-sample from the data in order to get a result that lends itself to a good practice question. I’ve been writing practice questions for a long time, and there are many little details that go into this. For example, GARP’s logistic regression shows 10 independent variables, and I reduced that to seven merely because I don’t need to show all the variables to make the point.\nAfter I seeded the result that appealed to me, I wrote the practice question below (the published question sans answer is here). After fiddling with the four choices, I’m happy with the final question. It’s an “EXCEPT FOR” question, which is what I often use when I’m trying to blanked the concept more comprehensively than a “TRUE” question. This is a bit more work because each distractor must be carefully written.\n\n23.6.1. Darlene is a risk analyst who evaluate the creditworthiness of loan applicants at her financial institution. Her department is testing a new logistic regression model. If the model performs well in testing, it will be deployed to assist in the underwriting decision-making process. The training data is a sub-sample (n = 800) from the same LendingClub database used in reading. In the logistic regression, the dependent variable is a 0/1 for the terminal state of the loan being either zero (fully paid off) or one (deemed irrecoverable or defaulted). In the actual code, this dependent variable is labeled ‘outcome’.\nThe following are the features (aka, independent variables) as given by their textual labels: Amount, Term, Interest_rate, Installment, Employ_hist, Income, and Bankruptcies. In regard to units in the database, please note the following: Amount is thousands of dollars ($000s); Term is months; Interest_rate is effectively multiplied by one hundred such that 7 equates to 7% or 0.070; Installment is dollars; Employment_hist is years; Income is thousand of dollars ($000); and Bankruptcies is a whole number {0, 1, 2, …}.\nThe table below displays the logistic regression results:\n&lt;&lt;See regression output below; table will paste here&gt;&gt;\n\n\nIn regard to this logistic regression, each of the following statements is true EXCEPT which is false?\n\nA single additional bankruptcy increases the expected odds of default by almost 58 percent\nIf she requires significance at the 5% level or better, then two of the coefficients (in addition to the intercept) are significant\nEach +100 basis points increase in the interest rate (e.g., from 8.0% to 9.0%) implies an increase of about 14.0 basis points in the default probability\nIf the cost of making a bad loan is high, she can decrease the threshold (i.e., set Z to a low value such as 0.05), but this will reject more good borrowers\n\n\nHere is the code with some comments. The logistic model itself, a type of glm(), requires only a single line and the model is stored in logit_model_1 as a list object. Most of my code is re-coding the dataset, and then rendering the model’s coefficients with the awesome gt package. Posit’s Richard Iannone does an incredible job maintaining the package. If you think about it, generating tables are really important in data!\n\nlibrary(tidyverse) \nlibrary(gt)\n# library(labelled) Didn't use but helpful\n\n# set.seed(xzy)\nset.seed(374)\n\nsample_size &lt;- 800\nlcfeatures &lt;- read_csv(\"lcfeatures.csv\") \n# Same LendingClub dataset used in FRM Chapter 15 (Logistic Regression Example)\n# Located at https://www.kaggle.com/datasets/wordsforthewise/lending-club\n# But lcfeatures is a random sample of 10,000 which is too large for my need\n# So I just sample_n as random subset of the 10,000\nlcfeatures &lt;- lcfeatures |&gt; sample_n(sample_size)\n\n# recoding \nlcfeatures$emp_length_n &lt;- gsub(\"&lt; 1\", \"0\", lcfeatures$emp_length)\nlcfeatures$emp_length_n2 &lt;- parse_number(lcfeatures$emp_length_n)\nlcfeatures$term_n &lt;- parse_number(lcfeatures$term)\n\nlcfeatures$home_ownership_simpler &lt;- recode(lcfeatures$home_ownership,\n                                             \"MORTGAGE\" = \"OWN\",\n                                             \"ANY\" = \"RENT\",\n                                             \"NONE\" = \"RENT\")\n\nlcfeatures$mortgage_simpler &lt;- recode(lcfeatures$home_ownership,\n                                       \"OWN\" = \"NO\",\n                                       \"ANY\" = \"NO\",\n                                       \"NONE\" = \"NO\",\n                                       \"RENT\" = \"NO\",\n                                       \"MORTGAGE\" = \"YES\")\n\nlcfeatures$loan_status_coded &lt;- recode(lcfeatures$loan_status,\n                                        \"Charged Off\" = \"Default\",\n                                        \"Does not meet the credit policy. Status:Charged Off\" = \"Default\",\n                                        \"Late (31-120 days)\" = \"Default\",\n                                        .default = \"Paid\")\n\nlcfeatures$home_ownership_bern &lt;- recode(lcfeatures$home_ownership_simpler,\n                                          \"RENT\" = 0,\n                                          \"OWN\" = 1)\n\nlcfeatures$mortgage_bern &lt;- recode(lcfeatures$mortgage_simpler,\n                                          \"NO\" = 0,\n                                          \"YES\" = 1)\n\nlcfeatures$loan_status_bern &lt;- recode(lcfeatures$loan_status_coded,\n                                          \"Paid\" = 0,\n                                          \"Default\" = 1)\n\nlcfeatures$loan_amnt_000 &lt;- lcfeatures$loan_amnt / 1000\nlcfeatures$annual_inc_000 &lt;- lcfeatures$annual_inc / 1000\nlcfeatures$outcome &lt;- lcfeatures$loan_status_bern\n\n# This is logistic regression model\nlogit_model_1 &lt;- glm(formula = outcome ~ loan_amnt_000 + term_n + int_rate + installment + \n        emp_length_n2 + annual_inc_000 + pub_rec_bankruptcies,\n        family = binomial(link = \"logit\"), data = lcfeatures)\n\ncoef_table &lt;- coef(summary(logit_model_1)) \ncoef_tbl  &lt;-  as_tibble(coef_table)\nCoeff_labels &lt;- c(\"(Intercept)\", \"Amount\", \"Term\", \"Interest_rate\", \"Installment\", \n                 \"Employment_hist\", \"Income\",\"Bankruptcies\")\ncoef_tbl &lt;- cbind(Coeff_labels, coef_tbl)\n\n# Using gt() to render a table\ncoef_tbl_gt &lt;- coef_tbl %&gt;% gt() |&gt; \n    opt_table_font(stack = \"humanist\") |&gt;\n    fmt_number(columns = everything(),\n               decimals = 3)\ncoef_tbl_gt\n\n\n\n\n\n  \n    \n    \n      Coeff_labels\n      Estimate\n      Std. Error\n      z value\n      Pr(&gt;|z|)\n    \n  \n  \n    (Intercept)\n−2.329\n0.841\n−2.769\n0.006\n    Amount\n0.123\n0.092\n1.339\n0.181\n    Term\n−0.041\n0.027\n−1.519\n0.129\n    Interest_rate\n0.140\n0.034\n4.108\n0.000\n    Installment\n−0.003\n0.003\n−1.033\n0.302\n    Employment_hist\n−0.032\n0.031\n−1.025\n0.305\n    Income\n−0.003\n0.003\n−0.937\n0.349\n    Bankruptcies\n0.457\n0.230\n1.991\n0.046\n  \n  \n  \n\n\n\n\nIf we use predict() with type = “response”, then the logistic regression returns the vector of predicted probabilities (from zero to 100%). We can classify the Bernoulli prediction (0 = nondefault, 1 = default) as a function of our desired conservative/aggressive threshold. Below I show the number of rejections would increase as we lower the threshold.\n\npredicted_probs &lt;- predict(logit_model_1, lcfeatures, type = \"response\")\nthresholds &lt;- c(0.4, 0.3, 0.2, 0.1, 0.05, 0.010)\nthresholds |&gt; map_int(\\(x) sum(ifelse(predicted_probs &gt; x, 1, 0), na.rm = TRUE))\n\n[1]   5  30  86 383 689 748\n\n\nInspired by this blog post on color coding the {gt} table, I added some color to highlight the significant coefficients (obviously not in the actual Q&A, just here!):\n\ncoef_tbl_gt |&gt; \n    data_color(\n        columns = 'Pr(&gt;|z|)', \n        palette = c(\"#19F000\",\"#E4FF00\"),\n        domain = c(0,0.05),\n        na_color = \"lightgrey\"\n    )\n\n\n\n\n\n  \n    \n    \n      Coeff_labels\n      Estimate\n      Std. Error\n      z value\n      Pr(&gt;|z|)\n    \n  \n  \n    (Intercept)\n−2.329\n0.841\n−2.769\n0.006\n    Amount\n0.123\n0.092\n1.339\n0.181\n    Term\n−0.041\n0.027\n−1.519\n0.129\n    Interest_rate\n0.140\n0.034\n4.108\n0.000\n    Installment\n−0.003\n0.003\n−1.033\n0.302\n    Employment_hist\n−0.032\n0.031\n−1.025\n0.305\n    Income\n−0.003\n0.003\n−0.937\n0.349\n    Bankruptcies\n0.457\n0.230\n1.991\n0.046"
  },
  {
    "objectID": "posts/this-quarto-site/index.html",
    "href": "posts/this-quarto-site/index.html",
    "title": "This is a Quarto website",
    "section": "",
    "text": "This is a demonstration post. Now that I’ve published this Quarto website (details here TBD, itself a switch from my distill site here, I wanted to test (and show) two capabilities:\n\nThe openai package can prompt GPT and DALL-E via the API\nQuarto can also run python code chunks (referring to dataframes defined in R)\n\n\nPrompting GPT and DALL-E via API\nThe openai package includes create_image() which returns a convenient list that contains a URL of the image. For this post, I only evaluated create_image() once and saved the DALL-E image to a .png file; then commented the code. However, the subsequent GPT text prompt (i.e., completion object) is evaluated.\n\nSys.setenv(OPENAI_API_KEY = openai_key)\n\nlibrary(openai)\nlibrary(gptstudio)\nlibrary(tidyverse)\n# library(reticulate)\n\nprompt_dalle &lt;- \"Create a high quality background for my laptop with a minimalist landscape of a mountain with forest with multiple sky colors during sunset\"\n\n# landscape &lt;- create_image(prompt_dalle)\n# landscape_url &lt;- landscape$data$url\n# destination &lt;- \"mylandscape.png\"\n# download.file(landscape_url, destfile = destination, mode = \"wb\")\n\n\n\n\nlandscape by DALL·E\n\n\nSimilarly, create_chat_completion returns a list. We can easily retrieve the reply:\n\nlibrary(stringr)\nprompt_gpt &lt;- \"what are likely to be the top three implications of artificial intelligence on edtech?\"\nprompt_gpt_chars &lt;- nchar(prompt_gpt)\nprompt_gpt_words &lt;- length(strsplit(prompt_gpt, \"\\\\s+\")[[1]])\n\n# Here is the call to GPT 3.5 with my prompt_gpt text\nreply_gpt &lt;- create_chat_completion(\n    model = \"gpt-3.5-turbo\",\n    messages = list(\n        list(\n            \"role\" = \"user\",\n            \"content\" = prompt_gpt\n        )\n    )\n)\n\n# The response by GPT is a chat completion object that contains an\n# array (list) of choices (can be more than one) including the message.content\nreply_gpt_message &lt;- reply_gpt$choices$message.content\nreply_gpt_chars &lt;- nchar(reply_gpt_message)\nreply_gpt_words &lt;- length(strsplit(reply_gpt_message, \"\\\\s+\")[[1]])\n\ntotal_chars &lt;- prompt_gpt_chars + reply_gpt_chars\ntotal_words &lt;- prompt_gpt_words + reply_gpt_words\ntotal_tokens &lt;- reply_gpt$usage$total_tokens\ntoken_stats_text &lt;- paste(\"Total tokens =\", total_tokens, \n                          \". Given\", total_words, \"words and\", total_chars, \"characters, that's\",\n                          sprintf(\"%.3f\", total_tokens/total_words), \"tokens/word and\",\n                          sprintf(\"%.3f\", total_tokens/total_chars), \"tokens/character.\")\n\nprint(token_stats_text)\n\n[1] \"Total tokens = 217 . Given 173 words and 1314 characters, that's 1.254 tokens/word and 0.165 tokens/character.\"\n\ncat(reply_gpt_message, sep = \"\\n\")\n\n1. Personalized Learning: Artificial intelligence (AI) can enable highly personalized learning experiences by analyzing individual student behaviors, needs, and strengths. AI algorithms can adapt the educational content and pace of learning to meet the unique requirements of each student, thus enhancing their learning outcomes.\n\n2. Intelligent Tutoring Systems: AI-powered intelligent tutoring systems can provide immediate, personalized feedback and guidance to learners. These systems can analyze student responses, identify their misconceptions, and offer tailored explanations or additional resources. This technology can greatly enhance the effectiveness of online education platforms, enabling more individualized and efficient learning.\n\n3. Automation and Administrative Support: AI has the potential to improve administrative tasks in education. Intelligent chatbots can be used to provide immediate assistance to students regarding inquiries, enrollment, scheduling, or even simple academic tasks. Additionally, AI can automate grading to save teachers' time and provide students with faster feedback. This automation would allow teachers to focus more on individual instruction and personalized learning experiences.\n\n\n\n\nExecuting a python code block and the sharing the dataframe\nNow I will just load the built-in diamonds dataset and lazily convert the three factor levels (cut, clarity, and color) to integers. But I will skip R’s regression model, lm(), because I am going to let python fit the linear model …\n\ndiamonds_df &lt;- diamonds\ndiamonds_df$cut_int &lt;- as.integer(diamonds_df$cut)\ndiamonds_df$clarity_int &lt;- as.integer(diamonds_df$clarity)\ndiamonds_df$color_int &lt;- as.integer(diamonds_df$color)\n\n# Going to skip lm() in R and let python fit the model!\n# lm_diamonds &lt;- lm(price ~ carat + cut_int + color_int + clarity_int, data = diamonds_df)\n# diamonds_df$residuals &lt;- resid(lm_diamonds)\n# diamonds_df$predictions &lt;- predict(lm_diamonds)\n# diamonds_df |&gt; ggplot(aes(x = predictions, y = residuals)) +\n#   geom_point() +\n#   geom_hline(yintercept = 0, linetype = \"dashed\") +\n#   labs(title = \"Residual Plot\", x = \"Predicted Values\", y = \"Residuals\")\n\n… and here is the python code chunk! This is possible because the first line of the fenced code braces the executable code with “python” per these instructions.. Of course, a python installation is required to render locally.\n\n```{python}\n#| message: false\n\ndiamonds_data_py = r.diamonds_df\n\nimport statsmodels.api as sm\ny = diamonds_data_py[[\"price\"]]\n\nx = diamonds_data_py[[\"carat\", \"cut_int\", \"color_int\", \"clarity_int\"]]\nx = sm.add_constant(x)\nmod = sm.OLS(y, x).fit()\ndiamonds_data_py[\"Predicted\"] = mod.predict(x)\ndiamonds_data_py[\"Residuals\"] = mod.resid\n```\n\nAnd, finally, I will revert back to R to utilize ggplot. As explained by Nicola Rennie the key here is to load the reticulate package so that we can use the py prefix to retrieve the diamonds_data_py object. But you can see: the original R dataframe, diamonds_df, was retreived in python, via diamonds_data_py = r.diamonds_df, and then R retrieved that model via diamonds_residuals &lt;- py$diamonds_data_py. Sweet!\n\nlibrary(reticulate)\nlibrary(ggplot2)\nlibrary(ggthemes)\ndiamonds_residuals &lt;- py$diamonds_data_py\nggplot(data = diamonds_residuals,\n       mapping = aes(x = Predicted,\n                     y = Residuals)) +\n    geom_point(colour = \"#2F4F4F\") +\n    geom_hline(yintercept = 0, colour = \"red\") +\n    theme_economist()"
  }
]