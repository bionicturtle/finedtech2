---
title: Cosine similarity
description: Word embedding enables measures of semantic relatedness.
author: David Harper, CFA, FRM
date: 2024-04-04
categories: [code, analysis]
execute: 
  echo: true
  warning: false
---

The [word2vec](https://cran.r-project.org/web/packages/word2vec/readme/README.html) package has a function to compute the cosine similarity, *word2vec_similarity (..., type = "cosine")*. This is a measure of semantic relatedness between the words in the two sets. My example uses the [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) which conveniently comes in four sizes. I'm using the smallest: it has 6B tokens, 400K vocab, and 50 dimensions. 

```{r}

library(data.table)
library(word2vec)

# First with some seasons
vector1_words <- c("snow","sun", "happy", "freedom", "graduate", "olympics", "vacation", 
                   "school", "resolutions", "beach", "mountain", "cold", "snowboard", "easter")
vector2_words <- c("winter", "spring", "summer", "fall")

```

```{r}
#| eval: false

# path to the GloVe file
glove_path <- "glove.6B/glove.6B.50d.txt"

# Function to load GloVe vectors from a file using data.table's fread for efficiency
read_glove <- function(file_path) {
    # Define column types: first column as character, remaining as numeric
    num_columns <- length(fread(file_path, nrows = 1, header = FALSE)) # Detect the number of columns
    col_types <- c("character", rep("numeric", num_columns - 1))

    # Load data using fread with specified column types for efficiency
    embeddings <- fread(file_path, header = FALSE, quote = "", colClasses = col_types)
    setnames(embeddings, old = names(embeddings), new = c("word", paste0("V", 1:(num_columns-1))))

    # Convert to data.table (if not already one, although fread should return a data.table)
    embeddings <- as.data.table(embeddings)

    return(embeddings)
}

# Load the GloVe embeddings
m_w2v <- read_glove(glove_path)

# Function to get embeddings for given words, retaining as dataframes
get_embeddings <- function(model, words) {
    setkey(model, word)
    embeddings <- model[J(words), .SD, .SDcols = -"word", nomatch = 0L]
    return(as.data.frame(embeddings))
}

# Obtain the vector for the words as dataframes
vector1_embeds <- get_embeddings(m_w2v, vector1_words)
vector2_embeds <- get_embeddings(m_w2v, vector2_words)

save(vector1_embeds, vector2_embeds, file = "embeddings.RData")

```

```{r}

# Load the saved data
load("embeddings.RData")

# Convert dataframes to matrices by dropping the first column
vector1_embeds_m <- as.matrix(vector1_embeds[, -1, drop = FALSE])
vector2_embeds_m <- as.matrix(vector2_embeds[, -1, drop = FALSE])

# Set row and column names for matrices
rownames(vector1_embeds_m) <- vector1_words
rownames(vector2_embeds_m) <- vector2_words

# Manual cosine similarity function
# cosine_similarity <- function(matrix1, matrix2) {
#     # Compute cosine similarity between each row of matrix1 and each row of matrix2
#     result <- matrix(0, nrow = nrow(matrix1), ncol = nrow(matrix2))
#     for (i in 1:nrow(matrix1)) {
#         for (j in 1:nrow(matrix2)) {
#             result[i, j] <- sum(matrix1[i, ] * matrix2[j, ]) / (sqrt(sum(matrix1[i, ]^2)) * sqrt(sum(matrix2[j, ]^2)))
#         }
#     }
#     dimnames(result) <- list(row.names(matrix1), row.names(matrix2))
#     return(result)
# }

# reduced to vector via perplexity.ai
cosine_similarity <- function(matrix1, matrix2) {
  norm_matrix1 <- sqrt(rowSums(matrix1^2))
  norm_matrix2 <- sqrt(rowSums(matrix2^2))
  result <- matrix1 %*% t(matrix2) / (norm_matrix1 %o% norm_matrix2)
  dimnames(result) <- list(row.names(matrix1), row.names(matrix2))
  return(result)
}

# Compute the similarity between the two collections of embeddings
similarity_results <- word2vec_similarity(vector1_embeds_m, vector2_embeds_m, type = "cosine")

# Manually compute cosine similarity
manual_similarity_results <- cosine_similarity(vector1_embeds_m, vector2_embeds_m)

# Print the word2vec similarity results with row and column names
print("word2vec_similarity results:")
print(similarity_results)

# Print the manually calculated cosine similarity results
print("Manual cosine_similarity results:")
print(manual_similarity_results)

```

Heatmap #1

```{r}
library(ggplot2)
similarity_matrix <- similarity_results

# Convert the matrix to a data frame for plotting
similarity_df <- as.data.frame(as.table(similarity_matrix))
names(similarity_df) <- c("Word1", "Word2", "Similarity")

# Plot the heatmap
ggplot(similarity_df, aes(x = Word2, y = Word1, fill = Similarity)) +
  geom_tile() +
  scale_fill_gradient2(low = "white", high = "darkgreen", mid = "lightgreen", midpoint = 0.5, na.value = "lightgrey",
                       limit = c(0.15, 0.85), space = "Lab", name="Cosine\nSimilarity") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = NULL, y = NULL, title = "Cosine Similarity Heatmap")

```

Now let's try some finance terms. Please note the following (attempted) terms were not in the vocabulary: mutual funds, ETFs, hedge funds, IPO, venture capital, interest rates, capital gains.

```{r}
#| eval: true

vector3_words <- c("stocks", "bonds", "options", "commodities", "rates", "derivatives", "forex", "dividends", "profits", "bitcoin")

vector4_words <- c("economy", "growth", "recession", "inflation", "employment", "trade", "policy", "taxation", "savings", "investment", "credit", "bubble", "crisis")

```


```{r}
#| eval: false

# Obtain the vector for the words as dataframes
vector3_embeds <- get_embeddings(m_w2v, vector3_words)
vector4_embeds <- get_embeddings(m_w2v, vector4_words)

save(vector3_embeds, vector4_embeds, file = "embeddings_34.RData")

```


```{r}
#| eval: true

# Load the saved data
load("embeddings_34.RData")

# Convert dataframes to matrices by dropping the first column
vector3_embeds_m <- as.matrix(vector3_embeds[, -1, drop = FALSE])
vector4_embeds_m <- as.matrix(vector4_embeds[, -1, drop = FALSE])

# Set row and column names for matrices
rownames(vector3_embeds_m) <- vector3_words
rownames(vector4_embeds_m) <- vector4_words

# let's take a peek at the embeddings
head(vector3_embeds_m)

# Compute the similarity between the two collections of embeddings
similarity_results_34 <- word2vec_similarity(vector3_embeds_m, vector4_embeds_m, type = "cosine")
manual_similarity_results_34 <- cosine_similarity(vector3_embeds_m, vector4_embeds_m)

print(similarity_results_34)
print(manual_similarity_results_34)

```

Heatmap #2

```{r}

similarity_matrix_34 <- similarity_results_34

# Convert the matrix to a data frame for plotting
similarity_df_34 <- as.data.frame(as.table(similarity_matrix_34))
names(similarity_df_34) <- c("Word1", "Word2", "Similarity")

# Plot the heatmap
ggplot(similarity_df_34, aes(x = Word2, y = Word1, fill = Similarity)) +
  geom_tile() +
  scale_fill_gradient2(low = "white", high = "darkgreen", mid = "lightgreen", midpoint = 0.5, na.value = "lightgrey",
                       limit = c(0.15, 0.85), space = "Lab", name="Cosine\nSimilarity") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = NULL, y = NULL, title = "Cosine Similarity Heatmap")

```

## Appendix

Illustration of cosine similarity
```{r}

# Load necessary library
library(ggplot2)
library(grid)  # For arrow functions

# Define three vectors
v1 <- c(1, 2)
v2 <- c(2, 3)
v3 <- c(-1, 1)

# Create a data frame for plotting
vector_data <- data.frame(
  x = c(0, 0, 0), y = c(0, 0, 0),
  xend = c(v1[1], v2[1], v3[1]),
  yend = c(v1[2], v2[2], v3[2]),
  vector = c("v1", "v2", "v3")
)

# Function to calculate cosine similarity
cosine_similarity <- function(a, b) {
  sum(a * b) / (sqrt(sum(a^2)) * sqrt(sum(b^2)))
}

# Function to calculate vector magnitude
vector_length <- function(v) {
  sqrt(sum(v^2))
}

# Calculate cosine similarities
sim_v1_v2 <- cosine_similarity(v1, v2)
sim_v1_v3 <- cosine_similarity(v1, v3)
sim_v2_v3 <- cosine_similarity(v2, v3)

# Calculate angles in degrees
angle_v1_v2 <- acos(sim_v1_v2) * (180 / pi)
angle_v1_v3 <- acos(sim_v1_v3) * (180 / pi)
angle_v2_v3 <- acos(sim_v2_v3) * (180 / pi)

# Calculate vector lengths
length_v1 <- vector_length(v1)
length_v2 <- vector_length(v2)
length_v3 <- vector_length(v3)

# Plot the vectors
ggplot(vector_data, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend, yend = yend, color = vector),
               arrow = arrow(length = unit(0.2, "inches")), size = 1.5, lineend = 'round') +
  geom_text(aes(x = xend, y = yend, label = vector), vjust = -0.5, hjust = -0.5) +
  coord_fixed(ratio = 1, xlim = c(-2, 3), ylim = c(-1, 4)) +
  ggtitle(paste("Vector Metrics:\n",
                "Lengths   v1: ", round(length_v1, 2), ", v2: ", 
                round(length_v2, 2), ", v3: ", 
                round(length_v3, 2), "\n",
                "Cosine Similarities   v1-v2: ", 
                round(sim_v1_v2, 2), ", v1-v3: ", 
                round(sim_v1_v3, 2), ", v2-v3: ", 
                round(sim_v2_v3, 2), "\n",
                "Angles (Degrees)   v1-v2: ", 
                round(angle_v1_v2, 2), ", v1-v3: ", 
                round(angle_v1_v3, 2), ", v2-v3: ", 
                round(angle_v2_v3, 2))) +
  theme_minimal() +
  scale_color_manual(values = c("darkolivegreen3", "darkgreen", "cyan3"))

# Print the lengths, cosine similarities, and angles
cat("Vector Lengths:\n")
cat("v1:", length_v1, "\n")
cat("v2:", length_v2, "\n")
cat("v3:", length_v3, "\n")
cat("Cosine Similarities:\n")
cat("v1-v2:", sim_v1_v2, "\n")
cat("v1-v3:", sim_v1_v3, "\n")
cat("v2-v3:", sim_v2_v3, "\n")
cat("Angles (Degrees):\n")
cat("v1-v2:", angle_v1_v2, "\n")
cat("v1-v3:", angle_v1_v3, "\n")
cat("v2-v3:", angle_v2_v3, "\n")


```

