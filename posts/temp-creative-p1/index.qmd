---
title: Creativity as temperature in LLMs
description: The softmax function converts a numeric vector into a probabilty distribution
author: David Harper, CFA, FRM
date: 2024-05-11
categories: [code, analysis]
execute: 
  echo: true
  warning: false
engines:
  r: knitr
  python: reticulate
---

I liked Logan Thorneloe's swift explainer of LLM *temperature* in his [What you need to understand about LLM creativity](https://societysbackend.com/p/temperature-in-llms). Below I illustrate the softmax function with a dead-simple example.

Let's assume our sentence begins "We live in Los Angeles, tomorrow we will travel to the ..." where the **possible** location-destinations are found in the vector {beach, mountains, lake, mall, park, city}.

```{r}

library(ggplot2)
library(tidyr)
library(reticulate)

softmax <- function(values, temps) {
  words <- names(values) 
  
  # Below is the softmax function
  exp_values <- exp(outer(values, 1 / temps))
  softmax_values <- sweep(exp_values, 2, colSums(exp_values), `/`)
  
  data.frame(
    temp = rep(temps, each = length(values)),
    word = rep(words, times = length(temps)),
    prob = c(softmax_values)
  )
}

# We live in Los Angeles, tomorrow we will travel to the ..
values <- c("beach"     = 9.1,
            "mountains" = 7.2, 
            "lake"      = 5.8,
            "mall"      = 3.4,
            "park"      = 2.3,
            "city"      = 1.5
           )

temps <- c(0.3, 1, 5, 10)
softmax_data <- softmax(values, temps)

# Plot
softmax_data |> ggplot(aes(y = reorder(word, -values[word]),
                           x = prob, fill = temp)) +
    geom_bar(stat = "identity") +
    facet_wrap(~temp, nrow = 2, ncol = 2, 
               labeller = labeller(temp = function(x) paste("Temp =", x))) +
    labs(title = "We live in Los Angeles, tomorrow we will travel to the ..",
         subtitle = "Horizontal axis is probability",
         x = NULL, y = NULL) +
    scale_fill_gradient(low = "darkseagreen4", high = "darkorange") + 
    theme_minimal() +
    theme(
        strip.text = element_text(face = "bold", size = 12), # facet labels
        axis.title.x = element_text(face = "bold", size = 12), # X-axis label
        axis.text.x = element_text(size = 11, face = "bold"),
        axis.text.y = element_text(size = 11, face = "bold")
    )

```

The above was my crafted example. Next I'll conduct a tiny experiment to see how the temperature setting affects the output of the OpenAI language model (LLM) GPT-3.5. According to my co-pilot, "The temperature setting controls the randomness of the output generated by the model. A lower temperature setting results in more deterministic output, while a higher temperature setting leads to more creative and diverse responses."

The first example is a financial advisor specializing in portfolio allocation. The second example will be a creative fiction writer using vivid imagery. Notice how I changed the system message to instruct the model to act as a creative writer.

```{python}
# | output.wrap: true
# | output.frame.width: 300

from openai import OpenAI
import os

# Initialize the OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Set the system message to instruct the model to act as a creative writer
system_message = {
    "role": "system",
    "content": "You are a financial advisor specializing in portfolio allocation. Your goal is to provide personalized investment recommendations based on an individual's risk tolerance, financial goals, and market conditions.",
}

# Define the user message
user_message = {
    "role": "user",
    "content": "Given an investor's risk tolerance of High and a target retirement age of 65, suggest an optimal portfolio allocation strategy.",
}


# Function to generate text with a specific temperature setting
def generate_text_with_temperature(prompt, temperature):
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo-16k",
        messages=[system_message, user_message],
        temperature=temperature,
        max_tokens=200,  # Adjust max_tokens as needed
    )
    return completion.choices[0].message.content.strip()


# Generate text with different temperature settings
temperatures = [0.1, 0.8, 1.0, 1.5]
for temp in temperatures:
    generated_text = generate_text_with_temperature(user_message, temp)
    print(f"Temperature {temp}:\n{generated_text}\n")
    
system_message = {
    "role": "system",
    "content": "You are a creative fiction writer who uses vivid imagery",
}
user_message = {
    "role": "user",
    "content": "Finish this sentence: She opened her cryptocurrency wallet and discovered ...",
}

temperatures = [0.1, 0.8, 1.0, 1.5]
for temp in temperatures:
    generated_text = generate_text_with_temperature(user_message, temp)
    print(f"Temperature {temp}:\n{generated_text}\n")
    
```
