{
  "hash": "a6ee78ccff13d3c903e842b76bf74ac5",
  "result": {
    "markdown": "---\ntitle: Decision Trees (part 1)\ndescription: Just some description here for now\nauthor: David Harper, CFA, FRM\ndate: 2023-10-07\ncategories: [code, analysis]\nexecute: \n  echo: true\n  warning: false\n---\n\n\nContents\n\n* Train (and graph) dividend payer with rpart(), rpart.plot and C5.0\n* Loan default train\n* Loan default prediction\n* Adding penalty matrix to make false negatives costly\n* Trees are random but not too fragile\n\nTo write a PQ set for decision trees, I experimented below. First the libraries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_libraries <- c(\"C50\", \"gmodels\", \"tidyverse\", \"openxlsx\", \n                  \"rattle\", \"rpart\", \"rpart.plot\")\nlapply(my_libraries, library, character.only = TRUE)\n```\n:::\n\n\n### Predicting dividend\n\nGARP's motivating example is a super simple (n = 20) dataset of public companies the either pay or do not pay a Dividend. The my20firms dataframe (you can see) is slightly altered to achieve a tree that I liked better for purposes of a practice question:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# my20firms <- garp_data\n# my20firms$Dividend[1] <- 0\n# my20firms$Dividend[9] <- 0\n# my20firms$Dividend[12] <- 1\n# my20firms$Dividend[13] <- 0\n# my20firms$Dividend[15] <- 0\n\n# colnames(my20firms)[colnames(my20firms) == \"Retail_investor\"] <- \"Retail\"\n# colnames(my20firms)[colnames(my20firms) == \"Large_cap\"] <- \"LargeCap\"\n\n# write.xlsx(my20firms, file = \"dividendExampleModified_v3.xlsx\")\n# my20firms <- read.xlsx(\"dividendExampleModified_v3.xlsx\")\n# saveRDS(my20firms, file = \"my20firms-rds.RDS\")\n\nmy20firms <- readRDS(\"my20firms-rds.RDS\")\nmy20firms\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Dividend Earnings LargeCap Retail Tech\n1         0        0        1     40    1\n2         1        1        1     30    0\n3         1        1        1     20    0\n4         0        0        0     80    1\n5         1        0        1     20    0\n6         0        1        0     30    1\n7         0        1        0     40    0\n8         1        0        1     60    0\n9         0        1        1     20    1\n10        0        1        1     40    0\n11        0        0        0     20    1\n12        1        0        1     70    0\n13        0        1        0     30    1\n14        1        0        1     70    0\n15        0        0        1     50    1\n16        1        0        1     60    1\n17        1        1        1     30    0\n18        0        1        0     30    1\n19        0        0        0     40    0\n20        1        1        1     50    0\n```\n:::\n\n```{.r .cell-code}\nfit2 <- rpart(Dividend ~ ., data = my20firms, \n              parms = list(split = \"gini\"),\n              control = rpart.control(minsplit = 1, \n                                      minbucket = 1,\n                                      maxdepth = 4))\n\n# summary(fit2) printout is too long\nprint(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 20 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 20 4.9500000 0.4500000  \n   2) LargeCap< 0.5 7 0.0000000 0.0000000 *\n   3) LargeCap>=0.5 13 2.7692310 0.6923077  \n     6) Tech>=0.5 4 0.7500000 0.2500000  \n      12) Retail< 55 3 0.0000000 0.0000000 *\n      13) Retail>=55 1 0.0000000 1.0000000 *\n     7) Tech< 0.5 9 0.8888889 0.8888889  \n      14) Earnings>=0.5 5 0.8000000 0.8000000  \n        28) Retail>=35 2 0.5000000 0.5000000 *\n        29) Retail< 35 3 0.0000000 1.0000000 *\n      15) Earnings< 0.5 4 0.0000000 1.0000000 *\n```\n:::\n\n```{.r .cell-code}\nprintcp(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRegression tree:\nrpart(formula = Dividend ~ ., data = my20firms, parms = list(split = \"gini\"), \n    control = rpart.control(minsplit = 1, minbucket = 1, maxdepth = 4))\n\nVariables actually used in tree construction:\n[1] Earnings LargeCap Retail   Tech    \n\nRoot node error: 4.95/20 = 0.2475\n\nn= 20 \n\n        CP nsplit rel error  xerror     xstd\n1 0.440559      0   1.00000 1.11610 0.056479\n2 0.228352      1   0.55944 1.12013 0.298518\n3 0.151515      2   0.33109 1.04063 0.309021\n4 0.039282      3   0.17957 0.81948 0.360160\n5 0.010000      5   0.10101 0.80808 0.361385\n```\n:::\n\n```{.r .cell-code}\nrpart.plot(fit2, yesno = 2, left=FALSE, type=2, branch.lty = 3, nn= TRUE, \n           box.palette = \"BuGn\", leaf.round=0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# converting the target to factor\nmy20firms$Dividend <- as_factor(my20firms$Dividend)\n\n\nfit3 <- rpart(Dividend ~ ., data = my20firms, \n              parms = list(split = \"gini\"),\n              control = rpart.control(minsplit = 1, \n                                      minbucket = 1,\n                                      maxdepth = 4))\n\nprint(fit3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 20 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 20 9 0 (0.5500000 0.4500000)  \n   2) LargeCap< 0.5 7 0 0 (1.0000000 0.0000000) *\n   3) LargeCap>=0.5 13 4 1 (0.3076923 0.6923077)  \n     6) Tech>=0.5 4 1 0 (0.7500000 0.2500000)  \n      12) Retail< 55 3 0 0 (1.0000000 0.0000000) *\n      13) Retail>=55 1 0 1 (0.0000000 1.0000000) *\n     7) Tech< 0.5 9 1 1 (0.1111111 0.8888889) *\n```\n:::\n\n```{.r .cell-code}\nrpart.plot(fit3, yesno = 2, left=FALSE, type=2, branch.lty = 3, nn= TRUE, \n           box.palette = \"BuGn\", leaf.round=0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\nI had to refresh my knowledge of decision trees, and for that I depended on the awesome book () that I will review in the future (almost done!). He uses C5.0 algorithm (per the C50 package) and I just wanted to see its defaults:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_c5 <- C5.0(Dividend ~ ., data = my20firms)\nplot(tree_c5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# set MinCases = 1\n\ntree_c5_v2 <- C5.0(Dividend ~ ., \n                   control = C5.0Control(minCases = 1),\n                   data = my20firms)\nplot(tree_c5_v2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\n### Loan default examples\n\nNow I will switch datasets, and use the same loan default dataset used in the book. But I will use the more familiar rpart() function to train the tree. The result is similar but not identical (and please not the difference is not due to sampling varation: my test sample is the same).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9829)\ntrain_sample <- sample(1000, 900)\n\ncredit <- read.csv(\"credit.csv\", stringsAsFactors = TRUE)\n\n# split the data frames\ncredit_train <- credit[train_sample, ]\ncredit_test  <- credit[-train_sample, ]\n\ncredit_train$credit_history <- credit_train$credit_history |> \n    fct_relevel(\"critical\", \"poor\", \"good\", \"very good\", \"perfect\")\n\ntree_credit_train <- rpart(default ~ ., data = credit_train,\n                           parms = list(split = \"gini\"),\n                           control = rpart.control(minsplit = 1, \n                                                   minbucket = 1,\n                                                   maxdepth = 4))\n\nrpart.plot(tree_credit_train, yesno = 2, left=FALSE, type=2, branch.lty = 3, nn= TRUE, \n           box.palette = c(\"palegreen\", \"pink\"), leaf.round=0, extra = 101, digits = 4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nprint(tree_credit_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 900 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 900 265 no (0.7055556 0.2944444)  \n   2) checking_balance=> 200 DM,unknown 415  55 no (0.8674699 0.1325301) *\n   3) checking_balance=< 0 DM,1 - 200 DM 485 210 no (0.5670103 0.4329897)  \n     6) credit_history=critical,poor,good 426 167 no (0.6079812 0.3920188)  \n      12) months_loan_duration< 27.5 324 105 no (0.6759259 0.3240741)  \n        24) amount< 9899.5 317  98 no (0.6908517 0.3091483) *\n        25) amount>=9899.5 7   0 yes (0.0000000 1.0000000) *\n      13) months_loan_duration>=27.5 102  40 yes (0.3921569 0.6078431)  \n        26) dependents>=1.5 14   4 no (0.7142857 0.2857143) *\n        27) dependents< 1.5 88  30 yes (0.3409091 0.6590909) *\n     7) credit_history=very good,perfect 59  16 yes (0.2711864 0.7288136)  \n      14) age< 22.5 3   0 no (1.0000000 0.0000000) *\n      15) age>=22.5 56  13 yes (0.2321429 0.7678571) *\n```\n:::\n\n```{.r .cell-code}\nprintcp(tree_credit_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nClassification tree:\nrpart(formula = default ~ ., data = credit_train, parms = list(split = \"gini\"), \n    control = rpart.control(minsplit = 1, minbucket = 1, maxdepth = 4))\n\nVariables actually used in tree construction:\n[1] age                  amount               checking_balance    \n[4] credit_history       dependents           months_loan_duration\n\nRoot node error: 265/900 = 0.29444\n\nn= 900 \n\n        CP nsplit rel error  xerror     xstd\n1 0.050943      0   1.00000 1.00000 0.051599\n2 0.026415      3   0.81509 0.83396 0.048726\n3 0.022642      4   0.78868 0.82642 0.048577\n4 0.011321      5   0.76604 0.81887 0.048425\n5 0.010000      6   0.75472 0.81887 0.048425\n```\n:::\n:::\n\n\n### Default prediction\n\nBecause there is a 10% test set, we can test the decision tree. It's not great. In terms of the mistake, notice that 28/35 actual defaulters were incorrectly predicted to repay; that's terrible. Compare this to only 7/65 actual re-payers who were predicted to default.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_credit_pred <- predict(tree_credit_train, credit_test, type = \"class\")\n\n\nCrossTable(credit_test$default, tree_credit_pred,\n           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,\n           dnn = c('actual default', 'predicted default'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  100 \n\n \n               | predicted default \nactual default |        no |       yes | Row Total | \n---------------|-----------|-----------|-----------|\n            no |        58 |         7 |        65 | \n               |     0.580 |     0.070 |           | \n---------------|-----------|-----------|-----------|\n           yes |        28 |         7 |        35 | \n               |     0.280 |     0.070 |           | \n---------------|-----------|-----------|-----------|\n  Column Total |        86 |        14 |       100 | \n---------------|-----------|-----------|-----------|\n\n \n```\n:::\n:::\n\n\n### Adding a loss (aka, penalty, cost) matrix\n\nIt's really easy to impose a penalty matrix. We will make the false negative three times more costly than a false positive. As desired, the false negatives flip with huge improvement: the updated model correctly traps 28/35 defaults with only 7/35 false negatives. But this comes with an equally huge trade-off: false positives jump from 7/65 to 27 out of 65 who are predicted to default but actually repay.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenalty_matrix <- matrix(c(0, 3,   # Actual: No\n                           1, 0),  # Actual: Yes\n                         ncol=2)\n\nrownames(penalty_matrix) <- colnames(penalty_matrix) <- c(\"No\", \"Yes\")\n\ntree_credit_cost_train <- rpart(default ~ ., data = credit_train,\n                           parms = list(split = \"gini\", loss=penalty_matrix),\n                           control = rpart.control(minsplit = 1, \n                                                   minbucket = 1,\n                                                   maxdepth = 4))\n\ntree_credit_cost_pred <- predict(tree_credit_cost_train, credit_test, type = \"class\")\n\nCrossTable(credit_test$default, tree_credit_cost_pred,\n           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,\n           dnn = c('actual default', 'predicted default'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  100 \n\n \n               | predicted default \nactual default |        no |       yes | Row Total | \n---------------|-----------|-----------|-----------|\n            no |        38 |        27 |        65 | \n               |     0.380 |     0.270 |           | \n---------------|-----------|-----------|-----------|\n           yes |         7 |        28 |        35 | \n               |     0.070 |     0.280 |           | \n---------------|-----------|-----------|-----------|\n  Column Total |        45 |        55 |       100 | \n---------------|-----------|-----------|-----------|\n\n \n```\n:::\n:::\n\n\n### Can I easily randomize?\n\nI'm interested in the fact that decision trees have random qualities (aside from sampling variation). Below I set a different seed and switched the split algo to entropy. But the ultimate tree is the same.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# different see and switch gini to information; aka, entropy\nset.seed(448)\n\ntree_credit_train_2 <- rpart(default ~ ., data = credit_train,\n                           parms = list(split = \"information\"),\n                           control = rpart.control(minsplit = 1, \n                                                   minbucket = 1,\n                                                   maxdepth = 4))\n\nrpart.plot(tree_credit_train_2, yesno = 2, left=FALSE, type=2, branch.lty = 3, nn= TRUE, \n           box.palette = c(\"palegreen\", \"pink\"), leaf.round=0, extra = 101, digits = 4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nprint(tree_credit_train_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 900 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 900 265 no (0.7055556 0.2944444)  \n   2) checking_balance=> 200 DM,unknown 415  55 no (0.8674699 0.1325301) *\n   3) checking_balance=< 0 DM,1 - 200 DM 485 210 no (0.5670103 0.4329897)  \n     6) credit_history=critical,poor,good 426 167 no (0.6079812 0.3920188)  \n      12) months_loan_duration< 27.5 324 105 no (0.6759259 0.3240741)  \n        24) amount< 9899.5 317  98 no (0.6908517 0.3091483) *\n        25) amount>=9899.5 7   0 yes (0.0000000 1.0000000) *\n      13) months_loan_duration>=27.5 102  40 yes (0.3921569 0.6078431)  \n        26) dependents>=1.5 14   4 no (0.7142857 0.2857143) *\n        27) dependents< 1.5 88  30 yes (0.3409091 0.6590909) *\n     7) credit_history=very good,perfect 59  16 yes (0.2711864 0.7288136)  \n      14) age< 22.5 3   0 no (1.0000000 0.0000000) *\n      15) age>=22.5 56  13 yes (0.2321429 0.7678571) *\n```\n:::\n\n```{.r .cell-code}\nprintcp(tree_credit_train_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nClassification tree:\nrpart(formula = default ~ ., data = credit_train, parms = list(split = \"information\"), \n    control = rpart.control(minsplit = 1, minbucket = 1, maxdepth = 4))\n\nVariables actually used in tree construction:\n[1] age                  amount               checking_balance    \n[4] credit_history       dependents           months_loan_duration\n\nRoot node error: 265/900 = 0.29444\n\nn= 900 \n\n        CP nsplit rel error  xerror     xstd\n1 0.050943      0   1.00000 1.00000 0.051599\n2 0.026415      3   0.81509 0.87547 0.049518\n3 0.022642      4   0.78868 0.86038 0.049236\n4 0.011321      5   0.76604 0.86415 0.049307\n5 0.010000      6   0.75472 0.86038 0.049236\n```\n:::\n\n```{.r .cell-code}\nidentical(tree_credit_train, tree_credit_train_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n\n```{.r .cell-code}\nall.equal(tree_credit_train, tree_credit_train_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Component \\\"call\\\": target, current do not match when deparsed\"                                \n[2] \"Component \\\"cptable\\\": Mean relative difference: 0.04736418\"                                   \n[3] \"Component \\\"parms\\\": Component \\\"split\\\": Mean relative difference: 1\"                         \n[4] \"Component \\\"splits\\\": Attributes: < Component \\\"dimnames\\\": Component 1: 4 string mismatches >\"\n[5] \"Component \\\"splits\\\": Mean relative difference: 5.545437\"                                      \n[6] \"Component \\\"csplit\\\": Attributes: < Component \\\"dim\\\": Mean relative difference: 0.04761905 >\" \n[7] \"Component \\\"csplit\\\": Numeric: lengths (126, 120) differ\"                                      \n[8] \"Component \\\"variable.importance\\\": Names: 2 string mismatches\"                                 \n[9] \"Component \\\"variable.importance\\\": Mean relative difference: 0.1894674\"                        \n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}