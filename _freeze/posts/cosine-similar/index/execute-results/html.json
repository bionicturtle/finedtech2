{
  "hash": "8557070b80405fb67abe98aee94a657f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Cosine similarity\ndescription: Word embedding enables measures of semantic relatedness.\nauthor: David Harper, CFA, FRM\ndate: 2024-04-04\ncategories: [code, analysis]\nexecute: \n  echo: true\n  warning: false\n---\n\n\nThe [word2vec](https://cran.r-project.org/web/packages/word2vec/readme/README.html) package has a function to compute the cosine similarity, *word2vec_similarity (..., type = \"cosine\")*. This is a measure of semantic relatedness between the words in the two sets. My example uses the [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) which conveniently comes in four sizes. I'm using the smallest: it has 6B tokens, 400K vocab, and 50 dimensions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\nlibrary(word2vec)\n\n# First with some seasons\nvector1_words <- c(\"snow\",\"sun\", \"happy\", \"freedom\", \"graduate\", \"olympics\", \"vacation\", \n                   \"school\", \"resolutions\", \"beach\", \"mountain\", \"cold\", \"snowboard\", \"easter\")\nvector2_words <- c(\"winter\", \"spring\", \"summer\", \"fall\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# path to the GloVe file\nglove_path <- \"glove.6B/glove.6B.50d.txt\"\n\n# Function to load GloVe vectors from a file using data.table's fread for efficiency\nread_glove <- function(file_path) {\n    # Define column types: first column as character, remaining as numeric\n    num_columns <- length(fread(file_path, nrows = 1, header = FALSE)) # Detect the number of columns\n    col_types <- c(\"character\", rep(\"numeric\", num_columns - 1))\n\n    # Load data using fread with specified column types for efficiency\n    embeddings <- fread(file_path, header = FALSE, quote = \"\", colClasses = col_types)\n    setnames(embeddings, old = names(embeddings), new = c(\"word\", paste0(\"V\", 1:(num_columns-1))))\n\n    # Convert to data.table (if not already one, although fread should return a data.table)\n    embeddings <- as.data.table(embeddings)\n\n    return(embeddings)\n}\n\n# Load the GloVe embeddings\nm_w2v <- read_glove(glove_path)\n\n# Function to get embeddings for given words, retaining as dataframes\nget_embeddings <- function(model, words) {\n    setkey(model, word)\n    embeddings <- model[J(words), .SD, .SDcols = -\"word\", nomatch = 0L]\n    return(as.data.frame(embeddings))\n}\n\n# Obtain the vector for the words as dataframes\nvector1_embeds <- get_embeddings(m_w2v, vector1_words)\nvector2_embeds <- get_embeddings(m_w2v, vector2_words)\n\nsave(vector1_embeds, vector2_embeds, file = \"embeddings.RData\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the saved data\nload(\"embeddings.RData\")\n\n# Convert dataframes to matrices by dropping the first column\nvector1_embeds_m <- as.matrix(vector1_embeds[, -1, drop = FALSE])\nvector2_embeds_m <- as.matrix(vector2_embeds[, -1, drop = FALSE])\n\n# Set row and column names for matrices\nrownames(vector1_embeds_m) <- vector1_words\nrownames(vector2_embeds_m) <- vector2_words\n\n# let's take a peek at the embeddings\nhead(vector1_embeds_m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              V2       V3        V4        V5       V6       V7        V8\nsnow     1.35750  0.42372  0.083063 -1.211500  0.27573 -0.90287  0.281930\nsun      1.66800  0.65616  1.284300 -0.070946 -1.61530 -0.28654 -0.673170\nhappy    0.25710 -0.58693 -0.370290  1.082800 -0.55466 -0.78142  0.586960\nfreedom  0.27814 -0.11302 -0.370570  0.578730 -0.14529  0.43383 -0.456070\ngraduate 1.26790 -0.21186 -1.184100  0.037335 -0.34643 -0.97186 -0.549650\nolympics 1.46510 -0.18481  0.943780 -0.642520 -0.22036 -0.36813  0.061099\n                V9      V10      V11      V12      V13       V14      V15\nsnow      0.120660 -0.95591 -0.50092 -0.15034  0.44381  0.169120  0.29532\nsun      -0.498870  0.10531  0.18092  0.40495  0.52388  0.285400 -0.25076\nhappy    -0.587140  0.46318 -0.11267  0.26060 -0.26928 -0.072466  1.24700\nfreedom   0.726200  0.31185  0.24079 -0.11883 -0.20353 -0.299890 -0.27301\ngraduate -0.327130 -0.16357  0.17323 -0.16327  0.15325 -0.373970 -0.16660\nolympics  0.053339 -0.31116  0.55168 -0.87847 -0.90427  0.183010  2.04560\n               V16      V17        V18       V19        V20      V21      V22\nsnow      0.882570  0.26583  0.0026311 -1.240000 -1.1411000 -0.25527  0.14486\nsun       0.751490 -1.21530  0.0534380 -1.064900  0.0371250 -0.22833  0.30398\nhappy     0.305710  0.56731  0.3050900 -0.050312 -0.6444300 -0.54513  0.86429\nfreedom  -0.236860  0.51582 -0.4709100  0.312370  0.0070554 -0.22833  0.89127\ngraduate -0.359320 -0.25719  1.2180000 -0.085867  0.7923400  0.58087  1.00980\nolympics  0.029855  0.59809  0.7379100 -1.652100 -0.5135600  0.27897 -0.12678\n                 V23       V24       V25      V26      V27      V28      V29\nsnow      1.42630000 -0.802170  0.795790 -0.35958 -0.40527  1.46130  1.13130\nsun       1.26880000 -0.854940  0.493300 -0.92463 -0.14650  0.53333  0.20661\nhappy     0.20914000  0.563340  1.122800 -1.05160 -0.78105  0.29656  0.72610\nfreedom  -0.32475000 -0.225810  0.437050 -1.66280 -0.64576 -1.00980  0.37792\ngraduate -0.09386400  0.039381 -0.021806 -1.48020  0.11213 -1.38060 -1.60380\nolympics -0.00045526 -0.042126  0.632920 -1.56250 -0.24168 -0.25314 -0.24796\n                V30    V31       V32      V33        V34      V35       V36\nsnow      0.2795800 2.1674  0.317700 -0.10946  0.3054600  0.10803  0.094701\nsun      -0.0545700 2.6193 -0.114240 -0.30039 -0.0018384 -0.77640 -0.582290\nhappy    -0.6139200 2.4225  1.014200 -0.17753  0.4147000 -0.12966 -0.470640\nfreedom   0.3350400 2.6654  0.585240 -1.34250 -0.4082400 -1.49580 -0.645440\ngraduate -0.4843900 1.9821  0.055824 -0.39400 -1.4108000  0.76513  0.200460\nolympics -0.0049081 1.5883  1.499000 -0.92456 -0.7842300 -0.75093 -0.060259\n               V37        V38      V39      V40      V41      V42      V43\nsnow     -0.337730 -0.0098902  0.73014  0.50604 -0.93652  0.71403  0.29732\nsun      -0.418170 -0.0403710  0.41883 -0.23931  0.21250 -0.17315 -0.31351\nhappy     0.380700  0.1630900 -0.32300 -0.77899 -0.42473 -0.30826 -0.42242\nfreedom   0.071664 -0.8043900 -0.76056  0.36512  0.32903 -0.25687  0.13765\ngraduate -0.425190  1.2023000  1.33160 -0.20597 -0.45072 -0.40206  0.19261\nolympics  0.691380  1.4169000 -1.18020 -0.52624 -0.61480 -0.75260  0.60151\n               V44      V45       V46      V47      V48       V49      V50\nsnow     -0.430210  0.48054 -0.116270  0.20659 -0.62230  0.073763 -0.37068\nsun       0.360400  0.79981 -0.171440  0.19295 -1.61960  0.341350 -0.17726\nhappy     0.055069  0.38267  0.037415 -0.43020 -0.39442  0.105110  0.87286\nfreedom   0.395330 -0.68773 -0.043908 -0.95513 -0.47569 -0.336710 -0.44242\ngraduate  0.378680 -0.44808 -0.444620  0.97568  0.20168 -0.948270  0.95086\nolympics -0.213030  0.23234 -0.125800 -0.26376 -0.22645  0.102860 -0.36960\n```\n\n\n:::\n\n```{.r .cell-code}\n# Manual cosine similarity function\n# cosine_similarity <- function(matrix1, matrix2) {\n#     # Compute cosine similarity between each row of matrix1 and each row of matrix2\n#     result <- matrix(0, nrow = nrow(matrix1), ncol = nrow(matrix2))\n#     for (i in 1:nrow(matrix1)) {\n#         for (j in 1:nrow(matrix2)) {\n#             result[i, j] <- sum(matrix1[i, ] * matrix2[j, ]) / (sqrt(sum(matrix1[i, ]^2)) * sqrt(sum(matrix2[j, ]^2)))\n#         }\n#     }\n#     dimnames(result) <- list(row.names(matrix1), row.names(matrix2))\n#     return(result)\n# }\n\n# reduced to vector via perplexity.ai\ncosine_similarity <- function(matrix1, matrix2) {\n  norm_matrix1 <- sqrt(rowSums(matrix1^2))\n  norm_matrix2 <- sqrt(rowSums(matrix2^2))\n  result <- matrix1 %*% t(matrix2) / (norm_matrix1 %o% norm_matrix2)\n  dimnames(result) <- list(row.names(matrix1), row.names(matrix2))\n  return(result)\n}\n\n# Compute the similarity between the two collections of embeddings\nsimilarity_results <- word2vec_similarity(vector1_embeds_m, vector2_embeds_m, type = \"cosine\")\n\n# Manually compute cosine similarity\nmanual_similarity_results <- cosine_similarity(vector1_embeds_m, vector2_embeds_m)\n\n# Print the word2vec similarity results with row and column names\nprint(\"word2vec_similarity results:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"word2vec_similarity results:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(similarity_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                winter    spring    summer      fall\nsnow        0.74125017 0.6963912 0.6185827 0.5663821\nsun         0.44478379 0.5830609 0.4773879 0.5118248\nhappy       0.48504846 0.5028063 0.5609525 0.5506670\nfreedom     0.18136758 0.2254465 0.2702612 0.3755605\ngraduate    0.25377881 0.3788511 0.4521937 0.3111673\nolympics    0.66424604 0.4863632 0.6722972 0.4371825\nvacation    0.66606219 0.6043653 0.7153726 0.5044602\nschool      0.39208831 0.5613046 0.5794563 0.4539855\nresolutions 0.09477322 0.1713795 0.1198187 0.2054978\nbeach       0.50492486 0.5174625 0.5725889 0.3445451\nmountain    0.56929335 0.5649946 0.5214697 0.4198440\ncold        0.70956314 0.6887602 0.6809916 0.6406256\nsnowboard   0.31661765 0.1385683 0.2266248 0.0351622\neaster      0.55143247 0.6239188 0.5490755 0.4957569\n```\n\n\n:::\n\n```{.r .cell-code}\n# Print the manually calculated cosine similarity results\nprint(\"Manual cosine_similarity results:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Manual cosine_similarity results:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(manual_similarity_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                winter    spring    summer      fall\nsnow        0.74125017 0.6963912 0.6185827 0.5663821\nsun         0.44478379 0.5830609 0.4773879 0.5118248\nhappy       0.48504846 0.5028063 0.5609525 0.5506670\nfreedom     0.18136758 0.2254465 0.2702612 0.3755605\ngraduate    0.25377881 0.3788511 0.4521937 0.3111673\nolympics    0.66424604 0.4863632 0.6722972 0.4371825\nvacation    0.66606219 0.6043653 0.7153726 0.5044602\nschool      0.39208831 0.5613046 0.5794563 0.4539855\nresolutions 0.09477322 0.1713795 0.1198187 0.2054978\nbeach       0.50492486 0.5174625 0.5725889 0.3445451\nmountain    0.56929335 0.5649946 0.5214697 0.4198440\ncold        0.70956314 0.6887602 0.6809916 0.6406256\nsnowboard   0.31661765 0.1385683 0.2266248 0.0351622\neaster      0.55143247 0.6239188 0.5490755 0.4957569\n```\n\n\n:::\n:::\n\n\nHeatmap #1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nsimilarity_matrix <- similarity_results\n\n# Convert the matrix to a data frame for plotting\nsimilarity_df <- as.data.frame(as.table(similarity_matrix))\nnames(similarity_df) <- c(\"Word1\", \"Word2\", \"Similarity\")\n\n# Plot the heatmap\nggplot(similarity_df, aes(x = Word2, y = Word1, fill = Similarity)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"white\", high = \"darkgreen\", mid = \"lightgreen\", midpoint = 0.5, na.value = \"lightgrey\",\n                       limit = c(0.15, 0.85), space = \"Lab\", name=\"Cosine\\nSimilarity\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(x = NULL, y = NULL, title = \"Cosine Similarity Heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nNow let's try some finance terms. Please note the following (attempted) terms were not in the vocabulary: mutual funds, ETFs, hedge funds, IPO, venture capital, interest rates, capital gains.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvector3_words <- c(\"stocks\", \"bonds\", \"options\", \"commodities\", \"rates\", \"derivatives\", \"forex\", \"dividends\", \"profits\", \"bitcoin\")\n\nvector4_words <- c(\"economy\", \"growth\", \"recession\", \"inflation\", \"employment\", \"trade\", \"policy\", \"taxation\", \"savings\", \"investment\", \"credit\", \"bubble\", \"crisis\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtain the vector for the words as dataframes\nvector3_embeds <- get_embeddings(m_w2v, vector3_words)\nvector4_embeds <- get_embeddings(m_w2v, vector4_words)\n\nsave(vector3_embeds, vector4_embeds, file = \"embeddings_34.RData\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the saved data\nload(\"embeddings_34.RData\")\n\n# Convert dataframes to matrices by dropping the first column\nvector3_embeds_m <- as.matrix(vector3_embeds[, -1, drop = FALSE])\nvector4_embeds_m <- as.matrix(vector4_embeds[, -1, drop = FALSE])\n\n# Set row and column names for matrices\nrownames(vector3_embeds_m) <- vector3_words\nrownames(vector4_embeds_m) <- vector4_words\n\n# Compute the similarity between the two collections of embeddings\nsimilarity_results_34 <- word2vec_similarity(vector3_embeds_m, vector4_embeds_m, type = \"cosine\")\nmanual_similarity_results_34 <- cosine_similarity(vector3_embeds_m, vector4_embeds_m)\n\nprint(similarity_results_34)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                economy     growth   recession   inflation employment\nstocks       0.65425959 0.62400151  0.59762752 0.599831902 0.42318812\nbonds        0.46402332 0.54405098  0.44274361 0.533199902 0.42457070\noptions      0.43687030 0.49661368  0.31539672 0.451595130 0.60445833\ncommodities  0.52723072 0.56284582  0.40773432 0.523646696 0.45590560\nrates        0.70896441 0.78751815  0.73347470 0.855651550 0.70941063\nderivatives  0.30290322 0.40283918  0.30166410 0.380573277 0.29504002\nforex        0.42746042 0.38058020  0.38743491 0.459205966 0.30298252\ndividends    0.35500459 0.48542552  0.32137017 0.442262514 0.47539279\nprofits      0.63026826 0.70406166  0.59810072 0.612064950 0.55401247\nbitcoin     -0.05630604 0.04042479 -0.04533643 0.003072138 0.03177759\n                 trade     policy  taxation    savings investment     credit\nstocks      0.61352555  0.3726410 0.2920333 0.51031381 0.66155099 0.58753750\nbonds       0.57332197  0.4097863 0.3491461 0.62760221 0.68650892 0.71764750\noptions     0.57757308  0.6185072 0.5086060 0.57442898 0.61132490 0.65763256\ncommodities 0.61400985  0.3290529 0.4441649 0.47704508 0.70238981 0.51550950\nrates       0.55238752  0.5755616 0.5431489 0.69475675 0.64490454 0.77282115\nderivatives 0.49012202  0.3827311 0.5318247 0.43474149 0.61920591 0.54190151\nforex       0.40819574  0.2797698 0.3576979 0.34075318 0.64516846 0.50694883\ndividends   0.32676795  0.2749764 0.5389510 0.73133016 0.58852246 0.60794111\nprofits     0.53869698  0.3672578 0.4587729 0.74918358 0.73580367 0.75875790\nbitcoin     0.05817392 -0.1300953 0.1242287 0.06331612 0.07221163 0.08769512\n               bubble      crisis\nstocks      0.4699493  0.47469567\nbonds       0.4333462  0.41757844\noptions     0.2980414  0.38857788\ncommodities 0.3962825  0.38706461\nrates       0.4628630  0.60169887\nderivatives 0.4796452  0.30869621\nforex       0.4183343  0.35766548\ndividends   0.1902493  0.25218451\nprofits     0.4467198  0.47026321\nbitcoin     0.2190691 -0.05570452\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(manual_similarity_results_34)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                economy     growth   recession   inflation employment\nstocks       0.65425959 0.62400151  0.59762752 0.599831902 0.42318812\nbonds        0.46402332 0.54405098  0.44274361 0.533199902 0.42457070\noptions      0.43687030 0.49661368  0.31539672 0.451595130 0.60445833\ncommodities  0.52723072 0.56284582  0.40773432 0.523646696 0.45590560\nrates        0.70896441 0.78751815  0.73347470 0.855651550 0.70941063\nderivatives  0.30290322 0.40283918  0.30166410 0.380573277 0.29504002\nforex        0.42746042 0.38058020  0.38743491 0.459205966 0.30298252\ndividends    0.35500459 0.48542552  0.32137017 0.442262514 0.47539279\nprofits      0.63026826 0.70406166  0.59810072 0.612064950 0.55401247\nbitcoin     -0.05630604 0.04042479 -0.04533643 0.003072138 0.03177759\n                 trade     policy  taxation    savings investment     credit\nstocks      0.61352555  0.3726410 0.2920333 0.51031381 0.66155099 0.58753750\nbonds       0.57332197  0.4097863 0.3491461 0.62760221 0.68650892 0.71764750\noptions     0.57757308  0.6185072 0.5086060 0.57442898 0.61132490 0.65763256\ncommodities 0.61400985  0.3290529 0.4441649 0.47704508 0.70238981 0.51550950\nrates       0.55238752  0.5755616 0.5431489 0.69475675 0.64490454 0.77282115\nderivatives 0.49012202  0.3827311 0.5318247 0.43474149 0.61920591 0.54190151\nforex       0.40819574  0.2797698 0.3576979 0.34075318 0.64516846 0.50694883\ndividends   0.32676795  0.2749764 0.5389510 0.73133016 0.58852246 0.60794111\nprofits     0.53869698  0.3672578 0.4587729 0.74918358 0.73580367 0.75875790\nbitcoin     0.05817392 -0.1300953 0.1242287 0.06331612 0.07221163 0.08769512\n               bubble      crisis\nstocks      0.4699493  0.47469567\nbonds       0.4333462  0.41757844\noptions     0.2980414  0.38857788\ncommodities 0.3962825  0.38706461\nrates       0.4628630  0.60169887\nderivatives 0.4796452  0.30869621\nforex       0.4183343  0.35766548\ndividends   0.1902493  0.25218451\nprofits     0.4467198  0.47026321\nbitcoin     0.2190691 -0.05570452\n```\n\n\n:::\n:::\n\n\nHeatmap #2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimilarity_matrix_34 <- similarity_results_34\n\n# Convert the matrix to a data frame for plotting\nsimilarity_df_34 <- as.data.frame(as.table(similarity_matrix_34))\nnames(similarity_df_34) <- c(\"Word1\", \"Word2\", \"Similarity\")\n\n# Plot the heatmap\nggplot(similarity_df_34, aes(x = Word2, y = Word1, fill = Similarity)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"white\", high = \"darkgreen\", mid = \"lightgreen\", midpoint = 0.5, na.value = \"lightgrey\",\n                       limit = c(0.15, 0.85), space = \"Lab\", name=\"Cosine\\nSimilarity\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(x = NULL, y = NULL, title = \"Cosine Similarity Heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Appendix\n\nIllustration of cosine similarity\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary library\nlibrary(ggplot2)\nlibrary(grid)  # For arrow functions\n\n# Define three vectors\nv1 <- c(1, 2)\nv2 <- c(2, 3)\nv3 <- c(-1, 1)\n\n# Create a data frame for plotting\nvector_data <- data.frame(\n  x = c(0, 0, 0), y = c(0, 0, 0),\n  xend = c(v1[1], v2[1], v3[1]),\n  yend = c(v1[2], v2[2], v3[2]),\n  vector = c(\"v1\", \"v2\", \"v3\")\n)\n\n# Function to calculate cosine similarity\ncosine_similarity <- function(a, b) {\n  sum(a * b) / (sqrt(sum(a^2)) * sqrt(sum(b^2)))\n}\n\n# Function to calculate vector magnitude\nvector_length <- function(v) {\n  sqrt(sum(v^2))\n}\n\n# Calculate cosine similarities\nsim_v1_v2 <- cosine_similarity(v1, v2)\nsim_v1_v3 <- cosine_similarity(v1, v3)\nsim_v2_v3 <- cosine_similarity(v2, v3)\n\n# Calculate angles in degrees\nangle_v1_v2 <- acos(sim_v1_v2) * (180 / pi)\nangle_v1_v3 <- acos(sim_v1_v3) * (180 / pi)\nangle_v2_v3 <- acos(sim_v2_v3) * (180 / pi)\n\n# Calculate vector lengths\nlength_v1 <- vector_length(v1)\nlength_v2 <- vector_length(v2)\nlength_v3 <- vector_length(v3)\n\n# Plot the vectors\nggplot(vector_data, aes(x = x, y = y)) +\n  geom_segment(aes(xend = xend, yend = yend, color = vector),\n               arrow = arrow(length = unit(0.2, \"inches\")), size = 1.5, lineend = 'round') +\n  geom_text(aes(x = xend, y = yend, label = vector), vjust = -0.5, hjust = -0.5) +\n  coord_fixed(ratio = 1, xlim = c(-2, 3), ylim = c(-1, 4)) +\n  ggtitle(paste(\"Vector Metrics:\\n\",\n                \"Lengths   v1: \", round(length_v1, 2), \", v2: \", \n                round(length_v2, 2), \", v3: \", \n                round(length_v3, 2), \"\\n\",\n                \"Cosine Similarities   v1-v2: \", \n                round(sim_v1_v2, 2), \", v1-v3: \", \n                round(sim_v1_v3, 2), \", v2-v3: \", \n                round(sim_v2_v3, 2), \"\\n\",\n                \"Angles (Degrees)   v1-v2: \", \n                round(angle_v1_v2, 2), \", v1-v3: \", \n                round(angle_v1_v3, 2), \", v2-v3: \", \n                round(angle_v2_v3, 2))) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"darkolivegreen3\", \"darkgreen\", \"cyan3\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Print the lengths, cosine similarities, and angles\ncat(\"Vector Lengths:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVector Lengths:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v1:\", length_v1, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv1: 2.236068 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v2:\", length_v2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv2: 3.605551 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v3:\", length_v3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv3: 1.414214 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Cosine Similarities:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCosine Similarities:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v1-v2:\", sim_v1_v2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv1-v2: 0.9922779 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v1-v3:\", sim_v1_v3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv1-v3: 0.3162278 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v2-v3:\", sim_v2_v3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv2-v3: 0.1961161 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Angles (Degrees):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAngles (Degrees):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v1-v2:\", angle_v1_v2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv1-v2: 7.125016 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v1-v3:\", angle_v1_v3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv1-v3: 71.56505 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"v2-v3:\", angle_v2_v3, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nv2-v3: 78.69007 \n```\n\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}