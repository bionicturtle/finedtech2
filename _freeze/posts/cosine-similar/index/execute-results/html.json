{
  "hash": "d4d5d587942ebfa30888903ed4948108",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Cosine similarity\ndescription: Word embedding enables measures of semantic relatedness.\nauthor: David Harper, CFA, FRM\ndate: 2024-04-04\ncategories: [code, analysis]\nexecute: \n  echo: true\n  warning: false\n---\n\n\nThe [word2vec](https://cran.r-project.org/web/packages/word2vec/readme/README.html) package has a function to compute the cosine similarity, *word2vec_similarity (..., type = \"cosine\")*. This is a measure of semantic relatedness between the words in the two sets. My example uses the [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) which conveniently comes in four sizes. I'm using the smallest: it has 6B tokens, 400K vocab, and 50 dimensions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\nlibrary(word2vec)\n\n# First with some seasons\nvector1_words <- c(\"snow\",\"sun\", \"happy\", \"freedom\", \"graduate\", \"olympics\", \"vacation\", \n                   \"school\", \"resolutions\", \"beach\", \"mountain\", \"cold\", \"snowboard\", \"easter\")\nvector2_words <- c(\"winter\", \"spring\", \"summer\", \"fall\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# path to the GloVe file\nglove_path <- \"glove.6B/glove.6B.50d.txt\"\n\n# Function to load GloVe vectors from a file using data.table's fread for efficiency\nread_glove <- function(file_path) {\n    # Define column types: first column as character, remaining as numeric\n    num_columns <- length(fread(file_path, nrows = 1, header = FALSE)) # Detect the number of columns\n    col_types <- c(\"character\", rep(\"numeric\", num_columns - 1))\n\n    # Load data using fread with specified column types for efficiency\n    embeddings <- fread(file_path, header = FALSE, quote = \"\", colClasses = col_types)\n    setnames(embeddings, old = names(embeddings), new = c(\"word\", paste0(\"V\", 1:(num_columns-1))))\n\n    # Convert to data.table (if not already one, although fread should return a data.table)\n    embeddings <- as.data.table(embeddings)\n\n    return(embeddings)\n}\n\n# Load the GloVe embeddings\nm_w2v <- read_glove(glove_path)\n\n# Function to get embeddings for given words, retaining as dataframes\nget_embeddings <- function(model, words) {\n    setkey(model, word)\n    embeddings <- model[J(words), .SD, .SDcols = -\"word\", nomatch = 0L]\n    return(as.data.frame(embeddings))\n}\n\n# Obtain the vector for the words as dataframes\nvector1_embeds <- get_embeddings(m_w2v, vector1_words)\nvector2_embeds <- get_embeddings(m_w2v, vector2_words)\n\nsave(vector1_embeds, vector2_embeds, file = \"embeddings.RData\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the saved data\nload(\"embeddings.RData\")\n\n# Convert dataframes to matrices by dropping the first column\nvector1_embeds_m <- as.matrix(vector1_embeds[, -1, drop = FALSE])\nvector2_embeds_m <- as.matrix(vector2_embeds[, -1, drop = FALSE])\n\n# Set row and column names for matrices\nrownames(vector1_embeds_m) <- vector1_words\nrownames(vector2_embeds_m) <- vector2_words\n\n# Manual cosine similarity function\n# cosine_similarity <- function(matrix1, matrix2) {\n#     # Compute cosine similarity between each row of matrix1 and each row of matrix2\n#     result <- matrix(0, nrow = nrow(matrix1), ncol = nrow(matrix2))\n#     for (i in 1:nrow(matrix1)) {\n#         for (j in 1:nrow(matrix2)) {\n#             result[i, j] <- sum(matrix1[i, ] * matrix2[j, ]) / (sqrt(sum(matrix1[i, ]^2)) * sqrt(sum(matrix2[j, ]^2)))\n#         }\n#     }\n#     dimnames(result) <- list(row.names(matrix1), row.names(matrix2))\n#     return(result)\n# }\n\n# reduced to vector via perplexity.ai\ncosine_similarity <- function(matrix1, matrix2) {\n  norm_matrix1 <- sqrt(rowSums(matrix1^2))\n  norm_matrix2 <- sqrt(rowSums(matrix2^2))\n  result <- matrix1 %*% t(matrix2) / (norm_matrix1 %o% norm_matrix2)\n  dimnames(result) <- list(row.names(matrix1), row.names(matrix2))\n  return(result)\n}\n\n# Compute the similarity between the two collections of embeddings\nsimilarity_results <- word2vec_similarity(vector1_embeds_m, vector2_embeds_m, type = \"cosine\")\n\n# Manually compute cosine similarity\nmanual_similarity_results <- cosine_similarity(vector1_embeds_m, vector2_embeds_m)\n\n# Print the word2vec similarity results with row and column names\nprint(\"word2vec_similarity results:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"word2vec_similarity results:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(similarity_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                winter    spring    summer      fall\nsnow        0.74125017 0.6963912 0.6185827 0.5663821\nsun         0.44478379 0.5830609 0.4773879 0.5118248\nhappy       0.48504846 0.5028063 0.5609525 0.5506670\nfreedom     0.18136758 0.2254465 0.2702612 0.3755605\ngraduate    0.25377881 0.3788511 0.4521937 0.3111673\nolympics    0.66424604 0.4863632 0.6722972 0.4371825\nvacation    0.66606219 0.6043653 0.7153726 0.5044602\nschool      0.39208831 0.5613046 0.5794563 0.4539855\nresolutions 0.09477322 0.1713795 0.1198187 0.2054978\nbeach       0.50492486 0.5174625 0.5725889 0.3445451\nmountain    0.56929335 0.5649946 0.5214697 0.4198440\ncold        0.70956314 0.6887602 0.6809916 0.6406256\nsnowboard   0.31661765 0.1385683 0.2266248 0.0351622\neaster      0.55143247 0.6239188 0.5490755 0.4957569\n```\n\n\n:::\n\n```{.r .cell-code}\n# Print the manually calculated cosine similarity results\nprint(\"Manual cosine_similarity results:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Manual cosine_similarity results:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(manual_similarity_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                winter    spring    summer      fall\nsnow        0.74125017 0.6963912 0.6185827 0.5663821\nsun         0.44478379 0.5830609 0.4773879 0.5118248\nhappy       0.48504846 0.5028063 0.5609525 0.5506670\nfreedom     0.18136758 0.2254465 0.2702612 0.3755605\ngraduate    0.25377881 0.3788511 0.4521937 0.3111673\nolympics    0.66424604 0.4863632 0.6722972 0.4371825\nvacation    0.66606219 0.6043653 0.7153726 0.5044602\nschool      0.39208831 0.5613046 0.5794563 0.4539855\nresolutions 0.09477322 0.1713795 0.1198187 0.2054978\nbeach       0.50492486 0.5174625 0.5725889 0.3445451\nmountain    0.56929335 0.5649946 0.5214697 0.4198440\ncold        0.70956314 0.6887602 0.6809916 0.6406256\nsnowboard   0.31661765 0.1385683 0.2266248 0.0351622\neaster      0.55143247 0.6239188 0.5490755 0.4957569\n```\n\n\n:::\n:::\n\n\nHeatmap #1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nsimilarity_matrix <- similarity_results\n\n# Convert the matrix to a data frame for plotting\nsimilarity_df <- as.data.frame(as.table(similarity_matrix))\nnames(similarity_df) <- c(\"Word1\", \"Word2\", \"Similarity\")\n\n# Plot the heatmap\nggplot(similarity_df, aes(x = Word2, y = Word1, fill = Similarity)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"white\", high = \"darkgreen\", mid = \"lightgreen\", midpoint = 0.5, na.value = \"lightgrey\",\n                       limit = c(0.15, 0.85), space = \"Lab\", name=\"Cosine\\nSimilarity\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(x = NULL, y = NULL, title = \"Cosine Similarity Heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nNow let's try some finance terms. Please note the following (attempted) terms were not in the vocabulary: mutual funds, ETFs, hedge funds, IPO, venture capital, interest rates, capital gains.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvector3_words <- c(\"stocks\", \"bonds\", \"options\", \"commodities\", \"rates\", \"derivatives\", \"forex\", \"dividends\", \"profits\", \"bitcoin\")\n\nvector4_words <- c(\"economy\", \"growth\", \"recession\", \"inflation\", \"employment\", \"trade\", \"policy\", \"taxation\", \"savings\", \"investment\", \"credit\", \"bubble\", \"crisis\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtain the vector for the words as dataframes\nvector3_embeds <- get_embeddings(m_w2v, vector3_words)\nvector4_embeds <- get_embeddings(m_w2v, vector4_words)\n\nsave(vector3_embeds, vector4_embeds, file = \"embeddings_34.RData\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the saved data\nload(\"embeddings_34.RData\")\n\n# Convert dataframes to matrices by dropping the first column\nvector3_embeds_m <- as.matrix(vector3_embeds[, -1, drop = FALSE])\nvector4_embeds_m <- as.matrix(vector4_embeds[, -1, drop = FALSE])\n\n# Set row and column names for matrices\nrownames(vector3_embeds_m) <- vector3_words\nrownames(vector4_embeds_m) <- vector4_words\n\n# Compute the similarity between the two collections of embeddings\nsimilarity_results_34 <- word2vec_similarity(vector3_embeds_m, vector4_embeds_m, type = \"cosine\")\nmanual_similarity_results_34 <- cosine_similarity(vector3_embeds_m, vector4_embeds_m)\n\nprint(similarity_results_34)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                economy     growth   recession   inflation employment\nstocks       0.65425959 0.62400151  0.59762752 0.599831902 0.42318812\nbonds        0.46402332 0.54405098  0.44274361 0.533199902 0.42457070\noptions      0.43687030 0.49661368  0.31539672 0.451595130 0.60445833\ncommodities  0.52723072 0.56284582  0.40773432 0.523646696 0.45590560\nrates        0.70896441 0.78751815  0.73347470 0.855651550 0.70941063\nderivatives  0.30290322 0.40283918  0.30166410 0.380573277 0.29504002\nforex        0.42746042 0.38058020  0.38743491 0.459205966 0.30298252\ndividends    0.35500459 0.48542552  0.32137017 0.442262514 0.47539279\nprofits      0.63026826 0.70406166  0.59810072 0.612064950 0.55401247\nbitcoin     -0.05630604 0.04042479 -0.04533643 0.003072138 0.03177759\n                 trade     policy  taxation    savings investment     credit\nstocks      0.61352555  0.3726410 0.2920333 0.51031381 0.66155099 0.58753750\nbonds       0.57332197  0.4097863 0.3491461 0.62760221 0.68650892 0.71764750\noptions     0.57757308  0.6185072 0.5086060 0.57442898 0.61132490 0.65763256\ncommodities 0.61400985  0.3290529 0.4441649 0.47704508 0.70238981 0.51550950\nrates       0.55238752  0.5755616 0.5431489 0.69475675 0.64490454 0.77282115\nderivatives 0.49012202  0.3827311 0.5318247 0.43474149 0.61920591 0.54190151\nforex       0.40819574  0.2797698 0.3576979 0.34075318 0.64516846 0.50694883\ndividends   0.32676795  0.2749764 0.5389510 0.73133016 0.58852246 0.60794111\nprofits     0.53869698  0.3672578 0.4587729 0.74918358 0.73580367 0.75875790\nbitcoin     0.05817392 -0.1300953 0.1242287 0.06331612 0.07221163 0.08769512\n               bubble      crisis\nstocks      0.4699493  0.47469567\nbonds       0.4333462  0.41757844\noptions     0.2980414  0.38857788\ncommodities 0.3962825  0.38706461\nrates       0.4628630  0.60169887\nderivatives 0.4796452  0.30869621\nforex       0.4183343  0.35766548\ndividends   0.1902493  0.25218451\nprofits     0.4467198  0.47026321\nbitcoin     0.2190691 -0.05570452\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(manual_similarity_results_34)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                economy     growth   recession   inflation employment\nstocks       0.65425959 0.62400151  0.59762752 0.599831902 0.42318812\nbonds        0.46402332 0.54405098  0.44274361 0.533199902 0.42457070\noptions      0.43687030 0.49661368  0.31539672 0.451595130 0.60445833\ncommodities  0.52723072 0.56284582  0.40773432 0.523646696 0.45590560\nrates        0.70896441 0.78751815  0.73347470 0.855651550 0.70941063\nderivatives  0.30290322 0.40283918  0.30166410 0.380573277 0.29504002\nforex        0.42746042 0.38058020  0.38743491 0.459205966 0.30298252\ndividends    0.35500459 0.48542552  0.32137017 0.442262514 0.47539279\nprofits      0.63026826 0.70406166  0.59810072 0.612064950 0.55401247\nbitcoin     -0.05630604 0.04042479 -0.04533643 0.003072138 0.03177759\n                 trade     policy  taxation    savings investment     credit\nstocks      0.61352555  0.3726410 0.2920333 0.51031381 0.66155099 0.58753750\nbonds       0.57332197  0.4097863 0.3491461 0.62760221 0.68650892 0.71764750\noptions     0.57757308  0.6185072 0.5086060 0.57442898 0.61132490 0.65763256\ncommodities 0.61400985  0.3290529 0.4441649 0.47704508 0.70238981 0.51550950\nrates       0.55238752  0.5755616 0.5431489 0.69475675 0.64490454 0.77282115\nderivatives 0.49012202  0.3827311 0.5318247 0.43474149 0.61920591 0.54190151\nforex       0.40819574  0.2797698 0.3576979 0.34075318 0.64516846 0.50694883\ndividends   0.32676795  0.2749764 0.5389510 0.73133016 0.58852246 0.60794111\nprofits     0.53869698  0.3672578 0.4587729 0.74918358 0.73580367 0.75875790\nbitcoin     0.05817392 -0.1300953 0.1242287 0.06331612 0.07221163 0.08769512\n               bubble      crisis\nstocks      0.4699493  0.47469567\nbonds       0.4333462  0.41757844\noptions     0.2980414  0.38857788\ncommodities 0.3962825  0.38706461\nrates       0.4628630  0.60169887\nderivatives 0.4796452  0.30869621\nforex       0.4183343  0.35766548\ndividends   0.1902493  0.25218451\nprofits     0.4467198  0.47026321\nbitcoin     0.2190691 -0.05570452\n```\n\n\n:::\n:::\n\n\nHeatmap #2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimilarity_matrix_34 <- similarity_results_34\n\n# Convert the matrix to a data frame for plotting\nsimilarity_df_34 <- as.data.frame(as.table(similarity_matrix_34))\nnames(similarity_df_34) <- c(\"Word1\", \"Word2\", \"Similarity\")\n\n# Plot the heatmap\nggplot(similarity_df_34, aes(x = Word2, y = Word1, fill = Similarity)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"white\", high = \"darkgreen\", mid = \"lightgreen\", midpoint = 0.5, na.value = \"lightgrey\",\n                       limit = c(0.15, 0.85), space = \"Lab\", name=\"Cosine\\nSimilarity\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(x = NULL, y = NULL, title = \"Cosine Similarity Heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}