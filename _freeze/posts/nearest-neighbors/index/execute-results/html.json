{
  "hash": "c030aa38b559058ecae9d38d37f002b4",
  "result": {
    "markdown": "---\ntitle: Nearest neighbors\ndescription: This lazy learning algorithm requires us to select k, but it's a fast and intuitive classifer\nauthor: David Harper, CFA, FRM\ndate: 2023-10-13\ncategories: [code, analysis]\nexecute: \n  echo: true\n  warning: false\n---\n\n\nContents\n\n-   Visualizing nearest neighbors in two-dimensional feature space (simulated borrow default based on credit score and income)\n-   Parallel coordinates plot to visualize nearest neighbors in\n    multi-dimensional feature space (Wisconsin breast cancer dataset with 30 features)\n\nFirst the libraries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_libraries <- c(\"tidyverse\", \"ggforce\", \"janitor\", \"openxlsx\", \"patchwork\", # for mine\n                  \"class\", \"GGally\", \"viridis\") # for Lantz's data\nlapply(my_libraries, library, character.only = TRUE)\n\nlibrary(class)\nlibrary(GGally)\nlibrary(viridis)\n```\n:::\n\n\n### Predicting loan default based on vote of nearest neighbors\n\nBecause it's easy to visualize, my first example is simulated data in a two-dimensional feature space. The training set is 100 borrowers with credit scores and incomes. This is supervised learning: the borrowers either defaulted or repaid. The **single test point** (see blue triangle below) is a borrower with a credit score of 605 and income of $41,000. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(743) \nn <- 100 \n\ncredit_scores <- rnorm(n, mean=650, sd=50)\nincomes <- rnorm(n, mean=50000, sd=10000)\n\n# Default if credit score is below 600 OR income is below $40,000\nlabels <- ifelse(credit_scores < 600 | incomes < 40000, \"default\", \"repay\")\n\n# But switch some \"repay\" to \"default\" to add noise\nrandom_indices <- sample(1:n, n/10) # Arbitrary 10%\nlabels[random_indices] <- \"default\"\n\ntrain <- data.frame(credit_score=credit_scores, income=incomes, label=labels)\n# In k-nn, we should either standardize or normalize the data\nmu_credit <- mean(train$credit_score); sig_credit <- sd(train$credit_score)\nmu_income <- mean(train$income); sig_income <- sd(train$income)\ntrain$credit_score_std <- (train$credit_score - mu_credit) / sig_credit\ntrain$income_std <- (train$income - mu_income) / sig_income\n\n# The test point; then standardized\nx <- 605\ny <- 41000\nx_std <- (x - mu_credit) / sig_credit\ny_std <- (y - mu_income) / sig_income\n\n# Euclidean distance (from all points) to test point\ndistances_std <- sqrt((train$credit_score_std - x_std)^2 + (train$income_std - y_std)^2)\n\n# The k-nearest neighbors are simply the k (=5 or =10 or =15, eg) smallest distances\nk05 <- 5; k10 <- 10; k15 <- 15\n\nk_nearest_indices_std_05 <- order(distances_std)[1:k05]\nk_nearest_indices_std_10 <- order(distances_std)[1:k10]\nk_nearest_indices_std_15 <- order(distances_std)[1:k15]\n\n# Add distances column and display  k-nearest neighbors with their distance\nk_nn <- train[k_nearest_indices_std_15, ]\nnearest <- distances_std[k_nearest_indices_std_15]\nk_nn$distance <- nearest\n\n# k_nearest_neighbors\nk_nn |> adorn_rounding(digits = 0, rounding = \"half up\", \n                                      all_of(c(\"credit_score\", \"income\"))) |> \n    adorn_rounding(digits = 3, rounding = \"half up\", credit_score_std:distance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   credit_score income   label credit_score_std income_std distance\n8           611  41988   repay           -0.804     -0.759    0.145\n82          598  39398 default           -1.035     -1.016    0.201\n41          603  43184   repay           -0.951     -0.641    0.220\n4           592  39166 default           -1.150     -1.039    0.300\n64          604  44168   repay           -0.932     -0.543    0.315\n32          587  41994 default           -1.243     -0.759    0.345\n75          621  44367   repay           -0.618     -0.523    0.445\n53          627  38749 default           -0.513     -1.081    0.457\n1           583  46197 default           -1.307     -0.342    0.650\n45          598  34567 default           -1.045     -1.496    0.652\n11          639  43148   repay           -0.282     -0.644    0.665\n80          640  43630   repay           -0.263     -0.596    0.699\n84          643  43094   repay           -0.219     -0.650    0.723\n52          646  41064   repay           -0.156     -0.851    0.756\n99          639  45497   repay           -0.295     -0.411    0.761\n```\n:::\n\n```{.r .cell-code}\n# Now the ggplots!\n# colors\nthree_colors <- c(\"default\" = \"lightpink1\", \"repay\" = \"lightgreen\")\nfive_colors <- c(\"default\" = \"lightpink1\", \"repay\" = \"lightgreen\", \"NN Default\" = \"red\", \"NN Repay\" = \"green4\")    \n\n# Base plots, with labels and without (and zoomed in per coord_cartesian)\np_base_lab <- ggplot(train, aes(x=credit_score_std, y=income_std)) +\n    geom_point(aes(x=x_std, y=y_std), color=\"dodgerblue2\", shape=17, size=4) +\n    xlab(\"Credit Score (Standardized)\") +\n    ylab(\"Income (Standardized)\") +\n    theme_minimal()\n\np_base <- ggplot(train, aes(x=credit_score_std, y=income_std)) + \n    geom_point(aes(x=x_std, y=y_std), color=\"dodgerblue2\", shape=17, size=4) +\n    theme_minimal() +\n    theme(legend.position = \"none\", axis.title = element_blank()) +\n    coord_cartesian(xlim = c(-1.75, 0), ylim = c(-1.75, 0))\n\np1_lab <- p_base_lab + \n    geom_point(aes(color = label), size = 3) + \n    labs(title = paste(\"The majority of how many k neighbors?\"),\n         subtitle = paste(\"Blue triangle is Test point\"),\n         color = \"Borrower\") +\n    scale_color_manual(values = three_colors)\n\np3_lab <- p_base_lab +\n    geom_point(aes(color = ifelse(row.names(train) %in% row.names(train[k_nearest_indices_std_10, ]),\n                                  ifelse(label == \"default\", \"NN Default\", \"NN Repay\"),\n                                  label)), size = 3) +\n    labs(title = paste(\"Let's ask k = 10 neighbors to vote\"),\n         subtitle = paste(\"Six defaulted and four repaid (radius is ~0.652)\"),\n         color = \"Borrower\") +\n    geom_circle(aes(x0 = x_std, y0 = y_std, r = 0.658), \n              color = \"blue\",linetype=\"dashed\", fill = NA) +\n    scale_color_manual(values = five_colors)\n\np1_lab\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\np3_lab\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\np1 <- p_base + \n    geom_point(aes(color = label), size = 3) +\n    scale_color_manual(values = three_colors)\n\np2 <- p_base + \n    geom_point(aes(color = ifelse(row.names(train) %in% row.names(train[k_nearest_indices_std_05, ]),\n                                  ifelse(label == \"default\", \"NN Default\", \"NN Repay\"),\n                                  label)), size = 3) +\n    geom_circle(aes(x0 = x_std, y0 = y_std, r = 0.330), \n              color = \"blue\",linetype=\"dashed\", fill = NA) +\n    scale_color_manual(values = five_colors) \n\n p3 <- p_base +\n    geom_point(aes(color = ifelse(row.names(train) %in% row.names(train[k_nearest_indices_std_10, ]),\n                                  ifelse(label == \"default\", \"NN Default\", \"NN Repay\"),\n                                  label)), size = 3) +\n    geom_circle(aes(x0 = x_std, y0 = y_std, r = 0.658), \n              color = \"blue\",linetype=\"dashed\", fill = NA) +\n    scale_color_manual(values = five_colors)\n\n p4 <- p_base +\n    geom_point(aes(color = ifelse(row.names(train) %in% row.names(train[k_nearest_indices_std_15, ]),\n                                  ifelse(label == \"default\", \"NN Default\", \"NN Repay\"),\n                                  label)), size = 3) +\n     geom_circle(aes(x0 = x_std, y0 = y_std, r = 0.763), \n              color = \"blue\",linetype=\"dashed\", fill = NA) +\n    scale_color_manual(values = five_colors) \n\n(p1 | p2) / (p3 | p4) + \n     plot_annotation(title = \"Top: None and k = 5, Bottom: k = 10 and k = 15\", \n                     subtitle = \"From repay (3/5) to default (6/10) to repay (9/15)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-3.png){width=672}\n:::\n:::\n\n\n### Predicting default based on vote of nearest neighbors\n\nMost datasets have many features. I quickly tried a few experiments to visualize multidimensional neighbors. At this point, my favorite is the parallel coordinates plot below. I'll use the dataset from my favorite machine learning introduction: [Machine Learning with R by Brent Lantz, 4th Edition](https://www.packtpub.com/product/machine-learning-with-r-fourth-edition/9781801071321). He did not attempt to visualize this nearest neighbor's example. \n\nWe're using the [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic). The dataset has 569 observations and 30 numeric features that describe characteristics of the cell nuclei present in the image. The target variable is the diagnosis (benign or malignant).\n\nBrett Lantz parses the dataset into 469 training observation and 100 test observations. Please note that these features are normalized (i.e., on a zero to one scale) rather than standardized (as I did above). Below, I  retrieve the first test instance and plot its two **nearest (Euclidean) neighbors** in the training set. Although this does not convey numerical distance (obviously), I think it's a fine way to illustrate the proximity of the features. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"wbcd_dfs.RData\") # wbcd_train, wbcd_train_labels, wbcd_test, wbcd_test_labels\n# I previously retrieved the nearest neighbors to the single test instance\n# k_nearest neighbors <- function(test_instance, train_data, k)\n# save(k_neighbors, file = \"k_neighbors.RData\")\nload(\"k_neighbors.RData\") # k_neighbors\n\nstr(wbcd_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t469 obs. of  30 variables:\n $ radius_mean      : num  0.253 0.171 0.192 0.203 0.389 ...\n $ texture_mean     : num  0.0906 0.3125 0.2408 0.1245 0.1184 ...\n $ perimeter_mean   : num  0.242 0.176 0.187 0.202 0.372 ...\n $ area_mean        : num  0.136 0.0861 0.0974 0.1024 0.2411 ...\n $ smoothness_mean  : num  0.453 0.399 0.497 0.576 0.244 ...\n $ compactness_mean : num  0.155 0.292 0.18 0.289 0.153 ...\n $ concavity_mean   : num  0.0934 0.1496 0.0714 0.1086 0.0795 ...\n $ points_mean      : num  0.184 0.131 0.123 0.238 0.132 ...\n $ symmetry_mean    : num  0.454 0.435 0.33 0.359 0.334 ...\n $ dimension_mean   : num  0.202 0.315 0.283 0.227 0.115 ...\n $ radius_se        : num  0.0451 0.1228 0.0309 0.0822 0.0242 ...\n $ texture_se       : num  0.0675 0.1849 0.2269 0.2172 0.0116 ...\n $ perimeter_se     : num  0.043 0.1259 0.0276 0.0515 0.0274 ...\n $ area_se          : num  0.0199 0.0379 0.0126 0.0365 0.0204 ...\n $ smoothness_se    : num  0.215 0.196 0.117 0.325 0.112 ...\n $ compactness_se   : num  0.0717 0.252 0.0533 0.2458 0.0946 ...\n $ concavity_se     : num  0.0425 0.0847 0.0267 0.0552 0.0392 ...\n $ points_se        : num  0.235 0.259 0.142 0.372 0.173 ...\n $ symmetry_se      : num  0.16 0.382 0.131 0.111 0.121 ...\n $ dimension_se     : num  0.0468 0.0837 0.045 0.088 0.0301 ...\n $ radius_worst     : num  0.198 0.141 0.159 0.142 0.294 ...\n $ texture_worst    : num  0.0965 0.291 0.3843 0.0999 0.0989 ...\n $ perimeter_worst  : num  0.182 0.139 0.147 0.13 0.269 ...\n $ area_worst       : num  0.0894 0.0589 0.0703 0.0611 0.1558 ...\n $ smoothness_worst : num  0.445 0.331 0.434 0.433 0.274 ...\n $ compactness_worst: num  0.0964 0.2175 0.1173 0.1503 0.142 ...\n $ concavity_worst  : num  0.0992 0.153 0.0852 0.0692 0.1088 ...\n $ points_worst     : num  0.323 0.272 0.255 0.296 0.281 ...\n $ symmetry_worst   : num  0.249 0.271 0.282 0.106 0.182 ...\n $ dimension_worst  : num  0.0831 0.1366 0.1559 0.084 0.0828 ...\n```\n:::\n\n```{.r .cell-code}\n# this knn() function is from the class package\n# and it classifies the test set; e.g., 1st is classified as Benign\nwbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,\n                      cl = wbcd_train_labels, k = 21)\nwbcd_test_pred[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] Benign\nLevels: Benign Malignant\n```\n:::\n\n```{.r .cell-code}\n# inserting first instance at top of training set for graph\nwbcd_train <- rbind(wbcd_test[1, ], wbcd_train) # 469 + 1 = 470\n\nwbcd_train$group <- \"Others\"\nwbcd_train$group[1] <- \"Test Instance\"\nobs_2_index <- k_neighbors[1] + 1\nwbcd_train$group[obs_2_index] <- \"Nearest #1\"\nobs_3_index <- k_neighbors[2] + 1\nwbcd_train$group[obs_3_index] <- \"Nearest #2\"\n\n# set.seed(479)\nset.seed(48514)\n\n# Set the row indices you want to include\nrow1 <- 1\nrow2 <- obs_2_index\nrow3 <- obs_3_index\n\n# Number of random rows to sample\nn <- 10\n\n# Sample without the specific rows, then combine with the specific rows\nsampled_indices <- sample(setdiff(1:nrow(wbcd_train), c(row1, row2, row3)), n)\nfinal_sample <- rbind(wbcd_train[c(row1, row2, row3), ], wbcd_train[sampled_indices, ])\n\nfinal_sample |> ggparcoord(columns = 1:30, \n                           groupColumn = \"group\",\n                           showPoints = TRUE,\n                           alphaLines = 0.3,\n                           scale = \"uniminmax\") +\n    scale_color_manual(values = c(\"Test Instance\" = \"blue\",\n                                  \"Nearest #1\" = \"green4\",\n                                  \"Nearest #2\" = \"green4\",\n                                  \"Others\" = \"yellow\")) + \n    theme_minimal() +\n    labs(title = \"Parallel Coordinates Plot\") +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}